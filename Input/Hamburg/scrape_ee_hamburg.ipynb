{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d251b8d",
   "metadata": {},
   "source": [
    "# EEHH Mitgliedsunternehmen – externe Websites extrahieren\n",
    "\n",
    "Dieses Notebook scrapt die Mitgliederübersicht der Erneuerbare Energien Hamburg (EEHH) und besucht jede Profilseite, um die externe Website der Firma aus dem Profiltext zu extrahieren.\n",
    "\n",
    "- Übersicht: https://www.erneuerbare-energien-hamburg.de/de/mitglieder/mitgliedsunternehmen.html\n",
    "- Pagination: über Parameter `?page_c3=2..7` (Seite 1 ist die Basis-URL)\n",
    "- Ausgabe: data/eeh_mitglieder.csv\n",
    "\n",
    "Hinweise:\n",
    "- Die Firmenseite ist auf der Detailseite häufig im Fließtext erwähnt (z. T. ohne anklickbaren Link). Das Notebook erkennt Domain-/URL-Muster (z. B. `www.example.com`) auch im reinen Text und normalisiert sie zu `https://...`.\n",
    "- Social-Links und interne Links (EEHH-Domain) werden gefiltert.\n",
    "- Ergebnis wird dedupliziert und nach Name/URL sortiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37738f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definitions loaded. Set RUN_FULL=True and re-run this cell to scrape.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import pathlib\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "LIST_BASE = 'https://www.erneuerbare-energien-hamburg.de/de/mitglieder/mitgliedsunternehmen.html'\n",
    "PAGES = list(range(1, 31))  # 1..30 pages total\n",
    "OUTPUT_DIR = pathlib.Path('data')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_FILE = OUTPUT_DIR / 'eeh_mitglieder.csv'\n",
    "\n",
    "# Set to True to run the full crawl; keep False when testing functions quickly in this notebook\n",
    "RUN_FULL = False\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36',\n",
    "    'Accept-Language': 'de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)\n",
    "\n",
    "SOCIAL = {\n",
    "    'linkedin.com','facebook.com','twitter.com','x.com','youtube.com','instagram.com',\n",
    "    'xing.com','kununu.com','t.me','bit.ly','goo.gl','google.com'\n",
    "}\n",
    "\n",
    "# Known non-company or project domains frequently mentioned in profiles\n",
    "BAD_DOMAINS = {\n",
    "    'norddeutsches-reallabor.de',\n",
    "    'eehh.de',\n",
    "}\n",
    "\n",
    "STOPWORDS = {\n",
    "    'gmbh','mbh','kg','co','ag','se','ug','eg','ev','e.v.','mbb','llp','ltd','inc',\n",
    "    'partnerschaft','partner','rechtsanwälte','rechtsanwaelte','anwälte','anwaelte',\n",
    "    'und','the','der','die','das','von','für','fuer','mit','hamburg','deutschland'\n",
    "}\n",
    "\n",
    "def clean(s: str | None) -> str:\n",
    "    if not s: return ''\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "def normalize_url(u: str | None) -> str | None:\n",
    "    if not u: return None\n",
    "    u = u.strip()\n",
    "    u = re.sub(r'^(https?://)+', 'https://', u, flags=re.I)\n",
    "    if not re.match(r'^https?://', u, flags=re.I):\n",
    "        if u.startswith('//'): u = 'https:' + u\n",
    "        else: u = 'https://' + u.lstrip('/')\n",
    "    return u\n",
    "\n",
    "def is_external(u: str) -> bool:\n",
    "    p = urlparse(u)\n",
    "    return bool(p.scheme and p.netloc) and 'erneuerbare-energien-hamburg.de' not in p.netloc.lower()\n",
    "\n",
    "def is_social(u: str) -> bool:\n",
    "    host = urlparse(u).netloc.lower().replace('www.', '')\n",
    "    return any(dom in host for dom in SOCIAL)\n",
    "\n",
    "def domain_from_url(u: str) -> str:\n",
    "    try:\n",
    "        host = urlparse(u).netloc.lower().replace('www.', '')\n",
    "        return host\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "URL_RE = re.compile(r'(?:https?://)?(?:www\\.)?([A-Za-z0-9][-A-Za-z0-9]{0,62}\\.[A-Za-z]{2,})(?:/[^\\s<]*)?')\n",
    "\n",
    "# --- Name ↔ domain alignment helpers ---\n",
    "\n",
    "def tokens_from_name(name: str) -> list[str]:\n",
    "    name = clean(name).lower()\n",
    "    # split on spaces, punctuation and hyphens\n",
    "    toks = re.split(r'[^a-z0-9]+', name)\n",
    "    toks = [t for t in toks if len(t) > 2 and t not in STOPWORDS]\n",
    "    return toks\n",
    "\n",
    "def domain_match_score(name_tokens: list[str], domain: str) -> int:\n",
    "    sld = domain.split(':')[0].split('.')\n",
    "    if len(sld) >= 2:\n",
    "        sld = sld[0]  # take most-specific label (left-most)\n",
    "    else:\n",
    "        sld = domain.split(':')[0]\n",
    "    score = 0\n",
    "    for t in name_tokens:\n",
    "        if t and t in sld:\n",
    "            score += 5\n",
    "    # small bonus for initials like BN in bn-kollegen\n",
    "    initials = ''.join(w[0] for w in name_tokens if w)\n",
    "    if len(initials) >= 2 and initials.lower() in sld:\n",
    "        score += 2\n",
    "    return score\n",
    "\n",
    "# --- Extract profile links on a listing page ---\n",
    "\n",
    "def extract_profile_links(list_url: str) -> list[tuple[str,str]]:\n",
    "    r = session.get(list_url, timeout=30); r.raise_for_status()\n",
    "    s = BeautifulSoup(r.text, 'lxml')\n",
    "    links = []\n",
    "    base_domain = 'https://www.erneuerbare-energien-hamburg.de/'\n",
    "    \n",
    "    for a in s.select('a[href*=\"mitgliedsunternehmen/details/\"]'):\n",
    "        href = a.get('href'); name = clean(a.get_text())\n",
    "        if not href or not name:\n",
    "            continue\n",
    "        \n",
    "        # Build URL correctly - href often starts with 'de/' so prepend base domain\n",
    "        if href.startswith('de/'):\n",
    "            u = base_domain + href\n",
    "        elif href.startswith('/'):\n",
    "            u = base_domain + href.lstrip('/')\n",
    "        else:\n",
    "            u = urljoin(base_domain, href)\n",
    "        \n",
    "        links.append((name, u))\n",
    "    # Dedupe by URL keeping first name encountered\n",
    "    out, seen = [], set()\n",
    "    for name, u in links:\n",
    "        if u in seen: continue\n",
    "        seen.add(u); out.append((name, u))\n",
    "    return out\n",
    "\n",
    "# --- Pick the best external website from a profile page ---\n",
    "\n",
    "def best_external_from_profile(profile_url: str, name_hint: str | None) -> str | None:\n",
    "    try:\n",
    "        rp = session.get(profile_url, timeout=30)\n",
    "        if not rp.ok: return None\n",
    "        sp = BeautifulSoup(rp.text, 'lxml')\n",
    "        name_tokens = tokens_from_name(name_hint or '')\n",
    "        \n",
    "        # 1) Consider explicit anchors with external URLs\n",
    "        candidates: list[tuple[int,str]] = []\n",
    "        for a in sp.select('a[href]'):\n",
    "            href = a.get('href')\n",
    "            if not href: continue\n",
    "            \n",
    "            # Handle protocol-relative URLs (//example.com)\n",
    "            if href.startswith('//'):\n",
    "                href = 'https:' + href\n",
    "            \n",
    "            # Build absolute URL\n",
    "            if href.startswith('http://') or href.startswith('https://'):\n",
    "                u = href\n",
    "            else:\n",
    "                u = urljoin(profile_url, href)\n",
    "            \n",
    "            # Normalize and check if external\n",
    "            u = normalize_url(u)\n",
    "            if not u or not is_external(u) or is_social(u):\n",
    "                continue\n",
    "            \n",
    "            dom = domain_from_url(u)\n",
    "            if any(bad in dom for bad in BAD_DOMAINS):\n",
    "                continue\n",
    "            \n",
    "            # Skip mailto, tel, fax, javascript\n",
    "            if any(u.lower().startswith(prefix) for prefix in ['mailto:', 'tel:', 'fax:', 'javascript:']):\n",
    "                continue\n",
    "            \n",
    "            # Score the link\n",
    "            label = clean(a.get_text()).lower()\n",
    "            score = 0\n",
    "            \n",
    "            # Base score for typical TLDs\n",
    "            if re.search(r'^https?://[^/]*\\.(com|de|io|net|org)/?$', u, re.I):\n",
    "                score += 3\n",
    "            \n",
    "            # Add alignment with company name\n",
    "            score += domain_match_score(name_tokens, dom)\n",
    "            \n",
    "            # Bonus for typical website indicators\n",
    "            if any(t in label for t in ['website','webseite','homepage','zur website']):\n",
    "                score += 2\n",
    "            if a.get('target') == '_blank':\n",
    "                score += 1\n",
    "            \n",
    "            candidates.append((score, u))\n",
    "        \n",
    "        if candidates:\n",
    "            candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "            return candidates[0][1]\n",
    "        \n",
    "        # 2) Fallback: mine plain text for domain-like patterns\n",
    "        body_text = sp.get_text(' ', strip=True)\n",
    "        text_domains: list[str] = []\n",
    "        for m in URL_RE.finditer(body_text):\n",
    "            domain = m.group(1).lower()\n",
    "            text_domains.append(domain)\n",
    "        # Preserve order, dedupe\n",
    "        seen = set(); ordered = []\n",
    "        for d in text_domains:\n",
    "            if d in seen: continue\n",
    "            seen.add(d); ordered.append(d)\n",
    "        # Filter externals and bad domains\n",
    "        filtered = []\n",
    "        for d in ordered:\n",
    "            if any(bad in d for bad in BAD_DOMAINS):\n",
    "                continue\n",
    "            u = normalize_url(d)\n",
    "            if not u: continue\n",
    "            if not is_external(u) or is_social(u):\n",
    "                continue\n",
    "            filtered.append((d, u))\n",
    "        if filtered:\n",
    "            # Score by name alignment\n",
    "            scored = []\n",
    "            for d, u in filtered:\n",
    "                s = domain_match_score(name_tokens, d)\n",
    "                # small bias towards shorter, brand-like domains\n",
    "                s += max(0, 4 - d.count('-'))\n",
    "                scored.append((s, u))\n",
    "            scored.sort(key=lambda x: x[0], reverse=True)\n",
    "            return scored[0][1]\n",
    "        \n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- Crawl all pages and profiles (guarded by RUN_FULL) ---\n",
    "if RUN_FULL:\n",
    "    rows = []\n",
    "    for p in PAGES:\n",
    "        list_url = LIST_BASE if p == 1 else f'{LIST_BASE}?page_c3={p}'\n",
    "        pairs = extract_profile_links(list_url)\n",
    "        print(f'Page {p}/{len(PAGES)}: Found {len(pairs)} companies')\n",
    "        \n",
    "        for i, (name_guess, prof_url) in enumerate(pairs, 1):\n",
    "            website = best_external_from_profile(prof_url, name_guess)\n",
    "            rows.append({\n",
    "                'category': 'mitgliedsunternehmen',\n",
    "                'name': name_guess,\n",
    "                'website_url': website,\n",
    "                'eeh_profile_url': prof_url,\n",
    "            })\n",
    "            if i % 5 == 0:\n",
    "                print(f'  Processed {i}/{len(pairs)} companies on page {p}')\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    # Build dataframe and clean\n",
    "    df = pd.DataFrame(rows) if rows else pd.DataFrame(columns=['category','name','website_url','eeh_profile_url'])\n",
    "    # Cleanup\n",
    "    df['name'] = df['name'].fillna('').apply(clean).replace({'': None})\n",
    "    df['website_url'] = df['website_url'].apply(normalize_url)\n",
    "    valid = df['website_url'].notna() & df['website_url'].str.match(r'^https?://', na=False, case=False)\n",
    "    df = df[valid]\n",
    "    # Drop anything accidentally landing on a bad domain (defensive)\n",
    "    df = df[~df['website_url'].str.contains('|'.join(BAD_DOMAINS), case=False, na=False)]\n",
    "\n",
    "    df = df.drop_duplicates(subset=['name','website_url']).sort_values(by=['category','name','website_url'], na_position='last').reset_index(drop=True)\n",
    "\n",
    "    # Save\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'✓ Successfully scraped {len(rows)} total entries')\n",
    "    print(f'✓ {len(df)} entries with valid website URLs (after filtering bad domains)')\n",
    "    print(f'✓ Saved to {OUTPUT_FILE}')\n",
    "    print(f'{\"=\"*60}\\n')\n",
    "    display(df.head(20))\n",
    "else:\n",
    "    print(\"Definitions loaded. Set RUN_FULL=True and re-run this cell to scrape.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d81bcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>name</th>\n",
       "      <th>website_url</th>\n",
       "      <th>eeh_profile_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>mitgliedsunternehmen</td>\n",
       "      <td>DURAG GmbH</td>\n",
       "      <td>https://www.durag.com/de/wasserstoff-4745.htm</td>\n",
       "      <td>https://www.erneuerbare-energien-hamburg.de/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>mitgliedsunternehmen</td>\n",
       "      <td>morEnergy GmbH</td>\n",
       "      <td>https://www.morenergy.net</td>\n",
       "      <td>https://www.erneuerbare-energien-hamburg.de/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>mitgliedsunternehmen</td>\n",
       "      <td>Staatliche Gewerbeschule Energietechnik-G 10</td>\n",
       "      <td>https://norddeutsches-reallabor.de/</td>\n",
       "      <td>https://www.erneuerbare-energien-hamburg.de/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>mitgliedsunternehmen</td>\n",
       "      <td>GFA Consulting Group GmbH</td>\n",
       "      <td>https://www.gfa-group.de</td>\n",
       "      <td>https://www.erneuerbare-energien-hamburg.de/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>mitgliedsunternehmen</td>\n",
       "      <td>Hamburger Energienetze GmbH</td>\n",
       "      <td>https://www.hamburger-energienetze.de/</td>\n",
       "      <td>https://www.erneuerbare-energien-hamburg.de/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>mitgliedsunternehmen</td>\n",
       "      <td>SAPOtech GmbH</td>\n",
       "      <td>https://www.sapotech.de</td>\n",
       "      <td>https://www.erneuerbare-energien-hamburg.de/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>mitgliedsunternehmen</td>\n",
       "      <td>PricewaterhouseCoopers AG</td>\n",
       "      <td>https://norddeutsches-reallabor.de/</td>\n",
       "      <td>https://www.erneuerbare-energien-hamburg.de/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>mitgliedsunternehmen</td>\n",
       "      <td>Teos Energy GmbH</td>\n",
       "      <td>https://www.teosenergy.com</td>\n",
       "      <td>https://www.erneuerbare-energien-hamburg.de/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>mitgliedsunternehmen</td>\n",
       "      <td>HFK Rechtsanwälte PartGmbB</td>\n",
       "      <td>https://norddeutsches-reallabor.de/</td>\n",
       "      <td>https://www.erneuerbare-energien-hamburg.de/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>mitgliedsunternehmen</td>\n",
       "      <td>H2Perform GmbH</td>\n",
       "      <td>https://www.h2perform.de</td>\n",
       "      <td>https://www.erneuerbare-energien-hamburg.de/de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 category                                          name  \\\n",
       "65   mitgliedsunternehmen                                    DURAG GmbH   \n",
       "296  mitgliedsunternehmen                                morEnergy GmbH   \n",
       "240  mitgliedsunternehmen  Staatliche Gewerbeschule Energietechnik-G 10   \n",
       "107  mitgliedsunternehmen                     GFA Consulting Group GmbH   \n",
       "137  mitgliedsunternehmen                   Hamburger Energienetze GmbH   \n",
       "220  mitgliedsunternehmen                                 SAPOtech GmbH   \n",
       "206  mitgliedsunternehmen                     PricewaterhouseCoopers AG   \n",
       "254  mitgliedsunternehmen                              Teos Energy GmbH   \n",
       "124  mitgliedsunternehmen                    HFK Rechtsanwälte PartGmbB   \n",
       "117  mitgliedsunternehmen                                H2Perform GmbH   \n",
       "\n",
       "                                       website_url  \\\n",
       "65   https://www.durag.com/de/wasserstoff-4745.htm   \n",
       "296                      https://www.morenergy.net   \n",
       "240            https://norddeutsches-reallabor.de/   \n",
       "107                       https://www.gfa-group.de   \n",
       "137         https://www.hamburger-energienetze.de/   \n",
       "220                        https://www.sapotech.de   \n",
       "206            https://norddeutsches-reallabor.de/   \n",
       "254                     https://www.teosenergy.com   \n",
       "124            https://norddeutsches-reallabor.de/   \n",
       "117                       https://www.h2perform.de   \n",
       "\n",
       "                                       eeh_profile_url  \n",
       "65   https://www.erneuerbare-energien-hamburg.de/de...  \n",
       "296  https://www.erneuerbare-energien-hamburg.de/de...  \n",
       "240  https://www.erneuerbare-energien-hamburg.de/de...  \n",
       "107  https://www.erneuerbare-energien-hamburg.de/de...  \n",
       "137  https://www.erneuerbare-energien-hamburg.de/de...  \n",
       "220  https://www.erneuerbare-energien-hamburg.de/de...  \n",
       "206  https://www.erneuerbare-energien-hamburg.de/de...  \n",
       "254  https://www.erneuerbare-energien-hamburg.de/de...  \n",
       "124  https://www.erneuerbare-energien-hamburg.de/de...  \n",
       "117  https://www.erneuerbare-energien-hamburg.de/de...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0951a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bn-kollegen.de\n"
     ]
    }
   ],
   "source": [
    "# Quick check: pick best website from BRAHMS NEBEL profile\n",
    "profile = \"https://www.erneuerbare-energien-hamburg.de/de/mitglieder/mitgliedsunternehmen/details/BRAHMS-NEBEL-Partnerschaft-von-Rechtsanw%C3%A4lten-mbB.html\"\n",
    "name_hint = \"BRAHMS NEBEL Partnerschaft von Rechtsanwälten mbB\"\n",
    "print(best_external_from_profile(profile, name_hint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "700203ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows to fix: 81\n",
      "Updated 81 rows and removed remaining bad-domain rows. Saved to data/eeh_mitglieder.csv\n"
     ]
    }
   ],
   "source": [
    "# Optional repair: patch existing CSV by re-extracting only rows with bad domains\n",
    "import os\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    fix_df = pd.read_csv(OUTPUT_FILE)\n",
    "    def has_bad(u: str) -> bool:\n",
    "        if not isinstance(u, str):\n",
    "            return False\n",
    "        host = domain_from_url(u)\n",
    "        return any(b in host for b in BAD_DOMAINS)\n",
    "    mask = fix_df['website_url'].apply(has_bad)\n",
    "    to_fix = fix_df[mask].copy()\n",
    "    print(f\"Rows to fix: {len(to_fix)}\")\n",
    "    updated = 0\n",
    "    for idx, row in to_fix.iterrows():\n",
    "        new_u = best_external_from_profile(str(row['eeh_profile_url']), str(row['name']))\n",
    "        if new_u and not any(b in domain_from_url(new_u) for b in BAD_DOMAINS):\n",
    "            fix_df.at[idx, 'website_url'] = new_u\n",
    "            updated += 1\n",
    "    # Drop any remaining bad domain rows entirely\n",
    "    fix_df = fix_df[~fix_df['website_url'].astype(str).str.contains('|'.join(BAD_DOMAINS), case=False, na=False)]\n",
    "    fix_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"Updated {updated} rows and removed remaining bad-domain rows. Saved to {OUTPUT_FILE}\")\n",
    "else:\n",
    "    print(\"No existing CSV found to repair.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Innoscence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
