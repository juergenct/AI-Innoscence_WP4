{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a4193f",
   "metadata": {},
   "source": [
    "# NGObase — Hamburg organizations (Development, Environment & Climate)\n",
    "\n",
    "We scrape two category pages:\n",
    "- https://ngobase.org/ciwa/DE.HA.HA/DEV/development-ngos-charities-hamburg\n",
    "- https://ngobase.org/ciwa/DE.HA.HA/ENC/environment-and-climate-ngos-charities-hamburg\n",
    "\n",
    "For each card, we extract:\n",
    "- name (title in the card)\n",
    "- website (external link behind the WWW icon)\n",
    "- domain (normalized)\n",
    "- category (development | environment)\n",
    "- source (page URL)\n",
    "\n",
    "Strategy:\n",
    "- Static parse (requests + BeautifulSoup) by locating external, non-social links and pairing them with the nearest title within the same card container.\n",
    "- If static yields 0, use Playwright fallback to evaluate anchors and headings in the rendered DOM.\n",
    "\n",
    "Output: `Input/Hamburg/ngobase_hamburg_organizations.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0728f247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiesen/.conda/envs/AI_Innoscence/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports & helpers\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "URLS = [\n",
    "    (\"https://ngobase.org/ciwa/DE.HA.HA/DEV/development-ngos-charities-hamburg\", \"development\"),\n",
    "    (\"https://ngobase.org/ciwa/DE.HA.HA/ENC/environment-and-climate-ngos-charities-hamburg\", \"environment\"),\n",
    "]\n",
    "\n",
    "notebook_dir = os.path.abspath(\".\")\n",
    "output_path = os.path.join(notebook_dir, \"ngobase_hamburg_organizations.csv\")\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121 Safari/537.36\",\n",
    "})\n",
    "\n",
    "SOCIAL_DOMAINS = (\n",
    "    \"facebook.com\", \"twitter.com\", \"x.com\", \"instagram.com\", \"linkedin.com\",\n",
    "    \"youtube.com\", \"youtu.be\", \"t.me\", \"tiktok.com\", \"vk.com\", \"pinterest.com\",\n",
    "    \"maps.google.\", \"goo.gl/maps\", \"openstreetmap.org\"\n",
    ")\n",
    "\n",
    "\n",
    "def is_external(href: str) -> bool:\n",
    "    try:\n",
    "        netloc = urlparse(href).netloc.lower()\n",
    "        return bool(netloc) and \"ngobase.org\" not in netloc\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_social(href: str) -> bool:\n",
    "    href_l = href.lower()\n",
    "    return any(dom in href_l for dom in SOCIAL_DOMAINS)\n",
    "\n",
    "\n",
    "def extract_domain(u: str) -> str:\n",
    "    try:\n",
    "        host = urlparse(u).netloc.lower()\n",
    "        return host[4:] if host.startswith(\"www.\") else host\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def fetch_html(url: str, retries: int = 2, timeout: int = 30) -> Optional[str]:\n",
    "    for i in range(retries + 1):\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=timeout)\n",
    "            if r.status_code == 200:\n",
    "                return r.text\n",
    "        except Exception:\n",
    "            time.sleep(1 + i)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_soup(url: str) -> Optional[BeautifulSoup]:\n",
    "    html = fetch_html(url)\n",
    "    if not html:\n",
    "        return None\n",
    "    return BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "\n",
    "def nearest_title_for_anchor(a) -> Optional[str]:\n",
    "    cur = a\n",
    "    # climb to card container and look for a clear title\n",
    "    for _ in range(7):\n",
    "        if not cur:\n",
    "            break\n",
    "        # within this container, look for an h2/h3/a strong text that looks like the organization name\n",
    "        for sel in [\"h3\", \"h2\", \"a\", \"strong\"]:\n",
    "            t = cur.select_one(sel)\n",
    "            if t and (t.get_text(strip=True) or \"\").strip():\n",
    "                text = t.get_text(strip=True)\n",
    "                # keep it reasonable (avoid \"Report correction\")\n",
    "                if len(text) > 2 and \"report correction\".lower() not in text.lower():\n",
    "                    return text\n",
    "        cur = cur.parent\n",
    "    # fallback: search previous heading siblings\n",
    "    cur = a\n",
    "    for _ in range(3):\n",
    "        if not cur:\n",
    "            break\n",
    "        prev = cur.find_previous([\"h3\", \"h2\"]) if hasattr(cur, 'find_previous') else None\n",
    "        if prev and prev.get_text(strip=True):\n",
    "            return prev.get_text(strip=True)\n",
    "        cur = cur.parent\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d437996a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static total: 19 rows across 2 pages\n",
      "Wrote /home/thiesen/Documents/AI-Innoscence_Ecosystem/Input/Hamburg/ngobase_hamburg_organizations.csv\n"
     ]
    }
   ],
   "source": [
    "# Static scraper for both categories\n",
    "\n",
    "def parse_ngobase_static() -> List[Dict]:\n",
    "    rows: List[Dict] = []\n",
    "    for url, category in URLS:\n",
    "        soup = get_soup(url)\n",
    "        if not soup:\n",
    "            print(f\"Static: failed to fetch {url}\")\n",
    "            continue\n",
    "\n",
    "        anchors = soup.find_all(\"a\", href=True)\n",
    "        used_in_this_page = set()\n",
    "\n",
    "        for a in anchors:\n",
    "            href = a.get(\"href\")\n",
    "            if not href:\n",
    "                continue\n",
    "            if not href.startswith(\"http\"):\n",
    "                href = urljoin(url, href)\n",
    "            if not is_external(href) or is_social(href):\n",
    "                continue\n",
    "\n",
    "            # Heuristic: prefer small icon links near a title within the same card\n",
    "            name = nearest_title_for_anchor(a) or \"\"\n",
    "            if not name:\n",
    "                continue\n",
    "\n",
    "            domain = extract_domain(href)\n",
    "            key = (category, domain)\n",
    "            if not domain or key in used_in_this_page:\n",
    "                continue\n",
    "            used_in_this_page.add(key)\n",
    "\n",
    "            rows.append({\n",
    "                \"category\": category,\n",
    "                \"name\": name,\n",
    "                \"website\": href,\n",
    "                \"domain\": domain,\n",
    "                \"source\": url,\n",
    "            })\n",
    "\n",
    "    # Global dedupe by (domain, category)\n",
    "    seen = set()\n",
    "    dedup = []\n",
    "    for r in rows:\n",
    "        key = (r[\"category\"], r[\"domain\"])  # preserve per-category uniqueness\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        dedup.append(r)\n",
    "\n",
    "    print(f\"Static total: {len(dedup)} rows across {len(URLS)} pages\")\n",
    "    return dedup\n",
    "\n",
    "static_rows = parse_ngobase_static()\n",
    "if static_rows:\n",
    "    pd.DataFrame(static_rows).to_csv(output_path, index=False)\n",
    "    print(f\"Wrote {output_path}\")\n",
    "else:\n",
    "    print(\"Static returned 0; try the Playwright fallback below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f29b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playwright fallback (if static fails)\n",
    "import asyncio\n",
    "\n",
    "async def scrape_ngobase_playwright(headless: bool = True) -> List[Dict]:\n",
    "    try:\n",
    "        from playwright.async_api import async_playwright\n",
    "    except Exception:\n",
    "        print(\"Playwright not available in this kernel.\")\n",
    "        return []\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=headless, args=[\"--no-sandbox\", \"--disable-dev-shm-usage\"])  \n",
    "        ctx = await browser.new_context()\n",
    "        page = await ctx.new_page()\n",
    "\n",
    "        all_rows: List[Dict] = []\n",
    "        for url, category in URLS:\n",
    "            await page.goto(url, wait_until=\"domcontentloaded\", timeout=60000)\n",
    "\n",
    "            # Let network settle & lazy images load\n",
    "            try:\n",
    "                await page.wait_for_load_state(\"networkidle\", timeout=12000)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            data = await page.evaluate(\n",
    "                \"\"\"\n",
    "                () => {\n",
    "                  const SOCIAL = ['facebook.com','twitter.com','x.com','instagram.com','linkedin.com','youtube.com','youtu.be','t.me','tiktok.com','vk.com','pinterest.com','maps.google.','goo.gl/maps','openstreetmap.org'];\n",
    "                  const isSocial = (u) => SOCIAL.some(s => (u || '').toLowerCase().includes(s));\n",
    "\n",
    "                  const anchors = Array.from(document.querySelectorAll('a[href]')).filter(a => {\n",
    "                    const href = a.getAttribute('href') || '';\n",
    "                    return href.startsWith('http') && !href.includes('ngobase.org') && !isSocial(href);\n",
    "                  });\n",
    "\n",
    "                  const titleFor = (a) => {\n",
    "                    let cur = a;\n",
    "                    for (let i = 0; i < 7 && cur; i++) {\n",
    "                      cur = cur.parentElement;\n",
    "                      if (!cur) break;\n",
    "                      const t = cur.querySelector('h3, h2, a, strong');\n",
    "                      if (t && (t.textContent || '').trim() && !/report correction/i.test(t.textContent)) return t.textContent.trim();\n",
    "                    }\n",
    "                    // fallback: previous headings\n",
    "                    let node = a;\n",
    "                    for (let i = 0; i < 3 && node; i++) {\n",
    "                      const prev = node.previousElementSibling;\n",
    "                      if (prev && /H[23]/.test(prev.tagName) && (prev.textContent || '').trim()) return prev.textContent.trim();\n",
    "                      node = node.parentElement;\n",
    "                    }\n",
    "                    return '';\n",
    "                  };\n",
    "\n",
    "                  const rows = [];\n",
    "                  for (const a of anchors) {\n",
    "                    const href = a.href;\n",
    "                    const name = titleFor(a);\n",
    "                    if (!name) continue;\n",
    "                    rows.push({ name, website: href });\n",
    "                  }\n",
    "                  return rows;\n",
    "                }\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            # Normalize and dedupe per-category\n",
    "            used = set()\n",
    "            for rec in data:\n",
    "                href = rec.get('website')\n",
    "                nm = (rec.get('name') or '').strip()\n",
    "                if not href or not nm:\n",
    "                    continue\n",
    "                dom = extract_domain(href)\n",
    "                key = (category, dom)\n",
    "                if not dom or key in used:\n",
    "                    continue\n",
    "                used.add(key)\n",
    "                all_rows.append({\n",
    "                    'category': category,\n",
    "                    'name': nm,\n",
    "                    'website': href,\n",
    "                    'domain': dom,\n",
    "                    'source': url,\n",
    "                })\n",
    "\n",
    "        await browser.close()\n",
    "        return all_rows\n",
    "\n",
    "# Driver for fallback\n",
    "fallback_rows: List[Dict] = []\n",
    "if not static_rows:\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        import nest_asyncio, asyncio as _asyncio\n",
    "        nest_asyncio.apply()\n",
    "        loop = _asyncio.get_event_loop()\n",
    "    fallback_rows = loop.run_until_complete(scrape_ngobase_playwright(headless=True))\n",
    "    print(f\"Playwright fallback total: {len(fallback_rows)}\")\n",
    "    if fallback_rows:\n",
    "        pd.DataFrame(fallback_rows).to_csv(output_path, index=False)\n",
    "        print(f\"Wrote {output_path}\")\n",
    "    else:\n",
    "        print(\"No rows via fallback. Consider headless=False.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8230c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>name</th>\n",
       "      <th>website</th>\n",
       "      <th>domain</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development</td>\n",
       "      <td>Der Paritätische Wohlfahrtsverband Hamburg</td>\n",
       "      <td>https://www.paritaet-hamburg.de/</td>\n",
       "      <td>paritaet-hamburg.de</td>\n",
       "      <td>https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development</td>\n",
       "      <td>Economy For The Common Good - ECG</td>\n",
       "      <td>https://www.econgood.org/</td>\n",
       "      <td>econgood.org</td>\n",
       "      <td>https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development</td>\n",
       "      <td>F.E.E.D. - Food, Education, Energy &amp; Developme...</td>\n",
       "      <td>https://feed-ev.de/</td>\n",
       "      <td>feed-ev.de</td>\n",
       "      <td>https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development</td>\n",
       "      <td>Gefangene Helfen Jugendlichen E.V.</td>\n",
       "      <td>https://ghj.social/</td>\n",
       "      <td>ghj.social</td>\n",
       "      <td>https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development</td>\n",
       "      <td>Hand In Hand Zurück Ins Leben Hamburg</td>\n",
       "      <td>https://handinhandhh.com/</td>\n",
       "      <td>handinhandhh.com</td>\n",
       "      <td>https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>development</td>\n",
       "      <td>ReeWie-Haus</td>\n",
       "      <td>https://www.reewie-haus.de/</td>\n",
       "      <td>reewie-haus.de</td>\n",
       "      <td>https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>development</td>\n",
       "      <td>THE NEW INSTITUTE</td>\n",
       "      <td>https://thenew.institute/</td>\n",
       "      <td>thenew.institute</td>\n",
       "      <td>https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>development</td>\n",
       "      <td>Der Paritätische Wohlfahrtsverband Hamburg</td>\n",
       "      <td>https://techbeavers.net/web-design-packages-pk...</td>\n",
       "      <td>techbeavers.net</td>\n",
       "      <td>https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>development</td>\n",
       "      <td>Verwaiste Eltern Und Geschwister Hamburg E.V.</td>\n",
       "      <td>https://www.verwaiste-eltern.de/</td>\n",
       "      <td>verwaiste-eltern.de</td>\n",
       "      <td>https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>environment</td>\n",
       "      <td>ANU Landesverband Hamburg / Schleswig-Holstein...</td>\n",
       "      <td>https://www.anu-hh-sh.de/</td>\n",
       "      <td>anu-hh-sh.de</td>\n",
       "      <td>https://ngobase.org/ciwa/DE.HA.HA/ENC/environm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                               name  \\\n",
       "0  development         Der Paritätische Wohlfahrtsverband Hamburg   \n",
       "1  development                  Economy For The Common Good - ECG   \n",
       "2  development  F.E.E.D. - Food, Education, Energy & Developme...   \n",
       "3  development                 Gefangene Helfen Jugendlichen E.V.   \n",
       "4  development              Hand In Hand Zurück Ins Leben Hamburg   \n",
       "5  development                                        ReeWie-Haus   \n",
       "6  development                                  THE NEW INSTITUTE   \n",
       "7  development         Der Paritätische Wohlfahrtsverband Hamburg   \n",
       "8  development      Verwaiste Eltern Und Geschwister Hamburg E.V.   \n",
       "9  environment  ANU Landesverband Hamburg / Schleswig-Holstein...   \n",
       "\n",
       "                                             website               domain  \\\n",
       "0                   https://www.paritaet-hamburg.de/  paritaet-hamburg.de   \n",
       "1                          https://www.econgood.org/         econgood.org   \n",
       "2                                https://feed-ev.de/           feed-ev.de   \n",
       "3                                https://ghj.social/           ghj.social   \n",
       "4                          https://handinhandhh.com/     handinhandhh.com   \n",
       "5                        https://www.reewie-haus.de/       reewie-haus.de   \n",
       "6                          https://thenew.institute/     thenew.institute   \n",
       "7  https://techbeavers.net/web-design-packages-pk...      techbeavers.net   \n",
       "8                   https://www.verwaiste-eltern.de/  verwaiste-eltern.de   \n",
       "9                          https://www.anu-hh-sh.de/         anu-hh-sh.de   \n",
       "\n",
       "                                              source  \n",
       "0  https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...  \n",
       "1  https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...  \n",
       "2  https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...  \n",
       "3  https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...  \n",
       "4  https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...  \n",
       "5  https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...  \n",
       "6  https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...  \n",
       "7  https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...  \n",
       "8  https://ngobase.org/ciwa/DE.HA.HA/DEV/developm...  \n",
       "9  https://ngobase.org/ciwa/DE.HA.HA/ENC/environm...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview\n",
    "import os\n",
    "if os.path.exists(output_path):\n",
    "    df = pd.read_csv(output_path)\n",
    "    print(df.shape)\n",
    "    display(df.head(10))\n",
    "else:\n",
    "    print(\"Output not found yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Innoscence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
