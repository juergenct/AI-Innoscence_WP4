{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57bbafbe",
   "metadata": {},
   "source": [
    "# UmweltPartner – Namen aus PDF extrahieren\n",
    "\n",
    "Dieses Notebook lädt die UmweltPartner-Partnerliste als PDF, extrahiert die Firmennamen und speichert sie als CSV.\n",
    "\n",
    "- Quelle (PDF): https://www.hamburg.de/resource/blob/288634/d7c60a4a7225771269ea685876377238/d-partnerliste-data.pdf\n",
    "- Ausgabe (Namen): data/umweltpartner_names.csv\n",
    "- Optional: URL-Anreicherung via DuckDuckGo für firmeneigene Websites (data/umweltpartner_enriched.csv)\n",
    "\n",
    "Hinweise:\n",
    "- Die Namen können im PDF als Tabelle oder Fließtext vorliegen. Wir versuchen zuerst Tabellen zu extrahieren und fallen dann auf Text-Zeilen zurück.\n",
    "- Social- und Aggregator-Domains werden beim URL-Matching gefiltert.\n",
    "- Bitte respektieren Sie die Website beim Abruf (kleine Delays bei Enrichment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b30cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted 1621 names from PDF\n",
      "✓ Saved to data/umweltpartner_names.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.2 Renewable Energy Experts Hamburg GmbH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A &amp; F Drucklufttechnik GmbH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A + S Antriebstechnik + Spannsysteme Vertriebs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a. hartrodt (GmbH &amp; Co) KG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A. Schmidt &amp; Co. GmbH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A. Witt &amp; Co. GmbH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A.W. Niemeyer GmbH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A.Walther &amp; co. (GmbH &amp; Co.) Spedition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A201 GmbH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abasto- Gesellschaft für re generativen und ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Abaton Kino Betriebs GmbH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ABC Reifendienst Bernd Hartmann Kfz. Meisterbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Abrahams Ambiente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ABS – Jachtservice Andreas Dose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Academic Work GmbH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Achmedi GmbH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>active commuting Deutschland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ad fontes Solartechnik GmbH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adelante Jugendhilfe GmbH Haus 23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ADM Hamburg AG – Werk Hamburg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name\n",
       "0           8.2 Renewable Energy Experts Hamburg GmbH\n",
       "1                         A & F Drucklufttechnik GmbH\n",
       "2   A + S Antriebstechnik + Spannsysteme Vertriebs...\n",
       "3                          a. hartrodt (GmbH & Co) KG\n",
       "4                               A. Schmidt & Co. GmbH\n",
       "5                                  A. Witt & Co. GmbH\n",
       "6                                  A.W. Niemeyer GmbH\n",
       "7              A.Walther & co. (GmbH & Co.) Spedition\n",
       "8                                           A201 GmbH\n",
       "9   abasto- Gesellschaft für re generativen und ra...\n",
       "10                          Abaton Kino Betriebs GmbH\n",
       "11  ABC Reifendienst Bernd Hartmann Kfz. Meisterbe...\n",
       "12                                  Abrahams Ambiente\n",
       "13                    ABS – Jachtservice Andreas Dose\n",
       "14                                 Academic Work GmbH\n",
       "15                                       Achmedi GmbH\n",
       "16                       active commuting Deutschland\n",
       "17                        ad fontes Solartechnik GmbH\n",
       "18                  adelante Jugendhilfe GmbH Haus 23\n",
       "19                      ADM Hamburg AG – Werk Hamburg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io, re, time, pathlib, sys\n",
    "from typing import Iterable, List\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "\n",
    "PDF_URL = 'https://www.hamburg.de/resource/blob/288634/d7c60a4a7225771269ea685876377238/d-partnerliste-data.pdf'\n",
    "OUTPUT_DIR = pathlib.Path('data'); OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "NAMES_CSV = OUTPUT_DIR / 'umweltpartner_names.csv'\n",
    "ENRICHED_CSV = OUTPUT_DIR / 'umweltpartner_enriched.csv'\n",
    "\n",
    "# Flags\n",
    "RUN_EXTRACT = True   # Namen aus PDF extrahieren\n",
    "RUN_ENRICH = True   # URLs via DuckDuckGo suchen (optional)\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36',\n",
    "    'Accept-Language': 'de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "}\n",
    "session = requests.Session(); session.headers.update(headers)\n",
    "\n",
    "def fetch_pdf_bytes(url: str) -> bytes:\n",
    "    r = session.get(url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "def clean(s: str | None) -> str:\n",
    "    if not s: return ''\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "ORG_SUFFIX = {\n",
    "    'gmbh','mbh','kg','co','ag','se','ug','eg','ev','e.v.','mbb','llp','ltd','inc','stiftung','verein',\n",
    "    'partnerschaft','partner','rechtsanwälte','rechtsanwaelte','anwälte','anwaelte','gbr','ohg','kgaa'\n",
    "}\n",
    "\n",
    "HEADER_TOKENS = {\n",
    "    'liste der umweltpartner',\n",
    "    'stand:',\n",
    "    'stand dezember',\n",
    "    'hamburg.de',\n",
    "}\n",
    "\n",
    "def looks_like_name(line: str) -> bool:\n",
    "    if not line: return False\n",
    "    low = line.lower()\n",
    "    if any(tok in low for tok in HEADER_TOKENS):\n",
    "        return False\n",
    "    if any(tok in low for tok in ['www.', 'http://', 'https://', '@', 'telefon', 'fax', 'tel.', 'mail']):\n",
    "        return False\n",
    "    # likely address lines\n",
    "    if re.search(r'\\b(str\\.|strasse|straße|weg|allee|platz|chaussee|damm|ring|ufer|markt)\\b', low):\n",
    "        return False\n",
    "    # Accept if it contains org suffix or multiple capitalized tokens\n",
    "    if any(suf in low for suf in ORG_SUFFIX):\n",
    "        return True\n",
    "    tokens = re.split(r'[^A-Za-zÄÖÜäöüß0-9&\\-\\+\\.]+', line)\n",
    "    caps = sum(1 for t in tokens if t and t[0].isupper())\n",
    "    return caps >= 2 and len(tokens) >= 2\n",
    "\n",
    "def extract_text_lines(pdf_bytes: bytes) -> list[str]:\n",
    "    reader = PdfReader(io.BytesIO(pdf_bytes))\n",
    "    lines: list[str] = []\n",
    "    for page in reader.pages:\n",
    "        txt = page.extract_text() or ''\n",
    "        # normalize Windows/Mac line breaks\n",
    "        txt = txt.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "        for raw in txt.split('\\n'):\n",
    "            line = clean(raw)\n",
    "            if line:\n",
    "                lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# --- New: consolidate bullet items and fix hyphenation across wrapped lines ---\n",
    "BULLET_CHARS = {'•', '-', '–', '—'}\n",
    "\n",
    "def is_bullet_start(line: str) -> bool:\n",
    "    s = line.lstrip()\n",
    "    return any(s.startswith(ch) for ch in BULLET_CHARS)\n",
    "\n",
    "def strip_bullet(line: str) -> str:\n",
    "    s = line.lstrip()\n",
    "    if s and s[0] in BULLET_CHARS:\n",
    "        s = s[1:].strip()\n",
    "    return s\n",
    "\n",
    "def merge_hyphenation(a: str, b: str) -> str:\n",
    "    # If a ends with '-', join without space; also fix patterns like 'ge - sellschaft' -> 'gesellschaft'\n",
    "    if a.endswith('-'):\n",
    "        return a[:-1] + b.lstrip()\n",
    "    # fix spaced hyphenation artifacts\n",
    "    joined = a + ' ' + b\n",
    "    joined = re.sub(r'(\\w)\\s+-\\s+(\\w)', r'\\1\\2', joined)\n",
    "    return clean(joined)\n",
    "\n",
    "def consolidate_bullets(lines: list[str]) -> list[str]:\n",
    "    items: list[str] = []\n",
    "    current: str | None = None\n",
    "    for line in lines:\n",
    "        if is_bullet_start(line):\n",
    "            # flush previous\n",
    "            if current:\n",
    "                items.append(clean(current))\n",
    "            current = strip_bullet(line)\n",
    "        else:\n",
    "            if current:\n",
    "                current = merge_hyphenation(current, line)\n",
    "            else:\n",
    "                # no active bullet; ignore or treat as header\n",
    "                pass\n",
    "    if current:\n",
    "        items.append(clean(current))\n",
    "    # final cleanup per item\n",
    "    cleaned = []\n",
    "    for it in items:\n",
    "        it = re.sub(r'\\s+', ' ', it).strip(' •\\t')\n",
    "        # remove trailing comma/semicolon and duplicate spaces\n",
    "        it = it.rstrip(',;')\n",
    "        cleaned.append(it)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def extract_names_from_text(pdf_bytes: bytes) -> list[str]:\n",
    "    lines = extract_text_lines(pdf_bytes)\n",
    "    items = consolidate_bullets(lines)\n",
    "    candidates: list[str]\n",
    "    if len(items) > 50:  # most pages use bullets; if many, prefer them\n",
    "        candidates = items\n",
    "    else:\n",
    "        # fallback to line-by-line heuristic\n",
    "        candidates = [ln for ln in lines if looks_like_name(ln)]\n",
    "    # Post-filter\n",
    "    out = []\n",
    "    for l in candidates:\n",
    "        l = clean(l)\n",
    "        if not l: continue\n",
    "        if any(tok in l.lower() for tok in HEADER_TOKENS):\n",
    "            continue\n",
    "        if re.fullmatch(r'[0-9A-Za-z/]+', l):\n",
    "            continue\n",
    "        if looks_like_name(l):\n",
    "            out.append(l)\n",
    "    return out\n",
    "\n",
    "\n",
    "def extract_unique_names(pdf_bytes: bytes) -> list[str]:\n",
    "    names = extract_names_from_text(pdf_bytes)\n",
    "    # Deduplicate while preserving order\n",
    "    seen = set(); out = []\n",
    "    for n in names:\n",
    "        if n in seen: continue\n",
    "        seen.add(n); out.append(n)\n",
    "    # Trim trailing commas and normalize internal spaces\n",
    "    out = [clean(n).rstrip(',;') for n in out]\n",
    "    return out\n",
    "\n",
    "if RUN_EXTRACT:\n",
    "    pdf_bytes = fetch_pdf_bytes(PDF_URL)\n",
    "    names = extract_unique_names(pdf_bytes)\n",
    "    df = pd.DataFrame({'name': names})\n",
    "    df.to_csv(NAMES_CSV, index=False)\n",
    "    print(f'✓ Extracted {len(df)} names from PDF')\n",
    "    print(f'✓ Saved to {NAMES_CSV}')\n",
    "    display(df.head(20))\n",
    "else:\n",
    "    print('Extraction disabled (set RUN_EXTRACT=True to run).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc03751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  processed 10…\n",
      "  processed 20…\n",
      "  processed 20…\n",
      "  processed 30…\n",
      "  processed 30…\n",
      "  processed 40…\n",
      "  processed 40…\n",
      "  processed 50…\n",
      "  processed 50…\n",
      "  processed 60…\n",
      "  processed 60…\n",
      "  processed 70…\n",
      "  processed 70…\n",
      "  processed 80…\n",
      "  processed 80…\n",
      "  processed 90…\n",
      "  processed 90…\n",
      "  processed 100…\n",
      "  processed 100…\n",
      "  processed 110…\n",
      "  processed 110…\n",
      "  processed 120…\n",
      "  processed 120…\n",
      "  processed 130…\n",
      "  processed 130…\n",
      "  processed 140…\n",
      "  processed 140…\n",
      "  processed 150…\n",
      "  processed 150…\n",
      "  processed 160…\n",
      "  processed 160…\n",
      "  processed 170…\n",
      "  processed 170…\n",
      "  processed 180…\n",
      "  processed 180…\n",
      "  processed 190…\n",
      "  processed 190…\n",
      "  processed 200…\n",
      "  processed 200…\n",
      "  processed 210…\n",
      "  processed 210…\n",
      "  processed 220…\n",
      "  processed 220…\n",
      "  processed 230…\n",
      "  processed 230…\n",
      "  processed 240…\n",
      "  processed 240…\n",
      "  processed 250…\n",
      "  processed 250…\n",
      "  processed 260…\n",
      "  processed 260…\n",
      "  processed 270…\n",
      "  processed 270…\n",
      "  processed 280…\n",
      "  processed 280…\n",
      "  processed 290…\n",
      "  processed 290…\n",
      "  processed 300…\n",
      "  processed 300…\n",
      "  processed 310…\n",
      "  processed 310…\n",
      "  processed 320…\n",
      "  processed 320…\n",
      "  processed 330…\n",
      "  processed 330…\n",
      "  processed 340…\n",
      "  processed 340…\n",
      "  processed 350…\n",
      "  processed 350…\n",
      "  processed 360…\n",
      "  processed 360…\n",
      "  processed 370…\n",
      "  processed 370…\n",
      "  processed 380…\n",
      "  processed 380…\n",
      "  processed 390…\n",
      "  processed 390…\n",
      "  processed 400…\n",
      "  processed 400…\n",
      "  processed 410…\n",
      "  processed 410…\n",
      "  processed 420…\n",
      "  processed 420…\n",
      "  processed 430…\n",
      "  processed 430…\n",
      "  processed 440…\n",
      "  processed 440…\n",
      "  processed 450…\n",
      "  processed 450…\n",
      "  processed 460…\n",
      "  processed 460…\n",
      "  processed 470…\n",
      "  processed 470…\n",
      "  processed 480…\n",
      "  processed 480…\n",
      "  processed 490…\n",
      "  processed 490…\n",
      "  processed 500…\n",
      "  processed 500…\n",
      "  processed 510…\n",
      "  processed 510…\n",
      "  processed 520…\n",
      "  processed 520…\n",
      "  processed 530…\n",
      "  processed 530…\n",
      "  processed 540…\n",
      "  processed 540…\n",
      "  processed 550…\n",
      "  processed 550…\n",
      "  processed 560…\n",
      "  processed 560…\n",
      "  processed 570…\n",
      "  processed 570…\n",
      "  processed 580…\n",
      "  processed 580…\n",
      "  processed 590…\n",
      "  processed 590…\n",
      "  processed 600…\n",
      "  processed 600…\n",
      "  processed 610…\n",
      "  processed 610…\n",
      "  processed 620…\n",
      "  processed 620…\n",
      "  processed 630…\n",
      "  processed 630…\n",
      "  processed 640…\n",
      "  processed 640…\n",
      "  processed 650…\n",
      "  processed 650…\n",
      "  processed 660…\n",
      "  processed 660…\n",
      "  processed 670…\n",
      "  processed 670…\n",
      "  processed 680…\n",
      "  processed 680…\n",
      "  processed 690…\n",
      "  processed 690…\n",
      "  processed 700…\n",
      "  processed 700…\n",
      "  processed 710…\n",
      "  processed 710…\n",
      "  processed 720…\n",
      "  processed 720…\n",
      "  processed 730…\n",
      "  processed 730…\n",
      "  processed 740…\n",
      "  processed 740…\n",
      "  processed 750…\n",
      "  processed 750…\n",
      "  processed 760…\n",
      "  processed 760…\n",
      "  processed 770…\n",
      "  processed 770…\n",
      "  processed 780…\n",
      "  processed 780…\n",
      "  processed 790…\n",
      "  processed 790…\n",
      "  processed 800…\n",
      "  processed 800…\n",
      "  processed 810…\n",
      "  processed 810…\n",
      "  processed 820…\n",
      "  processed 820…\n",
      "  processed 830…\n",
      "  processed 830…\n",
      "  processed 840…\n",
      "  processed 840…\n",
      "  processed 850…\n",
      "  processed 850…\n",
      "  processed 860…\n",
      "  processed 860…\n",
      "  processed 870…\n",
      "  processed 870…\n",
      "  processed 880…\n",
      "  processed 880…\n",
      "  processed 890…\n",
      "  processed 890…\n",
      "  processed 900…\n",
      "  processed 900…\n",
      "  processed 910…\n",
      "  processed 910…\n",
      "  processed 920…\n",
      "  processed 920…\n",
      "  processed 930…\n",
      "  processed 930…\n",
      "  processed 940…\n",
      "  processed 940…\n",
      "  processed 950…\n",
      "  processed 950…\n",
      "  processed 960…\n",
      "  processed 960…\n",
      "  processed 970…\n",
      "  processed 970…\n",
      "  processed 980…\n",
      "  processed 980…\n",
      "  processed 990…\n",
      "  processed 990…\n",
      "  processed 1000…\n",
      "  processed 1000…\n",
      "  processed 1010…\n",
      "  processed 1010…\n",
      "  processed 1020…\n",
      "  processed 1020…\n",
      "  processed 1030…\n",
      "  processed 1030…\n",
      "  processed 1040…\n",
      "  processed 1040…\n",
      "  processed 1050…\n",
      "  processed 1050…\n",
      "  processed 1060…\n",
      "  processed 1060…\n",
      "  processed 1070…\n",
      "  processed 1070…\n",
      "  processed 1080…\n",
      "  processed 1080…\n",
      "  processed 1090…\n",
      "  processed 1090…\n",
      "  processed 1100…\n",
      "  processed 1100…\n",
      "  processed 1110…\n",
      "  processed 1110…\n",
      "  processed 1120…\n",
      "  processed 1120…\n",
      "  processed 1130…\n",
      "  processed 1130…\n",
      "  processed 1140…\n",
      "  processed 1140…\n",
      "  processed 1150…\n",
      "  processed 1150…\n",
      "  processed 1160…\n",
      "  processed 1160…\n",
      "  processed 1170…\n",
      "  processed 1170…\n",
      "  processed 1180…\n",
      "  processed 1180…\n",
      "  processed 1190…\n",
      "  processed 1190…\n",
      "  processed 1200…\n",
      "  processed 1200…\n",
      "  processed 1210…\n",
      "  processed 1210…\n",
      "  processed 1220…\n",
      "  processed 1220…\n",
      "  processed 1230…\n",
      "  processed 1230…\n",
      "  processed 1240…\n",
      "  processed 1240…\n",
      "  processed 1250…\n",
      "  processed 1250…\n",
      "  processed 1260…\n",
      "  processed 1260…\n",
      "  processed 1270…\n",
      "  processed 1270…\n",
      "  processed 1280…\n",
      "  processed 1280…\n",
      "  processed 1290…\n",
      "  processed 1290…\n",
      "  processed 1300…\n",
      "  processed 1300…\n",
      "  processed 1310…\n",
      "  processed 1310…\n",
      "  processed 1320…\n",
      "  processed 1320…\n",
      "  processed 1330…\n",
      "  processed 1330…\n",
      "  processed 1340…\n",
      "  processed 1340…\n",
      "  processed 1350…\n",
      "  processed 1350…\n",
      "  processed 1360…\n",
      "  processed 1360…\n",
      "  processed 1370…\n",
      "  processed 1370…\n",
      "  processed 1380…\n",
      "  processed 1380…\n",
      "  processed 1390…\n",
      "  processed 1390…\n",
      "  processed 1400…\n",
      "  processed 1400…\n",
      "  processed 1410…\n",
      "  processed 1410…\n",
      "  processed 1420…\n",
      "  processed 1420…\n",
      "  processed 1430…\n",
      "  processed 1430…\n",
      "  processed 1440…\n",
      "  processed 1440…\n",
      "  processed 1450…\n",
      "  processed 1450…\n",
      "  processed 1460…\n",
      "  processed 1460…\n",
      "  processed 1470…\n",
      "  processed 1470…\n",
      "  processed 1480…\n",
      "  processed 1480…\n",
      "  processed 1490…\n",
      "  processed 1490…\n",
      "  processed 1500…\n",
      "  processed 1500…\n",
      "  processed 1510…\n",
      "  processed 1510…\n",
      "  processed 1520…\n",
      "  processed 1520…\n",
      "  processed 1530…\n",
      "  processed 1530…\n",
      "  processed 1540…\n",
      "  processed 1540…\n",
      "  processed 1550…\n",
      "  processed 1550…\n",
      "  processed 1560…\n",
      "  processed 1560…\n",
      "  processed 1570…\n",
      "  processed 1570…\n",
      "  processed 1580…\n",
      "  processed 1580…\n",
      "  processed 1590…\n",
      "  processed 1590…\n",
      "  processed 1600…\n",
      "  processed 1600…\n",
      "  processed 1610…\n",
      "  processed 1610…\n",
      "  processed 1620…\n",
      "  processed 1620…\n",
      "✓ Enriched 1621 rows; saved to data/umweltpartner_enriched.csv\n",
      "✓ Enriched 1621 rows; saved to data/umweltpartner_enriched.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>website_url</th>\n",
       "      <th>website_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.2 Renewable Energy Experts Hamburg GmbH</td>\n",
       "      <td>https://www.zhihu.com/tardis/bd/ans/32050636875</td>\n",
       "      <td>骁龙 8 Gen3 和骁龙 8 至尊版的差距有多大? - 知乎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A &amp; F Drucklufttechnik GmbH</td>\n",
       "      <td>https://www.zhihu.com/question/584414009</td>\n",
       "      <td>函数符号 f 和 f (·) 和 f (-) 有什么区别？ - 知乎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A + S Antriebstechnik + Spannsysteme Vertriebs...</td>\n",
       "      <td>https://www.s-bahn-muenchen.de/de</td>\n",
       "      <td>S -Bahn München - MVV-Fahrplan, Bahn Tickets, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A. Schmidt &amp; Co. GmbH</td>\n",
       "      <td>https://www.a-schmidtco.de/</td>\n",
       "      <td>Ihr Partner in der Lebensmittelindustrie - A.S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A. Witt &amp; Co. GmbH</td>\n",
       "      <td>https://www.witt-weiden.de/</td>\n",
       "      <td>Witt - Ihr Online Shop für Damenmode &amp; Wäsche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A.W. Niemeyer GmbH</td>\n",
       "      <td>https://stackoverflow.com/questions/2477452/â€...</td>\n",
       "      <td>\"â€™\" showing on page instead of - Stack Overflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A.Walther &amp; co. (GmbH &amp; Co.) Spedition</td>\n",
       "      <td>https://www.awalther-co.de/</td>\n",
       "      <td>A. Walther &amp; Co. - Spedition Hamburg (Transpor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A201 GmbH</td>\n",
       "      <td>https://www.hamburg-web.de/guide/rubrik/bauunt...</td>\n",
       "      <td>Bauunternehmen Hamburg - 38 Unternehmen im Ham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ABC Reifendienst Bernd Hartmann Kfz. Meisterbe...</td>\n",
       "      <td>https://kfz-serviceportal.de/werkstatt/hamburg...</td>\n",
       "      <td>ABC Reifendienst Bernd Hartmann in 22339 Hambu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ABS – Jachtservice Andreas Dose</td>\n",
       "      <td>https://www.europages.co.uk/ABSJACHTSERVICE/00...</td>\n",
       "      <td>ABS-JACHTSERVICE in Hamburg, Naval constructio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ADM Hamburg AG ; Werk Noblee &amp; Thörl</td>\n",
       "      <td>https://www.adm.com/en-us/about-adm/locations/...</td>\n",
       "      <td>ADM Europe, Middle East &amp; Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ADM Hamburg AG – Werk Hamburg</td>\n",
       "      <td>https://partnerderwissenschaft.de/candidate/ad...</td>\n",
       "      <td>ADM Hamburg AG Werk Hamburg | Partner der Wiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AFK Automobile</td>\n",
       "      <td>https://www.afkautomobile.de/</td>\n",
       "      <td>AFK Automobile | KFZ Werkstatt Hamburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AGT Bus- &amp; Eventlogistik GmbH</td>\n",
       "      <td>https://www.agtlogistik.de/en/bus-mieten-hamburg/</td>\n",
       "      <td>Bus rental Hamburg | agt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AHOI Events GmbH</td>\n",
       "      <td>https://www.ahoi-events.de/</td>\n",
       "      <td>VERANSTALTUNGEN | AHOI Events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>AM PM Textilreinigung</td>\n",
       "      <td>https://www.timeanddate.com/time/am-and-pm.html</td>\n",
       "      <td>AM and PM: What Do They Mean? - timeanddate.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>AMF Facharztklinik Hamburg GmbH</td>\n",
       "      <td>https://web2.cylex.de/firma-home/amf-facharztk...</td>\n",
       "      <td>AMF Facharztklinik Hamburg GmbH - Öffnungszeiten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>AMS advanced mobility solu tions GmbH</td>\n",
       "      <td>https://zhidao.baidu.com/question/212945156256...</td>\n",
       "      <td>船务术语AMS是什么意思？_百度知道</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>ARKTIK GmbH</td>\n",
       "      <td>https://allianz-entwicklung-klima.de/karte/ark...</td>\n",
       "      <td>ARKTIK GmbH - Stiftung Allianz für Entwicklung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>ARTEKO LED-Lighting GmbH</td>\n",
       "      <td>https://artekoled.com/</td>\n",
       "      <td>ARTEKO - Ihr Experte für LED -Beleuchtung</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  \\\n",
       "0           8.2 Renewable Energy Experts Hamburg GmbH   \n",
       "1                         A & F Drucklufttechnik GmbH   \n",
       "2   A + S Antriebstechnik + Spannsysteme Vertriebs...   \n",
       "4                               A. Schmidt & Co. GmbH   \n",
       "5                                  A. Witt & Co. GmbH   \n",
       "6                                  A.W. Niemeyer GmbH   \n",
       "7              A.Walther & co. (GmbH & Co.) Spedition   \n",
       "8                                           A201 GmbH   \n",
       "11  ABC Reifendienst Bernd Hartmann Kfz. Meisterbe...   \n",
       "13                    ABS – Jachtservice Andreas Dose   \n",
       "20               ADM Hamburg AG ; Werk Noblee & Thörl   \n",
       "19                      ADM Hamburg AG – Werk Hamburg   \n",
       "21                                     AFK Automobile   \n",
       "22                      AGT Bus- & Eventlogistik GmbH   \n",
       "24                                   AHOI Events GmbH   \n",
       "48                              AM PM Textilreinigung   \n",
       "50                    AMF Facharztklinik Hamburg GmbH   \n",
       "53              AMS advanced mobility solu tions GmbH   \n",
       "62                                        ARKTIK GmbH   \n",
       "66                           ARTEKO LED-Lighting GmbH   \n",
       "\n",
       "                                          website_url  \\\n",
       "0     https://www.zhihu.com/tardis/bd/ans/32050636875   \n",
       "1            https://www.zhihu.com/question/584414009   \n",
       "2                   https://www.s-bahn-muenchen.de/de   \n",
       "4                         https://www.a-schmidtco.de/   \n",
       "5                         https://www.witt-weiden.de/   \n",
       "6   https://stackoverflow.com/questions/2477452/â€...   \n",
       "7                         https://www.awalther-co.de/   \n",
       "8   https://www.hamburg-web.de/guide/rubrik/bauunt...   \n",
       "11  https://kfz-serviceportal.de/werkstatt/hamburg...   \n",
       "13  https://www.europages.co.uk/ABSJACHTSERVICE/00...   \n",
       "20  https://www.adm.com/en-us/about-adm/locations/...   \n",
       "19  https://partnerderwissenschaft.de/candidate/ad...   \n",
       "21                      https://www.afkautomobile.de/   \n",
       "22  https://www.agtlogistik.de/en/bus-mieten-hamburg/   \n",
       "24                        https://www.ahoi-events.de/   \n",
       "48    https://www.timeanddate.com/time/am-and-pm.html   \n",
       "50  https://web2.cylex.de/firma-home/amf-facharztk...   \n",
       "53  https://zhidao.baidu.com/question/212945156256...   \n",
       "62  https://allianz-entwicklung-klima.de/karte/ark...   \n",
       "66                             https://artekoled.com/   \n",
       "\n",
       "                                        website_title  \n",
       "0                     骁龙 8 Gen3 和骁龙 8 至尊版的差距有多大? - 知乎  \n",
       "1                  函数符号 f 和 f (·) 和 f (-) 有什么区别？ - 知乎  \n",
       "2   S -Bahn München - MVV-Fahrplan, Bahn Tickets, ...  \n",
       "4   Ihr Partner in der Lebensmittelindustrie - A.S...  \n",
       "5       Witt - Ihr Online Shop für Damenmode & Wäsche  \n",
       "6   \"â€™\" showing on page instead of - Stack Overflow  \n",
       "7   A. Walther & Co. - Spedition Hamburg (Transpor...  \n",
       "8   Bauunternehmen Hamburg - 38 Unternehmen im Ham...  \n",
       "11  ABC Reifendienst Bernd Hartmann in 22339 Hambu...  \n",
       "13  ABS-JACHTSERVICE in Hamburg, Naval constructio...  \n",
       "20                   ADM Europe, Middle East & Africa  \n",
       "19  ADM Hamburg AG Werk Hamburg | Partner der Wiss...  \n",
       "21             AFK Automobile | KFZ Werkstatt Hamburg  \n",
       "22                           Bus rental Hamburg | agt  \n",
       "24                      VERANSTALTUNGEN | AHOI Events  \n",
       "48    AM and PM: What Do They Mean? - timeanddate.com  \n",
       "50   AMF Facharztklinik Hamburg GmbH - Öffnungszeiten  \n",
       "53                                 船务术语AMS是什么意思？_百度知道  \n",
       "62  ARKTIK GmbH - Stiftung Allianz für Entwicklung...  \n",
       "66          ARTEKO - Ihr Experte für LED -Beleuchtung  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optional: URL-Enrichment via DuckDuckGo (prefer `ddgs`, fallback to `duckduckgo_search`)\n",
    "import warnings\n",
    "import atexit\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Prefer the renamed package `ddgs`; fallback to legacy `duckduckgo_search`\n",
    "HAS_DDG = False\n",
    "DDG_PROVIDER = None\n",
    "try:\n",
    "    from ddgs import DDGS  # pip install ddgs\n",
    "    HAS_DDG = True\n",
    "    DDG_PROVIDER = 'ddgs'\n",
    "except Exception:\n",
    "    try:\n",
    "        from duckduckgo_search import DDGS  # legacy package name\n",
    "        # Silence runtime rename warning from legacy package\n",
    "        warnings.filterwarnings(\n",
    "            'ignore',\n",
    "            message=r'This package.*renamed to ddgs',\n",
    "            category=RuntimeWarning\n",
    "        )\n",
    "        HAS_DDG = True\n",
    "        DDG_PROVIDER = 'duckduckgo_search'\n",
    "    except Exception:\n",
    "        HAS_DDG = False\n",
    "        DDG_PROVIDER = None\n",
    "\n",
    "# Optional: keep notebook output clean once we've addressed proper cleanup below\n",
    "SUPPRESS_SOCKET_WARNINGS = True\n",
    "if SUPPRESS_SOCKET_WARNINGS:\n",
    "    warnings.simplefilter('ignore', ResourceWarning)\n",
    "\n",
    "SOCIAL = {'linkedin.com','facebook.com','twitter.com','x.com','instagram.com','youtube.com','xing.com','wikipedia.org'}\n",
    "AGGREGATORS = {'hamburg.de','gelbeseiten.de','meinestadt.de','kompass.com','kununu.com','stepstone.de','indeed.de','yelp.de'}\n",
    "\n",
    "# Maintain a single DDGS client for the whole notebook to avoid opening/closing many HTTP/2 connections\n",
    "_ddg_client = None\n",
    "\n",
    "def _get_ddg_client():\n",
    "    global _ddg_client\n",
    "    if not HAS_DDG:\n",
    "        return None\n",
    "    if _ddg_client is None:\n",
    "        try:\n",
    "            _ddg_client = DDGS()\n",
    "        except Exception:\n",
    "            _ddg_client = None\n",
    "    return _ddg_client\n",
    "\n",
    "@atexit.register\n",
    "def _close_ddg_client():\n",
    "    global _ddg_client\n",
    "    if _ddg_client is not None:\n",
    "        try:\n",
    "            _ddg_client.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        _ddg_client = None\n",
    "\n",
    "def is_good_candidate(url: str, name_tokens: list[str]) -> bool:\n",
    "    host = urlparse(url).netloc.lower().replace('www.','')\n",
    "    if any(dom in host for dom in SOCIAL | AGGREGATORS):\n",
    "        return False\n",
    "    # prefer .de/.com/.eu and alignment with name tokens\n",
    "    tld_ok = host.endswith('.de') or host.endswith('.com') or host.endswith('.eu')\n",
    "    sld = host.split('.')\n",
    "    sld = sld[0] if sld else host\n",
    "    align = sum(1 for t in name_tokens if t in sld)\n",
    "    return tld_ok and align >= 1\n",
    "\n",
    "def tokens(name: str) -> list[str]:\n",
    "    name = name.lower()\n",
    "    toks = re.split(r'[^a-z0-9]+', name)\n",
    "    return [t for t in toks if len(t) > 2 and t not in {'hamburg','gmbh','mbh','kg','ag','se','ug','eg','ev','mbb','llp','ltd','inc','partnerschaft','stiftung','verein'}]\n",
    "\n",
    "def ddg_text(query: str, max_results: int = 6, region: str = 'de-de'):\n",
    "    # Unified wrapper so both providers behave the same\n",
    "    if not HAS_DDG:\n",
    "        return []\n",
    "    client = _get_ddg_client()\n",
    "    if client is None:\n",
    "        return []\n",
    "    try:\n",
    "        return list(client.text(query, region=region, max_results=max_results))\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def search_best_website(name: str) -> tuple[str | None, str | None]:\n",
    "    if not HAS_DDG:\n",
    "        return None, None\n",
    "    query = f\"{name} Hamburg Website\"\n",
    "    toks = tokens(name)\n",
    "    results = ddg_text(query, region='de-de', max_results=6)\n",
    "    for r in results:\n",
    "        url = r.get('href') or r.get('url')\n",
    "        title = r.get('title') or r.get('body') or ''\n",
    "        if not url: continue\n",
    "        if is_good_candidate(url, toks):\n",
    "            return url, title\n",
    "    # fallback: first non-social, non-aggregator\n",
    "    for r in results:\n",
    "        url = r.get('href') or r.get('url')\n",
    "        title = r.get('title') or r.get('body') or ''\n",
    "        if not url: continue\n",
    "        host = urlparse(url).netloc.lower().replace('www.','')\n",
    "        if any(dom in host for dom in SOCIAL | AGGREGATORS):\n",
    "            continue\n",
    "        return url, title\n",
    "    return None, None\n",
    "\n",
    "def enrich_with_websites(names: Iterable[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for i, n in enumerate(names, 1):\n",
    "        url, title = search_best_website(n)\n",
    "        rows.append({'name': n, 'website_url': url, 'website_title': title})\n",
    "        if i % 10 == 0:\n",
    "            print(f'  processed {i}…')\n",
    "        time.sleep(0.3)  # be polite\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "if RUN_ENRICH:\n",
    "    base_df = pd.read_csv(NAMES_CSV) if NAMES_CSV.exists() else pd.DataFrame({'name': []})\n",
    "    if not HAS_DDG:\n",
    "        print('ddgs/duckduckgo_search not available; install ddgs or set RUN_ENRICH=False')\n",
    "    elif base_df.empty:\n",
    "        print('No names CSV found or empty; run extraction first.')\n",
    "    else:\n",
    "        enr = enrich_with_websites(base_df['name'].tolist())\n",
    "        # keep first non-null per name\n",
    "        enr = enr.sort_values(by=['name','website_url'], na_position='last').drop_duplicates(subset=['name'], keep='first')\n",
    "        enr.to_csv(ENRICHED_CSV, index=False)\n",
    "        print(f'✓ Enriched {len(enr)} rows; saved to {ENRICHED_CSV}')\n",
    "        display(enr.head(20))\n",
    "else:\n",
    "    print('Enrichment disabled (set RUN_ENRICH=True to run).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Innoscence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
