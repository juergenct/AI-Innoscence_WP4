{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d62182dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816eb8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ce_oa_institution_hamburg = pd.read_csv('circular-economy-openalex-institution-hamburg.csv')\n",
    "df_ce_patents_applicant_hamburg = pd.read_csv('circular-economy-patents-applicant-hamburg.csv')\n",
    "df_ce_patents_inventor_hamburg = pd.read_csv('circular-economy-patents-inventor-hamburg.csv')\n",
    "df_ce_patents_owner_hamburg = pd.read_csv('circular-economy-patents-owner-hamburg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d8154a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Jurisdiction</th>\n",
       "      <th>Kind</th>\n",
       "      <th>Display Key</th>\n",
       "      <th>Lens ID</th>\n",
       "      <th>Publication Date</th>\n",
       "      <th>Publication Year</th>\n",
       "      <th>Application Number</th>\n",
       "      <th>Application Date</th>\n",
       "      <th>Priority Numbers</th>\n",
       "      <th>...</th>\n",
       "      <th>Sequence Count</th>\n",
       "      <th>CPC Classifications</th>\n",
       "      <th>IPCR Classifications</th>\n",
       "      <th>US Classifications</th>\n",
       "      <th>NPL Citation Count</th>\n",
       "      <th>NPL Resolved Citation Count</th>\n",
       "      <th>NPL Resolved Lens ID(s)</th>\n",
       "      <th>NPL Resolved External ID(s)</th>\n",
       "      <th>NPL Citations</th>\n",
       "      <th>Legal Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>B2</td>\n",
       "      <td>US 7276808 B2</td>\n",
       "      <td>044-230-643-354-954</td>\n",
       "      <td>2007-10-02</td>\n",
       "      <td>2007</td>\n",
       "      <td>US 56916406 A</td>\n",
       "      <td>2006-02-22</td>\n",
       "      <td>DE 10339438 A;;EP 2004009486 W</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>E02D27/42;;E04H2012/006;;F05B2240/40;;F05B2240...</td>\n",
       "      <td>F03D11/04</td>\n",
       "      <td>290/55;;290/44;;415/4.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXPIRED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>A1</td>\n",
       "      <td>US 2006/0267348 A1</td>\n",
       "      <td>016-328-013-942-102</td>\n",
       "      <td>2006-11-30</td>\n",
       "      <td>2006</td>\n",
       "      <td>US 56916406 A</td>\n",
       "      <td>2006-02-22</td>\n",
       "      <td>DE 10339438 A;;EP 2004009486 W</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>E02D27/42;;E04H2012/006;;F05B2240/40;;F05B2240...</td>\n",
       "      <td>F03D9/00;;F03D11/04;;H02P9/04</td>\n",
       "      <td>290/55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXPIRED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>A1</td>\n",
       "      <td>US 2013/0306793 A1</td>\n",
       "      <td>156-946-737-768-407</td>\n",
       "      <td>2013-11-21</td>\n",
       "      <td>2013</td>\n",
       "      <td>US 201313954240 A</td>\n",
       "      <td>2013-07-30</td>\n",
       "      <td>US 201313954240 A;;DE 102011009806 A;;EP 20120...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>B64C1/00;;B64C2001/0027;;B64D11/00;;B64D11/00;...</td>\n",
       "      <td>B64D11/00;;B64C1/22</td>\n",
       "      <td>244/118.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Uline Shipping Supplies, 12/25/2008, https://w...</td>\n",
       "      <td>DISCONTINUED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>A1</td>\n",
       "      <td>US 2005/0061914 A1</td>\n",
       "      <td>189-246-065-546-00X</td>\n",
       "      <td>2005-03-24</td>\n",
       "      <td>2005</td>\n",
       "      <td>US 93476604 A</td>\n",
       "      <td>2004-09-07</td>\n",
       "      <td>DE 10211437 A;;EP 0301386 W</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>B64D11/0696;;B60N2/01575;;B64D11/06;;B64D11/06...</td>\n",
       "      <td>B60N2/06;;B60N2/015;;B64D11/06</td>\n",
       "      <td>244/118.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXPIRED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>US</td>\n",
       "      <td>B2</td>\n",
       "      <td>US 7232094 B2</td>\n",
       "      <td>046-724-247-193-24X</td>\n",
       "      <td>2007-06-19</td>\n",
       "      <td>2007</td>\n",
       "      <td>US 93476604 A</td>\n",
       "      <td>2004-09-07</td>\n",
       "      <td>DE 10211437 A;;EP 0301386 W</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>B64D11/0696;;B60N2/01575;;B64D11/06;;B64D11/06...</td>\n",
       "      <td>B60N2/06;;B64D11/06;;B60N2/015</td>\n",
       "      <td>244/118.6;;297/217.3;;297/248;;297/257</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXPIRED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # Jurisdiction Kind         Display Key              Lens ID  \\\n",
       "0  1           US   B2       US 7276808 B2  044-230-643-354-954   \n",
       "1  2           US   A1  US 2006/0267348 A1  016-328-013-942-102   \n",
       "2  3           US   A1  US 2013/0306793 A1  156-946-737-768-407   \n",
       "3  4           US   A1  US 2005/0061914 A1  189-246-065-546-00X   \n",
       "4  5           US   B2       US 7232094 B2  046-724-247-193-24X   \n",
       "\n",
       "  Publication Date  Publication Year Application Number Application Date  \\\n",
       "0       2007-10-02              2007      US 56916406 A       2006-02-22   \n",
       "1       2006-11-30              2006      US 56916406 A       2006-02-22   \n",
       "2       2013-11-21              2013  US 201313954240 A       2013-07-30   \n",
       "3       2005-03-24              2005      US 93476604 A       2004-09-07   \n",
       "4       2007-06-19              2007      US 93476604 A       2004-09-07   \n",
       "\n",
       "                                    Priority Numbers  ... Sequence Count  \\\n",
       "0                     DE 10339438 A;;EP 2004009486 W  ...              0   \n",
       "1                     DE 10339438 A;;EP 2004009486 W  ...              0   \n",
       "2  US 201313954240 A;;DE 102011009806 A;;EP 20120...  ...              0   \n",
       "3                        DE 10211437 A;;EP 0301386 W  ...              0   \n",
       "4                        DE 10211437 A;;EP 0301386 W  ...              0   \n",
       "\n",
       "                                 CPC Classifications  \\\n",
       "0  E02D27/42;;E04H2012/006;;F05B2240/40;;F05B2240...   \n",
       "1  E02D27/42;;E04H2012/006;;F05B2240/40;;F05B2240...   \n",
       "2  B64C1/00;;B64C2001/0027;;B64D11/00;;B64D11/00;...   \n",
       "3  B64D11/0696;;B60N2/01575;;B64D11/06;;B64D11/06...   \n",
       "4  B64D11/0696;;B60N2/01575;;B64D11/06;;B64D11/06...   \n",
       "\n",
       "             IPCR Classifications                      US Classifications  \\\n",
       "0                       F03D11/04                 290/55;;290/44;;415/4.2   \n",
       "1   F03D9/00;;F03D11/04;;H02P9/04                                  290/55   \n",
       "2             B64D11/00;;B64C1/22                               244/118.1   \n",
       "3  B60N2/06;;B60N2/015;;B64D11/06                               244/118.5   \n",
       "4  B60N2/06;;B64D11/06;;B60N2/015  244/118.6;;297/217.3;;297/248;;297/257   \n",
       "\n",
       "  NPL Citation Count NPL Resolved Citation Count NPL Resolved Lens ID(s)  \\\n",
       "0                  0                           0                     NaN   \n",
       "1                  0                           0                     NaN   \n",
       "2                  1                           0                     NaN   \n",
       "3                  0                           0                     NaN   \n",
       "4                  0                           0                     NaN   \n",
       "\n",
       "  NPL Resolved External ID(s)  \\\n",
       "0                         NaN   \n",
       "1                         NaN   \n",
       "2                         NaN   \n",
       "3                         NaN   \n",
       "4                         NaN   \n",
       "\n",
       "                                       NPL Citations  Legal Status  \n",
       "0                                                NaN       EXPIRED  \n",
       "1                                                NaN       EXPIRED  \n",
       "2  Uline Shipping Supplies, 12/25/2008, https://w...  DISCONTINUED  \n",
       "3                                                NaN       EXPIRED  \n",
       "4                                                NaN       EXPIRED  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ce_patents_owner_hamburg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ffccc",
   "metadata": {},
   "source": [
    "# Paper Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e44e94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affiliation enrichment setup: OpenAlex helpers, caching, Hamburg detection, Crunchbase supplement\n",
    "\n",
    "# Config\n",
    "OPENALEX_BASE = \"https://api.openalex.org/works\"\n",
    "OPENALEX_INSTITUTIONS = \"https://api.openalex.org/institutions\"\n",
    "CONTACT_EMAIL = \"juergen.thiesen@tuhh.de\"  # used in User-Agent per OpenAlex best practices\n",
    "CACHE_DIR = \"data/openalex_cache\"\n",
    "INST_CACHE_DIR = os.path.join(CACHE_DIR, \"institutions\")\n",
    "ENRICHED_OUT_DIR = \"data/enriched\"\n",
    "\n",
    "# Imports\n",
    "import os, re, json, time, hashlib\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Ensure dirs\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(INST_CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(ENRICHED_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Requests session\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": f\"ai-innoscence-wp4/1.0 (mailto:{CONTACT_EMAIL})\"\n",
    "})\n",
    "\n",
    "# ---------- Work-level helpers (by DOI) ----------\n",
    "\n",
    "def _doi_norm(doi: str) -> str:\n",
    "    return (doi or \"\").strip().lower().replace(\"https://doi.org/\", \"\")\n",
    "\n",
    "\n",
    "def _work_cache_path(doi: str) -> str:\n",
    "    key = hashlib.sha256(_doi_norm(doi).encode(\"utf-8\")).hexdigest()\n",
    "    return os.path.join(CACHE_DIR, f\"work_{key}.json\")\n",
    "\n",
    "\n",
    "def _load_cached_work(doi: str) -> Optional[dict]:\n",
    "    p = _work_cache_path(doi)\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def _save_cached_work(doi: str, payload: dict) -> None:\n",
    "    p = _work_cache_path(doi)\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def fetch_work_by_doi(doi: str, max_retries: int = 3, backoff: float = 1.5) -> Optional[dict]:\n",
    "    doi = _doi_norm(doi)\n",
    "    if not doi:\n",
    "        return None\n",
    "    cached = _load_cached_work(doi)\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "    url = f\"{OPENALEX_BASE}/https://doi.org/{doi}\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = session.get(url, timeout=20)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                _save_cached_work(doi, data)\n",
    "                return data\n",
    "            if r.status_code == 404:\n",
    "                _save_cached_work(doi, {\"__not_found__\": True})\n",
    "                return None\n",
    "            if r.status_code in (429, 502, 503, 504):\n",
    "                time.sleep(backoff ** (attempt + 1))\n",
    "                continue\n",
    "            _save_cached_work(doi, {\"__error__\": r.status_code})\n",
    "            return None\n",
    "        except requests.RequestException:\n",
    "            time.sleep(backoff ** (attempt + 1))\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_authorships(work: dict) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    if not work or not isinstance(work, dict):\n",
    "        return rows\n",
    "    for auth in work.get(\"authorships\", []) or []:\n",
    "        author = auth.get(\"author\", {}) or {}\n",
    "        institutions = auth.get(\"institutions\", []) or []\n",
    "        if institutions:\n",
    "            for inst in institutions:\n",
    "                rows.append({\n",
    "                    \"work_id\": work.get(\"id\"),\n",
    "                    \"work_doi\": _doi_norm((work.get(\"doi\") or \"\").replace(\"https://doi.org/\", \"\")),\n",
    "                    \"work_title\": work.get(\"title\"),\n",
    "                    \"author_id\": author.get(\"id\"),\n",
    "                    \"author_display_name\": author.get(\"display_name\"),\n",
    "                    \"institution_id\": inst.get(\"id\"),\n",
    "                    \"institution_display_name\": inst.get(\"display_name\"),\n",
    "                    \"institution_country_code\": inst.get(\"country_code\"),\n",
    "                    \"institution_ror\": inst.get(\"ror\"),\n",
    "                    \"raw_affiliation_string\": auth.get(\"raw_affiliation_string\"),\n",
    "                    \"is_corresponding\": bool(auth.get(\"is_corresponding\")),\n",
    "                    \"author_position\": auth.get(\"author_position\"),\n",
    "                })\n",
    "        else:\n",
    "            rows.append({\n",
    "                \"work_id\": work.get(\"id\"),\n",
    "                \"work_doi\": _doi_norm((work.get(\"doi\") or \"\").replace(\"https://doi.org/\", \"\")),\n",
    "                \"work_title\": work.get(\"title\"),\n",
    "                \"author_id\": author.get(\"id\"),\n",
    "                \"author_display_name\": author.get(\"display_name\"),\n",
    "                \"institution_id\": None,\n",
    "                \"institution_display_name\": None,\n",
    "                \"institution_country_code\": None,\n",
    "                \"institution_ror\": None,\n",
    "                \"raw_affiliation_string\": auth.get(\"raw_affiliation_string\"),\n",
    "                \"is_corresponding\": bool(auth.get(\"is_corresponding\")),\n",
    "                \"author_position\": auth.get(\"author_position\"),\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "# ---------- Hamburg detection ----------\n",
    "HAMBURG_PATTERNS = [\n",
    "    re.compile(r\"\\bhamburg\\b\", re.I),\n",
    "    re.compile(r\"\\bhamburg[-\\s]?harburg\\b\", re.I),\n",
    "    re.compile(r\"\\buniversit[aä]t hamburg\\b|\\buniversity of hamburg\\b|\\buhh\\b\", re.I),\n",
    "    re.compile(r\"\\btechnische universität hamburg\\b|\\btechnical university of hamburg\\b|\\btuhh\\b\", re.I),\n",
    "    re.compile(r\"\\bhamburg university of applied sciences\\b|\\bhaw hamburg\\b\", re.I),\n",
    "    re.compile(r\"\\bhelmholtz (center|zentrum).{0,30}hamburg\\b\", re.I),\n",
    "    re.compile(r\"\\bdesy\\b|\\bdeutsches elektronen[-\\s]?synchrotron\\b\", re.I),\n",
    "    re.compile(r\"\\bmax[-\\s]?planck.{0,30}hamburg\\b\", re.I),\n",
    "    re.compile(r\"\\buke\\b|\\b(universit[aä]ts)?klinikum hamburg[-\\s]?eppendorf\\b\", re.I),\n",
    "    re.compile(r\"\\bhafencity university\\b|\\bhafencity universit[aä]t\\b|\\bhcu\\b\", re.I),\n",
    "    re.compile(r\"\\bhelmut schmidt university\\b|\\buniversit[aä]t der bundeswehr hamburg\\b|\\bhsu\\b\", re.I),\n",
    "    re.compile(r\"\\bhamburg school of business administration\\b|\\bhsba\\b\", re.I),\n",
    "]\n",
    "\n",
    "def is_hamburg_related(name: Optional[str], raw_affil: Optional[str]) -> bool:\n",
    "    txt = \" \".join([str(x) for x in [name, raw_affil] if x]).lower()\n",
    "    if not txt.strip():\n",
    "        return False\n",
    "    return any(p.search(txt) for p in HAMBURG_PATTERNS)\n",
    "\n",
    "# ---------- Institutions helpers ----------\n",
    "\n",
    "def _inst_cache_path(inst_id: str) -> str:\n",
    "    key = hashlib.sha256((inst_id or \"\").encode(\"utf-8\")).hexdigest()\n",
    "    return os.path.join(INST_CACHE_DIR, f\"inst_{key}.json\")\n",
    "\n",
    "\n",
    "def _load_cached_inst(inst_id: str) -> Optional[dict]:\n",
    "    p = _inst_cache_path(inst_id)\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def _save_cached_inst(inst_id: str, payload: dict) -> None:\n",
    "    p = _inst_cache_path(inst_id)\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def fetch_institution(inst_id: str, max_retries: int = 3, backoff: float = 1.5) -> Optional[dict]:\n",
    "    if not inst_id:\n",
    "        return None\n",
    "    cached = _load_cached_inst(inst_id)\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "    suffix = inst_id.split(\"/\")[-1] if inst_id.startswith(\"http\") else inst_id\n",
    "    url = f\"{OPENALEX_INSTITUTIONS}/{suffix}\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = session.get(url, timeout=20)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                _save_cached_inst(inst_id, data)\n",
    "                return data\n",
    "            if r.status_code == 404:\n",
    "                _save_cached_inst(inst_id, {\"__not_found__\": True})\n",
    "                return None\n",
    "            if r.status_code in (429, 502, 503, 504):\n",
    "                time.sleep(backoff ** (attempt + 1))\n",
    "                continue\n",
    "            _save_cached_inst(inst_id, {\"__error__\": r.status_code})\n",
    "            return None\n",
    "        except requests.RequestException:\n",
    "            time.sleep(backoff ** (attempt + 1))\n",
    "    return None\n",
    "\n",
    "# ---------- Name normalization & Crunchbase supplement ----------\n",
    "\n",
    "def normalize_name(s: Optional[str]) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    s = re.sub(r\"\\b(university|universitaet|universit[aä]t|hochschule|gmbh|ag|ggmbh|ev|e v|institute|institut|college|school|center|centre)\\b\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# ---------- Orchestration helpers ----------\n",
    "\n",
    "def build_authorships_from_dois(df: pd.DataFrame, doi_col: str = \"DOI\", limit: Optional[int] = None, sleep_between: float = 0.0) -> pd.DataFrame:\n",
    "    dois = (\n",
    "        df[doi_col].dropna().astype(str).map(_doi_norm).replace(\"\", pd.NA).dropna().unique().tolist()\n",
    "    )\n",
    "    if limit is not None:\n",
    "        dois = dois[:limit]\n",
    "    rows = []\n",
    "    for i, doi in enumerate(dois, start=1):\n",
    "        work = fetch_work_by_doi(doi)\n",
    "        rows.extend(extract_authorships(work) if work else [{\n",
    "            \"work_id\": None, \"work_doi\": doi, \"work_title\": None,\n",
    "            \"author_id\": None, \"author_display_name\": None,\n",
    "            \"institution_id\": None, \"institution_display_name\": None,\n",
    "            \"institution_country_code\": None, \"institution_ror\": None,\n",
    "            \"raw_affiliation_string\": None, \"is_corresponding\": None,\n",
    "            \"author_position\": None,\n",
    "        }])\n",
    "        if sleep_between > 0:\n",
    "            time.sleep(sleep_between)\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processed {i} DOIs...\")\n",
    "    auth_df = pd.DataFrame(rows)\n",
    "    if not auth_df.empty:\n",
    "        auth_df[\"is_hamburg_institution\"] = auth_df.apply(\n",
    "            lambda r: is_hamburg_related(r.get(\"institution_display_name\"), r.get(\"raw_affiliation_string\")), axis=1\n",
    "        )\n",
    "    return auth_df\n",
    "\n",
    "\n",
    "def add_institution_homepages(auth_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if auth_df.empty or \"institution_id\" not in auth_df.columns:\n",
    "        return auth_df\n",
    "    uniq_ids = auth_df[\"institution_id\"].dropna().astype(str).unique().tolist()\n",
    "    inst_rows = []\n",
    "    for i, inst_id in enumerate(uniq_ids, start=1):\n",
    "        data = fetch_institution(inst_id)\n",
    "        if data:\n",
    "            inst_rows.append({\n",
    "                \"institution_id\": inst_id,\n",
    "                \"inst_display_name_api\": data.get(\"display_name\"),\n",
    "                \"inst_homepage_url\": (data.get(\"homepage_url\") or \"\").strip() or None,\n",
    "                \"inst_country_code\": data.get(\"country_code\"),\n",
    "                \"inst_ror\": data.get(\"ror\"),\n",
    "                \"inst_lineage\": \", \".join(data.get(\"lineage\", []) if isinstance(data.get(\"lineage\"), list) else []),\n",
    "                \"inst_type\": data.get(\"type\"),\n",
    "            })\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Resolved {i} institutions...\")\n",
    "    inst_df = pd.DataFrame(inst_rows)\n",
    "    return auth_df.merge(inst_df, on=\"institution_id\", how=\"left\") if not inst_df.empty else auth_df\n",
    "\n",
    "\n",
    "def supplement_homepages_from_crunchbase(auth_df: pd.DataFrame, cb_csv_path: str = \"crunchbase_companies_DEU_Hamburg.csv\") -> pd.DataFrame:\n",
    "    try:\n",
    "        df_cb = pd.read_csv(cb_csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Crunchbase CSV not found; skipping supplemental websites.\")\n",
    "        return auth_df\n",
    "    name_cols = [c for c in df_cb.columns if c.lower() in (\"name\", \"company\", \"organization\")]\n",
    "    url_cols = [c for c in df_cb.columns if \"website\" in c.lower() or c.lower() == \"url\"]\n",
    "    if not (name_cols and url_cols):\n",
    "        print(\"Crunchbase: could not find expected name/website columns; skipping supplemental match.\")\n",
    "        return auth_df\n",
    "    df_cb = df_cb[[name_cols[0], url_cols[0]]].rename(columns={name_cols[0]: \"cb_name\", url_cols[0]: \"cb_website\"})\n",
    "    df_cb[\"name_norm\"] = df_cb[\"cb_name\"].apply(normalize_name)\n",
    "    tmp = auth_df.copy()\n",
    "    inst_name_series = tmp[\"institution_display_name\"].fillna(tmp.get(\"inst_display_name_api\"))\n",
    "    tmp[\"name_norm\"] = inst_name_series.apply(normalize_name)\n",
    "    tmp = tmp.merge(df_cb[[\"name_norm\", \"cb_website\"]].dropna().drop_duplicates(\"name_norm\"), on=\"name_norm\", how=\"left\")\n",
    "    tmp[\"inst_homepage_url\"] = tmp[\"inst_homepage_url\"].where(\n",
    "        tmp[\"inst_homepage_url\"].notna() & (tmp[\"inst_homepage_url\"].str.len() > 0), tmp[\"cb_website\"]\n",
    "    )\n",
    "    return tmp.drop(columns=[\"name_norm\", \"cb_website\"], errors=\"ignore\")\n",
    "\n",
    "\n",
    "def aggregate_and_merge(df_src: pd.DataFrame, auth_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if auth_df.empty:\n",
    "        return df_src.copy()\n",
    "    agg = auth_df.groupby(\"work_doi\").agg(\n",
    "        authors_total=(\"author_id\", lambda s: int(s.notna().sum())),\n",
    "        institutions_total=(\"institution_id\", lambda s: int(s.notna().nunique())),\n",
    "        has_hamburg_affil=(\"is_hamburg_institution\", \"any\"),\n",
    "        hamburg_affil_count=(\"is_hamburg_institution\", \"sum\"),\n",
    "    ).reset_index()\n",
    "    df_aug = df_src.copy()\n",
    "    df_aug[\"doi_norm\"] = df_aug[\"DOI\"].astype(str).map(_doi_norm)\n",
    "    merged = df_aug.merge(agg, how=\"left\", left_on=\"doi_norm\", right_on=\"work_doi\")\n",
    "    merged[\"has_hamburg_affil\"] = merged[\"has_hamburg_affil\"].fillna(False)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def save_artifacts(auth_df: pd.DataFrame, merged: Optional[pd.DataFrame] = None):\n",
    "    # authorships with websites\n",
    "    p_parquet = os.path.join(ENRICHED_OUT_DIR, \"openalex_authorships_with_websites.parquet\")\n",
    "    p_csv = os.path.join(ENRICHED_OUT_DIR, \"openalex_authorships_with_websites.csv\")\n",
    "    try:\n",
    "        auth_df.to_parquet(p_parquet, index=False)\n",
    "        print(f\"Saved: {p_parquet}\")\n",
    "    except Exception:\n",
    "        auth_df.to_csv(p_csv, index=False)\n",
    "        print(f\"Saved: {p_csv}\")\n",
    "\n",
    "    # institution directory\n",
    "    inst_directory = (auth_df[[\n",
    "        \"institution_id\", \"institution_display_name\", \"inst_display_name_api\", \"inst_homepage_url\", \"inst_country_code\", \"inst_ror\"\n",
    "    ]].drop_duplicates().sort_values([\"institution_display_name\", \"inst_display_name_api\"]))\n",
    "    inst_dir_csv = os.path.join(ENRICHED_OUT_DIR, \"institution_directory_with_websites.csv\")\n",
    "    inst_directory.to_csv(inst_dir_csv, index=False)\n",
    "\n",
    "    if merged is not None:\n",
    "        m_parquet = os.path.join(ENRICHED_OUT_DIR, \"df_oa_with_hamburg_flags.parquet\")\n",
    "        m_csv = os.path.join(ENRICHED_OUT_DIR, \"df_oa_with_hamburg_flags.csv\")\n",
    "        try:\n",
    "            merged.to_parquet(m_parquet, index=False)\n",
    "            print(f\"Saved: {m_parquet}\")\n",
    "        except Exception:\n",
    "            merged.to_csv(m_csv, index=False)\n",
    "            print(f\"Saved: {m_csv}\")\n",
    "\n",
    "# End of setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70c1fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 DOIs...\n",
      "Processed 100 DOIs...\n",
      "Processed 150 DOIs...\n",
      "Processed 200 DOIs...\n",
      "Processed 250 DOIs...\n",
      "Processed 200 DOIs...\n",
      "Processed 250 DOIs...\n",
      "Resolved 50 institutions...\n",
      "Resolved 50 institutions...\n",
      "Resolved 100 institutions...\n",
      "Resolved 100 institutions...\n",
      "Resolved 150 institutions...\n",
      "Resolved 150 institutions...\n",
      "Resolved 200 institutions...\n",
      "Resolved 200 institutions...\n",
      "Resolved 250 institutions...\n",
      "Resolved 250 institutions...\n",
      "Resolved 300 institutions...\n",
      "Resolved 300 institutions...\n",
      "Resolved 350 institutions...\n",
      "Resolved 350 institutions...\n",
      "Resolved 400 institutions...\n",
      "Resolved 400 institutions...\n",
      "Resolved 450 institutions...\n",
      "Resolved 450 institutions...\n",
      "Resolved 500 institutions...\n",
      "Resolved 500 institutions...\n",
      "Resolved 550 institutions...\n",
      "Resolved 550 institutions...\n",
      "Resolved 600 institutions...\n",
      "Resolved 600 institutions...\n",
      "Resolved 650 institutions...\n",
      "Resolved 650 institutions...\n",
      "Resolved 700 institutions...\n",
      "Resolved 700 institutions...\n",
      "Resolved 750 institutions...\n",
      "Resolved 750 institutions...\n",
      "Resolved 800 institutions...\n",
      "Resolved 800 institutions...\n",
      "Resolved 850 institutions...\n",
      "Resolved 850 institutions...\n",
      "Resolved 900 institutions...\n",
      "Resolved 900 institutions...\n",
      "Resolved 950 institutions...\n",
      "Resolved 950 institutions...\n",
      "Resolved 1000 institutions...\n",
      "Resolved 1000 institutions...\n",
      "Resolved 1050 institutions...\n",
      "Resolved 1050 institutions...\n",
      "Resolved 1100 institutions...\n",
      "Resolved 1100 institutions...\n",
      "Resolved 1150 institutions...\n",
      "Resolved 1150 institutions...\n",
      "Resolved 1200 institutions...\n",
      "Resolved 1200 institutions...\n",
      "Resolved 1250 institutions...\n",
      "Resolved 1250 institutions...\n",
      "Resolved 1300 institutions...\n",
      "Resolved 1300 institutions...\n",
      "Resolved 1350 institutions...\n",
      "Resolved 1350 institutions...\n",
      "Resolved 1400 institutions...\n",
      "Resolved 1400 institutions...\n",
      "Resolved 1450 institutions...\n",
      "Resolved 1450 institutions...\n",
      "Resolved 1500 institutions...\n",
      "Resolved 1500 institutions...\n",
      "Resolved 1550 institutions...\n",
      "Resolved 1550 institutions...\n",
      "Resolved 1600 institutions...\n",
      "Resolved 1600 institutions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1626747/2354720738.py:264: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_cb = pd.read_csv(cb_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crunchbase: could not find expected name/website columns; skipping supplemental match.\n",
      "Saved: data/enriched/openalex_authorships_with_websites.parquet\n",
      "Saved: data/enriched/df_oa_with_hamburg_flags.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'authorships_rows': 34509,\n",
       " 'institutions_with_websites': 28382,\n",
       " 'merged_rows': 278}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Affiliation enrichment run: build authorships, resolve websites, aggregate, save\n",
    "\n",
    "# 1) Build authorships from DOIs\n",
    "authorships_df = build_authorships_from_dois(df_ce_oa_institution_hamburg, doi_col=\"DOI\", limit=None, sleep_between=0.0)\n",
    "\n",
    "# 2) Resolve institution homepages from OpenAlex\n",
    "authorships_enriched_df = add_institution_homepages(authorships_df)\n",
    "\n",
    "# 3) Supplement missing websites from Crunchbase (optional, if CSV present)\n",
    "authorships_enriched_df = supplement_homepages_from_crunchbase(authorships_enriched_df, cb_csv_path=\"crunchbase_companies_DEU_Hamburg.csv\")\n",
    "\n",
    "# 4) Aggregate and merge back to the original dataframe\n",
    "merged = aggregate_and_merge(df_ce_oa_institution_hamburg, authorships_enriched_df)\n",
    "\n",
    "# 5) Save artifacts\n",
    "save_artifacts(authorships_enriched_df, merged)\n",
    "\n",
    "# Preview\n",
    "{\n",
    "    \"authorships_rows\": len(authorships_enriched_df),\n",
    "    \"institutions_with_websites\": int(authorships_enriched_df[\"inst_homepage_url\"].notna().sum()),\n",
    "    \"merged_rows\": len(merged),\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Innoscence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
