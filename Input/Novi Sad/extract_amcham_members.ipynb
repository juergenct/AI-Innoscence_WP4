{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1df5189",
   "metadata": {},
   "source": [
    "# Extract AmCham Serbia Members (name + website)\n",
    "\n",
    "This notebook scrapes all members from https://amcham.rs/members/ (all letters are on one page) and extracts:\n",
    "\n",
    "- company_name\n",
    "- website (as shown on the page)\n",
    "\n",
    "Results are previewed and saved to CSV in this folder as `amcham_members.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53f8cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiesen/.conda/envs/AI_Innoscence/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup: imports and configuration\n",
    "import re, time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "AMCHAM_URL = \"https://amcham.rs/members/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491cc333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch helper with simple retry and user-agent\n",
    "def fetch_html(url: str, timeout: int = 30, retries: int = 3, backoff: float = 1.5) -> str:\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36',\n",
    "        'Accept-Language': 'en,en-US;q=0.8',\n",
    "    }\n",
    "    err = None\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            return resp.text\n",
    "        except Exception as e:\n",
    "            err = e\n",
    "            time.sleep(backoff ** (i + 1))\n",
    "    raise RuntimeError(f\"Failed to fetch {url}: {err}\")\n",
    "\n",
    "html = fetch_html(AMCHAM_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1600fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse members: extract (company_name, website) pairs\n",
    "URL_RE = re.compile(r\"((?:https?://)?(?:www\\.)?[\\w.-]+\\.[a-z]{2,}(?:/[\\w\\-./?%&=]*)?)\", re.I)\n",
    "\n",
    "def normalize_site(site: str) -> str:\n",
    "    s = site.strip()\n",
    "    s = s.rstrip('.,;:\\u2013\\u2014')\n",
    "    if not s:\n",
    "        return s\n",
    "    if not s.lower().startswith(('http://', 'https://')):\n",
    "        s = 'https://' + s.lstrip('/')\n",
    "    s = re.sub(r'^(https?://)+(https?://)+', r'\\1', s)\n",
    "    return s\n",
    "\n",
    "def choose_container(soup: BeautifulSoup):\n",
    "    selectors = [\n",
    "        'div.entry-content',\n",
    "        'div.page-content',\n",
    "        'div.elementor-widget-text-editor',\n",
    "        'div.elementor-widget-container',\n",
    "        'main',\n",
    "        'article',\n",
    "        'div.site-content',\n",
    "        'body'\n",
    "    ]\n",
    "    best, best_score = soup.body, -1\n",
    "    for sel in selectors:\n",
    "        for node in soup.select(sel):\n",
    "            text = node.get_text('\\n', strip=True)\n",
    "            score = text.count(' | ') + text.count('www.') + len(node.find_all('a'))\n",
    "            if score > best_score:\n",
    "                best, best_score = node, score\n",
    "    return best\n",
    "\n",
    "def parse_members(html: str):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    container = choose_container(soup)\n",
    "    results = []\n",
    "    seen = set()\n",
    "    for tag in container.find_all(['p','li','div','span'], recursive=True):\n",
    "        text = tag.get_text(' ', strip=True)\n",
    "        if not text:\n",
    "            continue\n",
    "        m = URL_RE.search(text)\n",
    "        if not m:\n",
    "            continue\n",
    "        site_raw = m.group(1)\n",
    "        site = site_raw.replace('\\u200b','').strip()\n",
    "        name = None\n",
    "        a = tag.find('a')\n",
    "        if a and a.get_text(strip=True):\n",
    "            name = a.get_text(' ', strip=True)\n",
    "        if not name:\n",
    "            if '|' in text:\n",
    "                name = text.split('|', 1)[0].strip()\n",
    "            else:\n",
    "                name = text.replace(site_raw, '').strip().strip('-|:').strip()\n",
    "        if len(name) < 2:\n",
    "            continue\n",
    "        display = site\n",
    "        canonical = normalize_site(site)\n",
    "        key = (name, canonical.lower())\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        results.append({'company_name': name, 'website': display})\n",
    "    return results\n",
    "\n",
    "members = parse_members(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5bb1f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1 Srbija d.o.o. Beograd</td>\n",
       "      <td>www.a1.rs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3 Architects Studio d.o.o.</td>\n",
       "      <td>www.a3-architects.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AbbVie d.o.o. Beograd</td>\n",
       "      <td>www.abbvie.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actavis d.o.o.</td>\n",
       "      <td>www.actavis.rs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Addiko Bank</td>\n",
       "      <td>www.addiko.rs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  company_name                website\n",
       "0     A1 Srbija d.o.o. Beograd              www.a1.rs\n",
       "1  A3 Architects Studio d.o.o.  www.a3-architects.com\n",
       "2        AbbVie d.o.o. Beograd         www.abbvie.com\n",
       "3               Actavis d.o.o.         www.actavis.rs\n",
       "4                  Addiko Bank          www.addiko.rs"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean results and create DataFrame\n",
    "from pathlib import Path\n",
    "\n",
    "def domain_of(url_like: str) -> str:\n",
    "    u = url_like.strip()\n",
    "    if not u.lower().startswith(('http://','https://')):\n",
    "        u = 'https://' + u\n",
    "    try:\n",
    "        netloc = urlparse(u).netloc.lower()\n",
    "        if netloc.startswith('www.'):\n",
    "            netloc = netloc[4:]\n",
    "        return netloc\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "clean = []\n",
    "for row in members:\n",
    "    name = row['company_name'].strip()\n",
    "    site = row['website'].strip()\n",
    "    dom = domain_of(site)\n",
    "    if not dom or dom.endswith('amcham.rs'):\n",
    "        continue\n",
    "    if '@' in name or 'email' in name.lower():\n",
    "        continue\n",
    "    if re.search(r'\\b\\+?\\d[\\d\\s()./-]{5,}', name):\n",
    "        continue\n",
    "    clean.append({'company_name': name, 'website': site})\n",
    "\n",
    "df = pd.DataFrame(clean).drop_duplicates().reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b946f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 271 rows to /home/thiesen/Documents/AI-Innoscence_Ecosystem/Input/Novi Sad/amcham_members.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV in the same folder\n",
    "from pathlib import Path\n",
    "\n",
    "output_path = Path('/home/thiesen/Documents/AI-Innoscence_Ecosystem/Input/Novi Sad/amcham_members.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f'Saved {len(df)} rows to {output_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Innoscence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
