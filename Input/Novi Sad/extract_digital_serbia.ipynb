{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0169f7",
   "metadata": {},
   "source": [
    "# Digital Serbia Initiative — Members and Friends scraper\n",
    "\n",
    "This notebook scrapes the members and friends listed on:\n",
    "\n",
    "- https://www.dsi.rs/en/members/\n",
    "\n",
    "What it extracts:\n",
    "- Group: \"member\" or \"friend\"\n",
    "- Name: best-effort from logo alt/title/aria-label; falls back to domain\n",
    "- Website: external URL linked from each tile\n",
    "- Domain: normalized domain of the website\n",
    "- Source: the page URL\n",
    "\n",
    "How it works:\n",
    "- First tries a fast static parse using requests + BeautifulSoup\n",
    "- If the static parse returns 0 rows, there is an optional Playwright fallback cell you can run to render the page and extract links from the interactive tiles\n",
    "\n",
    "Outputs:\n",
    "- digital_serbia_members_friends.csv (saved next to this notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "294db1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiesen/.conda/envs/AI_Innoscence/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports and helpers\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import pandas as pd\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "})\n",
    "\n",
    "SOURCE_URL = \"https://www.dsi.rs/en/members/\"\n",
    "\n",
    "\n",
    "def fetch_html(url: str, retries: int = 3, backoff: float = 1.5, timeout: int = 20) -> str:\n",
    "    last_err = None\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            resp = SESSION.get(url, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            return resp.text\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(backoff ** i)\n",
    "    raise last_err\n",
    "\n",
    "\n",
    "def get_soup(url: str) -> BeautifulSoup:\n",
    "    html = fetch_html(url)\n",
    "    return BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "\n",
    "def canonicalize_url(url: Optional[str], base: str) -> Optional[str]:\n",
    "    if not url:\n",
    "        return None\n",
    "    u = url.strip()\n",
    "    if not u:\n",
    "        return None\n",
    "    u = urljoin(base, u)\n",
    "    parsed = urlparse(u)\n",
    "    if not parsed.scheme:\n",
    "        u = f\"https://{u}\"\n",
    "        parsed = urlparse(u)\n",
    "    if parsed.scheme not in (\"http\", \"https\"):\n",
    "        return None\n",
    "    # strip fragments\n",
    "    return parsed._replace(fragment=\"\").geturl()\n",
    "\n",
    "\n",
    "def extract_domain(url: Optional[str]) -> Optional[str]:\n",
    "    if not url:\n",
    "        return None\n",
    "    try:\n",
    "        netloc = urlparse(url).netloc.lower()\n",
    "        if netloc.startswith(\"www.\"):\n",
    "            netloc = netloc[4:]\n",
    "        return netloc or None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def looks_like_external(href: Optional[str]) -> bool:\n",
    "    if not href:\n",
    "        return False\n",
    "    href = href.strip()\n",
    "    if href.startswith(\"mailto:\") or href.startswith(\"tel:\"):\n",
    "        return False\n",
    "    # Skip on-site anchors\n",
    "    if href.startswith(\"#\"):\n",
    "        return False\n",
    "    # Consider absolute URLs external\n",
    "    parsed = urlparse(href)\n",
    "    if parsed.scheme in (\"http\", \"https\"):\n",
    "        # Treat any absolute as external; we'll still canonicalize\n",
    "        return True\n",
    "    return True  # Relative could still be external after urljoin\n",
    "\n",
    "\n",
    "def text_clean(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "\n",
    "def tile_name_from_element(el: Tag) -> Optional[str]:\n",
    "    # Try common attributes on images or anchors\n",
    "    # 1) <img alt=\"...\">\n",
    "    img = el.find(\"img\")\n",
    "    if img:\n",
    "        for key in (\"alt\", \"title\"):\n",
    "            val = img.get(key)\n",
    "            if val and text_clean(val):\n",
    "                return text_clean(val)\n",
    "    # 2) Anchor attributes\n",
    "    for a in el.find_all(\"a\"):\n",
    "        for key in (\"aria-label\", \"title\"):\n",
    "            val = a.get(key)\n",
    "            if val and text_clean(val):\n",
    "                return text_clean(val)\n",
    "    # 3) Fallback to visible text inside the tile\n",
    "    txt = text_clean(el.get_text(\" \"))\n",
    "    return txt or None\n",
    "\n",
    "\n",
    "def find_section_by_heading(soup: BeautifulSoup, title: str) -> Optional[Tag]:\n",
    "    # Find heading with this text (case-insensitive), then return the nearest container that holds the grid\n",
    "    heading = None\n",
    "    for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]):\n",
    "        if text_clean(tag.get_text()).lower() == title.lower():\n",
    "            heading = tag\n",
    "            break\n",
    "    if not heading:\n",
    "        # fallback: partial match\n",
    "        for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]):\n",
    "            if title.lower() in text_clean(tag.get_text()).lower():\n",
    "                heading = tag\n",
    "                break\n",
    "    if not heading:\n",
    "        return None\n",
    "    # Heuristic: grid is commonly in a following sibling section or div\n",
    "    container = heading.find_parent([\"section\", \"div\"]) or heading.parent\n",
    "    # Explore next siblings to find a grid-like container with many links or images\n",
    "    probe = container\n",
    "    limit = 6\n",
    "    while probe and limit > 0:\n",
    "        # Candidate if it contains many <a> or <img>\n",
    "        links = probe.find_all(\"a\")\n",
    "        imgs = probe.find_all(\"img\")\n",
    "        if len(links) >= 5 or len(imgs) >= 5:\n",
    "            return probe\n",
    "        probe = probe.find_next_sibling([\"section\", \"div\"]) or probe.find_next_sibling()\n",
    "        limit -= 1\n",
    "    return container\n",
    "\n",
    "\n",
    "def scrape_grid(soup: BeautifulSoup, base_url: str, group: str, section_title: str) -> List[Dict]:\n",
    "    sec = find_section_by_heading(soup, section_title)\n",
    "    if not sec:\n",
    "        return []\n",
    "\n",
    "    tiles: List[Tag] = []\n",
    "    # Prefer tiles that look like columns/cards\n",
    "    for cls in [\"grid\", \"row\", \"columns\", \"column\", \"col\", \"wp-block-columns\", \"wp-block-column\", \"logos\", \"partners\", \"members\"]:\n",
    "        tiles.extend(sec.find_all(class_=lambda c: isinstance(c, str) and cls in c))\n",
    "    # If the above didn't find specific wrappers, fall back to direct link parents with images\n",
    "    if not tiles:\n",
    "        for a in sec.find_all(\"a\"):\n",
    "            if a.find(\"img\"):\n",
    "                tiles.append(a.parent or a)\n",
    "\n",
    "    results: List[Dict] = []\n",
    "    seen: set[Tuple[str, str]] = set()  # (group, domain)\n",
    "\n",
    "    # Walk through anchors with external-ish hrefs\n",
    "    for a in sec.find_all(\"a\"):\n",
    "        href = a.get(\"href\")\n",
    "        if not looks_like_external(href):\n",
    "            continue\n",
    "        url = canonicalize_url(href, base_url)\n",
    "        if not url:\n",
    "            continue\n",
    "        dom = extract_domain(url)\n",
    "        if not dom:\n",
    "            continue\n",
    "        # find a reasonable tile/container name for the anchor\n",
    "        tile = a\n",
    "        # climb up to a card-like element with an image inside\n",
    "        climb = a\n",
    "        depth = 0\n",
    "        while climb and depth < 4 and not climb.find(\"img\"):\n",
    "            climb = climb.parent\n",
    "            depth += 1\n",
    "        if climb and climb.name in (\"div\", \"a\", \"figure\", \"li\"):\n",
    "            tile = climb\n",
    "        name = tile_name_from_element(tile) or dom\n",
    "\n",
    "        key = (group, dom)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        results.append({\n",
    "            \"group\": group,\n",
    "            \"name\": name,\n",
    "            \"website\": url,\n",
    "            \"domain\": dom,\n",
    "            \"source\": base_url,\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f99449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static scrape results — members: 0, friends: 0, total: 0\n",
      "No rows parsed via static HTML. Consider running the Playwright fallback cell below.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>name</th>\n",
       "      <th>website</th>\n",
       "      <th>domain</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [group, name, website, domain, source]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Static scrape: Members + Friends from DSI page\n",
    "soup = get_soup(SOURCE_URL)\n",
    "\n",
    "members = scrape_grid(soup, SOURCE_URL, group=\"member\", section_title=\"Our members\")\n",
    "friends = scrape_grid(soup, SOURCE_URL, group=\"friend\", section_title=\"Friends\")\n",
    "\n",
    "rows = members + friends\n",
    "\n",
    "# Build DataFrame and clean\n",
    "if rows:\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Deduplicate by (group, domain)\n",
    "    df = df.sort_values([\"group\", \"name\"]).drop_duplicates(subset=[\"group\", \"domain\"]).reset_index(drop=True)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\"group\", \"name\", \"website\", \"domain\", \"source\"])  # empty\n",
    "\n",
    "print(f\"Static scrape results — members: {len(members)}, friends: {len(friends)}, total: {len(df)}\")\n",
    "\n",
    "# Save CSV next to this notebook\n",
    "import os\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\".\"))  # working dir is notebook dir in VS Code\n",
    "# Prefer saving alongside this notebook path explicitly\n",
    "notebook_path = r\"/home/thiesen/Documents/AI-Innoscence_Ecosystem/Input/Novi Sad/extract_digital_serbia.ipynb\"\n",
    "output_path = os.path.join(os.path.dirname(notebook_path), \"digital_serbia_members_friends.csv\")\n",
    "\n",
    "if len(df) > 0:\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved {len(df)} rows to {output_path}\")\n",
    "else:\n",
    "    print(\"No rows parsed via static HTML. Consider running the Playwright fallback cell below.\")\n",
    "\n",
    "# Show a quick preview\n",
    "try:\n",
    "    display(df.head(20))\n",
    "except Exception:\n",
    "    print(df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8976aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional fallback: Playwright (async, notebook-safe)\n",
    "# Note: Only run if static scrape returns 0 rows or seems incomplete.\n",
    "try:\n",
    "    import asyncio\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "except Exception as e:\n",
    "    async_playwright = None\n",
    "\n",
    "async def _accept_cookies(page):\n",
    "    candidates = [\n",
    "        \"button:has-text('Accept')\",\n",
    "        \"text=Accept\",\n",
    "        \"button:has-text('I agree')\",\n",
    "        \"text=I agree\",\n",
    "        \"button:has-text('Prihvati')\",\n",
    "        \"button:has-text('Slažem se')\",\n",
    "        \"role=button[name='Accept']\",\n",
    "        \"#onetrust-accept-btn-handler\",\n",
    "        \".ot-sdk-container button\",\n",
    "    ]\n",
    "    for sel in candidates:\n",
    "        try:\n",
    "            loc = page.locator(sel)\n",
    "            if await loc.count():\n",
    "                await loc.first.click(timeout=1500)\n",
    "                await page.wait_for_timeout(300)\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "async def _extract_with_js(page, title: str, group: str):\n",
    "    js = r'''\n",
    "    (args) => {\n",
    "      const { title, group } = args;\n",
    "      function textClean(s){return (s||'').replace(/\\s+/g,' ').trim()}\n",
    "      function findHeading(title){\n",
    "        const hs = Array.from(document.querySelectorAll('h1,h2,h3,h4,h5,h6'))\n",
    "        return hs.find(h => textClean(h.textContent).toLowerCase() === title.toLowerCase()) ||\n",
    "               hs.find(h => textClean(h.textContent).toLowerCase().includes(title.toLowerCase()))\n",
    "      }\n",
    "      function sectionContainer(h){\n",
    "        if(!h) return null\n",
    "        let node = h.closest('section,div') || h.parentElement\n",
    "        return node\n",
    "      }\n",
    "      function unique(arr, keyFn){\n",
    "        const seen = new Set();\n",
    "        const out = [];\n",
    "        for(const x of arr){\n",
    "          const k = keyFn(x);\n",
    "          if(!seen.has(k)){ seen.add(k); out.push(x) }\n",
    "        }\n",
    "        return out\n",
    "      }\n",
    "      function cleanUrl(u){ try { const url = new URL(u, location.href); url.hash=''; return url.toString() } catch { return null } }\n",
    "      function domain(u){ try { return new URL(u).hostname.replace(/^www\\./,'') } catch { return null } }\n",
    "      function nameFromTile(el){\n",
    "        const img = el.querySelector('img');\n",
    "        if(img){ if(img.alt && img.alt.trim()) return img.alt.trim(); if(img.title && img.title.trim()) return img.title.trim() }\n",
    "        if(el.title && el.title.trim()) return el.title.trim();\n",
    "        if(el.getAttribute('aria-label')) return el.getAttribute('aria-label').trim();\n",
    "        const txt = textClean(el.textContent); return txt || null\n",
    "      }\n",
    "      const h = findHeading(title)\n",
    "      const container = sectionContainer(h) || document\n",
    "\n",
    "      const anchors = Array.from(container.querySelectorAll('a[href]'))\n",
    "        .filter(a => !a.getAttribute('href').startsWith('#') && !a.getAttribute('href').startsWith('mailto:') && !a.getAttribute('href').startsWith('tel:'))\n",
    "        .map(a => ({el:a, url: cleanUrl(a.getAttribute('href'))}))\n",
    "        .filter(x => x.url)\n",
    "        .map(x => ({ group, url: x.url, dom: domain(x.url), name: nameFromTile(x.el.closest('a,div,figure,li') || x.el) }))\n",
    "        .filter(x => x.dom)\n",
    "\n",
    "      const onclicks = Array.from(container.querySelectorAll('[onclick]'))\n",
    "        .map(el => ({el, onclick: el.getAttribute('onclick')||''}))\n",
    "        .filter(x => /https?:\\/\\//i.test(x.onclick))\n",
    "        .map(x => {\n",
    "          const m = x.onclick.match(/https?:\\/\\/[^'\"\\)\\s]+/i)\n",
    "          const url = m ? cleanUrl(m[0]) : null\n",
    "          return { group, url, dom: domain(url), name: nameFromTile(x.el.closest('a,div,figure,li') || x.el) }\n",
    "        })\n",
    "        .filter(x => x.url && x.dom)\n",
    "\n",
    "      const all = anchors.concat(onclicks)\n",
    "      const final = unique(all, x => group + '|' + (x.dom||'') )\n",
    "      return final\n",
    "    }\n",
    "    '''\n",
    "    return await page.evaluate(js, {\"title\": title, \"group\": group})\n",
    "\n",
    "async def _hover_extract_urls(page, title: str, group: str):\n",
    "    results = []\n",
    "    tiles = page.locator(\n",
    "        f\"xpath=(//h1[contains(., '{title}')] | //h2[contains(., '{title}')] | //h3[contains(., '{title}')])\"\n",
    "        \" /ancestor-or-self::*[self::section or self::div][1]//*[img]\")\n",
    "    n = await tiles.count()\n",
    "    for i in range(n):\n",
    "        t = tiles.nth(i)\n",
    "        try:\n",
    "            await t.scroll_into_view_if_needed()\n",
    "            await t.hover()\n",
    "            links = await t.evaluate_all(\"(nodes) => nodes.map(n => Array.from(n.querySelectorAll('a[href]')).map(a => a.href)).flat()\")\n",
    "            for url in links:\n",
    "                dom = extract_domain(url)\n",
    "                if not dom:\n",
    "                    continue\n",
    "                results.append({\"group\": group, \"website\": url, \"domain\": dom, \"name\": dom})\n",
    "        except Exception:\n",
    "            continue\n",
    "    uniq = {}\n",
    "    for r in results:\n",
    "        uniq[(r['group'], r['domain'])] = r\n",
    "    return list(uniq.values())\n",
    "\n",
    "async def _click_collect_urls(page, title: str, group: str):\n",
    "    collected = []\n",
    "    heading = page.locator(f\"xpath=//h1[contains(., '{title}')] | //h2[contains(., '{title}')] | //h3[contains(., '{title}')] \").first\n",
    "    if not await heading.count():\n",
    "        return collected\n",
    "    images = page.locator(\n",
    "        f\"xpath=(//h1[contains(., '{title}')] | //h2[contains(., '{title}')] | //h3[contains(., '{title}')])\"\n",
    "        \" /ancestor-or-self::*[self::section or self::div][1]//img\")\n",
    "    n = await images.count()\n",
    "    for i in range(n):\n",
    "        img = images.nth(i)\n",
    "        try:\n",
    "            await img.scroll_into_view_if_needed()\n",
    "            try:\n",
    "                box = await img.bounding_box()\n",
    "                if box:\n",
    "                    await page.mouse.move(box['x'] + box['width']/2, box['y'] + box['height']/2)\n",
    "                    await page.wait_for_timeout(100)\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                async with page.expect_event(\"popup\", timeout=1500) as pop_wait:\n",
    "                    await img.click(force=True)\n",
    "                popup = await pop_wait.value\n",
    "                await popup.wait_for_load_state(\"domcontentloaded\")\n",
    "                url = popup.url\n",
    "                await popup.close()\n",
    "                collected.append({\"group\": group, \"url\": url})\n",
    "                continue\n",
    "            except PlaywrightTimeoutError:\n",
    "                pass\n",
    "            try:\n",
    "                async with page.expect_navigation(timeout=1500):\n",
    "                    await img.click(force=True)\n",
    "                url = page.url\n",
    "                await page.goto(SOURCE_URL, wait_until=\"networkidle\")\n",
    "                await _accept_cookies(page)\n",
    "                collected.append({\"group\": group, \"url\": url})\n",
    "            except PlaywrightTimeoutError:\n",
    "                pass\n",
    "        except Exception:\n",
    "            continue\n",
    "    seen = set(); out=[]\n",
    "    for x in collected:\n",
    "        url = x.get(\"url\"); dom = extract_domain(url)\n",
    "        if not url or not dom: continue\n",
    "        if (group, dom) in seen: continue\n",
    "        seen.add((group, dom))\n",
    "        out.append({\"group\": group, \"website\": url, \"domain\": dom, \"name\": dom})\n",
    "    return out\n",
    "\n",
    "async def scrape_with_playwright(url: str) -> pd.DataFrame:\n",
    "    if async_playwright is None:\n",
    "        print(\"Playwright not available; install 'playwright' and run 'playwright install' to enable fallback.\")\n",
    "        return pd.DataFrame(columns=[\"group\", \"name\", \"website\", \"domain\", \"source\"])  \n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, wait_until=\"networkidle\")\n",
    "        await _accept_cookies(page)\n",
    "        try:\n",
    "            await page.wait_for_timeout(1500)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        data_members = await _extract_with_js(page, \"Our members\", \"member\")\n",
    "        data_friends = await _extract_with_js(page, \"Friends\", \"friend\")\n",
    "        data = (data_members or []) + (data_friends or [])\n",
    "\n",
    "        if not data:\n",
    "            data = await _hover_extract_urls(page, \"Our members\", \"member\") + await _hover_extract_urls(page, \"Friends\", \"friend\")\n",
    "        if not data:\n",
    "            data = await _click_collect_urls(page, \"Our members\", \"member\") + await _click_collect_urls(page, \"Friends\", \"friend\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "        if not data:\n",
    "            return pd.DataFrame(columns=[\"group\", \"name\", \"website\", \"domain\", \"source\"])  \n",
    "        df = pd.DataFrame(data)\n",
    "        if \"website\" not in df.columns and \"url\" in df.columns:\n",
    "            df.rename(columns={\"url\": \"website\"}, inplace=True)\n",
    "        if \"domain\" not in df.columns:\n",
    "            df[\"domain\"] = df[\"website\"].map(extract_domain)\n",
    "        if \"name\" not in df.columns:\n",
    "            df[\"name\"] = df[\"domain\"]\n",
    "        df[\"name\"] = df[\"name\"].fillna(df[\"domain\"]).replace(\"\", pd.NA).fillna(df[\"domain\"]) \n",
    "        df[\"source\"] = url\n",
    "        df = df.sort_values([\"group\", \"name\"]).drop_duplicates(subset=[\"group\", \"domain\"]).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "# Helper to run fallback when needed\n",
    "async def maybe_run_playwright_if_needed(current_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if len(current_df) > 0:\n",
    "        return current_df\n",
    "    print(\"Attempting Playwright fallback…\")\n",
    "    df2 = await scrape_with_playwright(SOURCE_URL)\n",
    "    if len(df2) > 0:\n",
    "        import os\n",
    "        notebook_path = r\"/home/thiesen/Documents/AI-Innoscence_Ecosystem/Input/Novi Sad/extract_digital_serbia.ipynb\"\n",
    "        output_path = os.path.join(os.path.dirname(notebook_path), \"digital_serbia_members_friends.csv\")\n",
    "        df2.to_csv(output_path, index=False)\n",
    "        print(f\"Saved {len(df2)} rows to {output_path} (Playwright fallback)\")\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf6e55a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting Playwright fallback…\n",
      "Final row count: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>name</th>\n",
       "      <th>website</th>\n",
       "      <th>domain</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [group, name, website, domain, source]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If static parse returned 0 rows, you may run this to trigger the fallback automatically\n",
    "# (This cell is safe to run regardless; it only uses Playwright when needed.)\n",
    "try:\n",
    "    loop = asyncio.get_event_loop()\n",
    "except Exception:\n",
    "    loop = None\n",
    "\n",
    "if 'df' in globals():\n",
    "    if async_playwright is None and len(df) == 0:\n",
    "        print(\"Playwright not installed; static scrape returned 0 rows. Install fallback if needed:\")\n",
    "        print(\"  pip install playwright nest_asyncio\")\n",
    "        print(\"  python -m playwright install\")\n",
    "    elif len(df) == 0:\n",
    "        if loop and loop.is_running():\n",
    "            import nest_asyncio as _na\n",
    "            _na.apply()\n",
    "            df = await maybe_run_playwright_if_needed(df)\n",
    "        else:\n",
    "            df = loop.run_until_complete(maybe_run_playwright_if_needed(df)) if loop else asyncio.run(maybe_run_playwright_if_needed(df))\n",
    "else:\n",
    "    print(\"No 'df' found from the static scrape cell — please run the previous cell first.\")\n",
    "\n",
    "# Preview (again) if available\n",
    "if 'df' in globals():\n",
    "    print(f\"Final row count: {len(df)}\")\n",
    "    try:\n",
    "        display(df.head(20))\n",
    "    except Exception:\n",
    "        print(df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dff2203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headings found: ['Our members', 'Friends']\n",
      "Iframes found: 0\n"
     ]
    }
   ],
   "source": [
    "# Debug: Inspect HTML structure with Playwright (iframes too)\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "async def debug_dump():\n",
    "    if async_playwright is None:\n",
    "        print(\"Playwright not available.\")\n",
    "        return\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(SOURCE_URL, wait_until=\"networkidle\")\n",
    "        await page.wait_for_timeout(1000)\n",
    "        html = await page.content()\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        headings = [h.get_text(strip=True) for h in soup.select('h1,h2,h3,h4,h5,h6')]\n",
    "        print(\"Headings found:\", [h for h in headings if 'member' in h.lower() or 'friend' in h.lower()][:10])\n",
    "        # Iframes\n",
    "        iframes = soup.select('iframe')\n",
    "        print(\"Iframes found:\", len(iframes))\n",
    "        for i, fr in enumerate(iframes[:5], 1):\n",
    "            print(f\"  iframe {i} src=\", fr.get('src'))\n",
    "        await browser.close()\n",
    "\n",
    "try:\n",
    "    if loop and loop.is_running():\n",
    "        await debug_dump()\n",
    "    else:\n",
    "        asyncio.run(debug_dump())\n",
    "except Exception as e:\n",
    "    print(\"Debug failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1670b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.dsi.rs/wp-json 200 application/json; charset=UTF-8\n",
      "  sample: {\n",
      "  \"name\": \"Inicijativa \\u201eDigitalna Srbija\\u201c\",\n",
      "  \"description\": \"\",\n",
      "  \"url\": \"https://www.dsi.rs\",\n",
      "  \"home\": \"https://www.dsi.rs\",\n",
      "  \"gmt_offset\": 1,\n",
      "  \"timezone_string\": \"Europe/Belgrade\",\n",
      "  \"namespaces\": [\n",
      "    \"oembed/1.0\",\n",
      "    \"akismet/v1\",\n",
      "    \"contact-form-7/v1\",\n",
      "    \"disqus/v1\",\n",
      "    \" ...\n",
      "https://www.dsi.rs/wp-json/wp/v2/pages?search=members 200 application/json; charset=UTF-8\n",
      "  sample: [] ...\n",
      "https://www.dsi.rs/en/wp-json/wp/v2/pages?search=members 200 application/json; charset=UTF-8\n",
      "  sample: [\n",
      "  {\n",
      "    \"id\": 9044,\n",
      "    \"date\": \"2019-12-20T10:27:59\",\n",
      "    \"date_gmt\": \"2019-12-20T09:27:59\",\n",
      "    \"guid\": {\n",
      "      \"rendered\": \"https://www.dsi.rs/politika-privatnosti-inicijative-digitalna-srbija/\"\n",
      "    },\n",
      "    \"modified\": \"2025-10-08T11:03:52\",\n",
      "    \"modified_gmt\": \"2025-10-08T09:03:52\",\n",
      "    \"slug\": ...\n",
      "https://www.dsi.rs/wp-json/wp/v2/pages?per_page=5 200 application/json; charset=UTF-8\n",
      "  sample: [\n",
      "  {\n",
      "    \"id\": 18078,\n",
      "    \"date\": \"2025-09-19T11:12:21\",\n",
      "    \"date_gmt\": \"2025-09-19T09:12:21\",\n",
      "    \"guid\": {\n",
      "      \"rendered\": \"https://www.dsi.rs/?page_id=18078\"\n",
      "    },\n",
      "    \"modified\": \"2025-09-24T01:25:00\",\n",
      "    \"modified_gmt\": \"2025-09-23T23:25:00\",\n",
      "    \"slug\": \"intervjui\",\n",
      "    \"status\": \"publis ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'https://www.dsi.rs/wp-json': '{\\n  \"name\": \"Inicijativa \\\\u201eDigitalna Srbija\\\\u201c\",\\n  \"description\": \"\",\\n  \"url\": \"https://www.dsi.rs\",\\n  \"home\": \"https://www.dsi.rs\",\\n  \"gmt_offset\": 1,\\n  \"timezone_string\": \"Europe/Belgrade\",\\n  \"namespaces\": [\\n    \"oembed/1.0\",\\n    \"akismet/v1\",\\n    \"contact-form-7/v1\",\\n    \"disqus/v1\",\\n    \"mailin/v1\",\\n    \"wordfence/v1\",\\n    \"yoast/v1\",\\n    \"wpml/tm/v1\",\\n    \"dsi\",\\n    \"regenerate-thumbnails/v1\",\\n    \"wp/v2\"\\n  ],\\n  \"authentication\": [],\\n  \"routes\": {\\n    \"/\": {\\n      \"namespace\": \"\",\\n      \"methods\": [\\n        \"GET\"\\n      ],\\n      \"endpoints\": [\\n        {\\n          \"methods\": [\\n            \"GET\"\\n          ],\\n          \"args\": {\\n            \"context\": {\\n              \"required\": false,\\n              \"default\": \"view\"\\n            }\\n          }\\n        }\\n      ],\\n      \"_links\": {\\n        \"self\": \"https://www.dsi.rs/wp-json/\"\\n      }\\n    },\\n    \"/oembed/1.0\": {\\n      \"namespace\": \"oembed/1.0\",\\n      \"methods\": [\\n        \"GET\"\\n      ],\\n      \"endpoints\": [\\n        {\\n          \"met',\n",
       " 'https://www.dsi.rs/wp-json/wp/v2/pages?search=members': '[]',\n",
       " 'https://www.dsi.rs/en/wp-json/wp/v2/pages?search=members': '[\\n  {\\n    \"id\": 9044,\\n    \"date\": \"2019-12-20T10:27:59\",\\n    \"date_gmt\": \"2019-12-20T09:27:59\",\\n    \"guid\": {\\n      \"rendered\": \"https://www.dsi.rs/politika-privatnosti-inicijative-digitalna-srbija/\"\\n    },\\n    \"modified\": \"2025-10-08T11:03:52\",\\n    \"modified_gmt\": \"2025-10-08T09:03:52\",\\n    \"slug\": \"privacy-policy\",\\n    \"status\": \"publish\",\\n    \"type\": \"page\",\\n    \"link\": \"https://www.dsi.rs/en/privacy-policy/\",\\n    \"title\": {\\n      \"rendered\": \"Privacy policy of the Digital Serbia Initiative Association\"\\n    },\\n    \"content\": {\\n      \"rendered\": \"<p>[vc_row css=&#8221;.vc_custom_1576834767699{padding-bottom: 50px !important;}&#8221;][vc_column][vc_column_text]<span style=\\\\\"color: #353556; font-size: 20px; line-height: 28px;\\\\\">Updated on 8 October 2025</span></p>\\\\n<p>The Digital Serbia Initiative Association, with its registered seat at Milutina Milenkovi\\\\u0107a 11a, 11000 Belgrade, Republic of Serbia, company number: 28231083, tax ID (PIB): 110058888 (hereinafter: the \\\\u201c<strong>A',\n",
       " 'https://www.dsi.rs/wp-json/wp/v2/pages?per_page=5': '[\\n  {\\n    \"id\": 18078,\\n    \"date\": \"2025-09-19T11:12:21\",\\n    \"date_gmt\": \"2025-09-19T09:12:21\",\\n    \"guid\": {\\n      \"rendered\": \"https://www.dsi.rs/?page_id=18078\"\\n    },\\n    \"modified\": \"2025-09-24T01:25:00\",\\n    \"modified_gmt\": \"2025-09-23T23:25:00\",\\n    \"slug\": \"intervjui\",\\n    \"status\": \"publish\",\\n    \"type\": \"page\",\\n    \"link\": \"https://www.dsi.rs/karijera40/intervjui/\",\\n    \"title\": {\\n      \"rendered\": \"Upoznaj zanimanja mentora\"\\n    },\\n    \"content\": {\\n      \"rendered\": \"<p>[vc_row full_width=&#8220;stretch_row&#8220; el_class=&#8220;karijera40-2025-header-intervjui&#8220;][vc_column][vc_raw_html]JTNDaDElM0VVcG96bmFqJTIwemFuaW1hbmphJTIwbWVudG9yYSUzQyUyRmgxJTNFJTBBJTNDaW1nJTIwc3JjJTNEJTIyaHR0cHMlM0ElMkYlMkZ3d3cuZHNpLnJzJTJGd3AtY29udGVudCUyRnRoZW1lcyUyRmZpbmFuY2Vwcm8tY2hpbGQlMkZhc3NldHMlMkZpbWclMkZrYXJpamVyYTQwLTIwMjUlMkZrYXJpamVyYTQwLWxvZ28ucG5nJTIyJTIwYWx0JTNEJTIyS2FyaWplcmElMjA0LjAlMjBtZW50b3Jza2klMjBwcm9ncmFtJTIyJTNF[/vc_raw_html][/vc_column][/vc_row][vc_row css=&#8220;.vc_cu'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternate approach: probe WordPress REST API for members data\n",
    "import json\n",
    "\n",
    "endpoints_to_try = [\n",
    "    \"https://www.dsi.rs/wp-json\",\n",
    "    \"https://www.dsi.rs/wp-json/wp/v2/pages?search=members\",\n",
    "    \"https://www.dsi.rs/en/wp-json/wp/v2/pages?search=members\",\n",
    "    \"https://www.dsi.rs/wp-json/wp/v2/pages?per_page=5\",\n",
    "]\n",
    "responses = {}\n",
    "for url in endpoints_to_try:\n",
    "    try:\n",
    "        r = SESSION.get(url, timeout=20)\n",
    "        print(url, r.status_code, r.headers.get('content-type'))\n",
    "        if r.ok and 'application/json' in (r.headers.get('content-type') or ''):\n",
    "            data = r.json()\n",
    "            # Keep it small for display\n",
    "            snippet = json.dumps(data[:1] if isinstance(data, list) else data, indent=2)[:1000]\n",
    "            responses[url] = snippet\n",
    "            print(\"  sample:\", snippet[:300], '...')\n",
    "    except Exception as e:\n",
    "        print(url, 'error', e)\n",
    "\n",
    "responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50a454c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wp-json/dsi 200\n",
      "{\"namespace\":\"dsi\",\"routes\":{\"\\/dsi\":{\"namespace\":\"dsi\",\"methods\":[\"GET\"],\"endpoints\":[{\"methods\":[\"GET\"],\"args\":{\"namespace\":{\"required\":false,\"default\":\"dsi\"},\"context\":{\"required\":false,\"default\":\"view\"}}}],\"_links\":{\"self\":\"https:\\/\\/www.dsi.rs\\/wp-json\\/dsi\"}},\"\\/dsi\\/random-posts\":{\"namespace\":\"dsi\",\"methods\":[\"GET\"],\"endpoints\":[{\"methods\":[\"GET\"],\"args\":[]}],\"_links\":{\"self\":\"https:\\/\\/www.dsi.rs\\/wp-json\\/dsi\\/random-posts\"}}},\"_links\":{\"up\":[{\"href\":\"https:\\/\\/www.dsi.rs\\/wp-json\\/\"}]}}\n"
     ]
    }
   ],
   "source": [
    "# Explore custom REST namespace `dsi`\n",
    "try:\n",
    "    r = SESSION.get(\"https://www.dsi.rs/wp-json/dsi\", timeout=20)\n",
    "    print(\"/wp-json/dsi\", r.status_code)\n",
    "    print((r.text or \"\")[:800])\n",
    "except Exception as e:\n",
    "    print(\"dsi namespace fetch error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b918ea95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interesting network calls (first 15):\n",
      "   stylesheet https://fonts.googleapis.com/css?family=Open+Sans%3A400%2C600%2C700%7CRoboto%3A400%2C500%2C700&ver=1.6\n",
      "   script https://www.dsi.rs/wp-content/plugins/wdv-mailchimp-ajax//assets/js/wdv_mailchimp_ajaxcall.js?ver=1.0\n",
      "   script https://www.dsi.rs/wp-content/plugins/mc4wp-premium/ajax-forms/assets/js/ajax-forms.min.js?ver=4.5.13\n",
      "   script https://www.dsi.rs/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.5.3\n",
      "   script https://ajax.googleapis.com/ajax/libs/webfont/1.5.3/webfont.js\n",
      "   stylesheet https://fonts.googleapis.com/css?family=Exo+2:500,400,700%7COpen+Sans:400%7CExo+2:500italic,600italic,700italic\n",
      "   xhr https://www.dsi.rs/wp-admin/admin-ajax.php\n",
      "   stylesheet https://fonts.googleapis.com/css?family=Open+Sans%3A400%2C600%2C700%7CRoboto%3A400%2C500%2C700&ver=1.6\n",
      "   script https://www.dsi.rs/wp-content/plugins/wdv-mailchimp-ajax//assets/js/wdv_mailchimp_ajaxcall.js?ver=1.0\n",
      "   script https://www.dsi.rs/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.5.3\n",
      "   script https://www.dsi.rs/wp-content/plugins/mc4wp-premium/ajax-forms/assets/js/ajax-forms.min.js?ver=4.5.13\n",
      "   script https://ajax.googleapis.com/ajax/libs/webfont/1.5.3/webfont.js\n",
      "   stylesheet https://fonts.googleapis.com/css?family=Exo+2:500,400,700%7COpen+Sans:400%7CExo+2:500italic,600italic,700italic\n",
      "   stylesheet https://fonts.googleapis.com/css2?family=Exo+2:ital,wght@0,300;1,100;1,900&display=swap\n",
      "   fetch https://region1.google-analytics.com/g/collect?v=2&tid=G-ZMTV390D5E&gtm=45je5b31v9107722934za200zd9107722934&_p=1762357845773&gcd=13l3l3l2l1l1&npa=1&dma_cps=syphamo&dma=1&cid=2032730999.1762357846&ul=en-us&sr=1280x720&ir=1&uaa=x86&uab=64&uafvl=Chromium%3B140.0.7339.16%7CNot%253DA%253FBrand%3B24.0.0.0%7CHeadlessChrome%3B140.0.7339.16&uamb=0&uam=&uap=Linux&uapv=6.14.5&uaw=0&are=1&frm=0&pscdl=noapi&_eu=EA&_s=1&tag_exp=101509157~103116026~103200004~103233427~104527906~104528500~104684208~104684211~104948813~115480710~115583767~115938465~115938469~116217636~116217638&sid=1762357845&sct=1&seg=0&dl=https%3A%2F%2Fwww.dsi.rs%2Fen%2Fmembers%2F&dr=https%3A%2F%2Fwww.dsi.rs%2Fen%2Fmembers%2F&dt=Companies%20and%20organisations%20we%20gather%20%7C%20Digital%20Serbia%20Initiative&en=page_view&_fv=1&_nsi=1&_ss=1&_ee=1&tfd=4099\n"
     ]
    }
   ],
   "source": [
    "# Debug: capture network requests on page load to discover data endpoints\n",
    "async def debug_network():\n",
    "    if async_playwright is None:\n",
    "        print(\"Playwright not available.\")\n",
    "        return\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        events = []\n",
    "        page.on(\"response\", lambda resp: events.append((resp.url, resp.request.resource_type)))\n",
    "        await page.goto(SOURCE_URL, wait_until=\"networkidle\")\n",
    "        await _accept_cookies(page)\n",
    "        await page.wait_for_timeout(1200)\n",
    "        # Filter interesting URLs\n",
    "        interesting = [\n",
    "            (u,t) for (u,t) in events\n",
    "            if any(k in u for k in [\"admin-ajax.php\", \"wp-json\", \"api\", \"ajax\", \"json\"])\n",
    "        ]\n",
    "        print(\"Interesting network calls (first 15):\")\n",
    "        for u, t in interesting[:15]:\n",
    "            print(\"  \", t, u)\n",
    "        await browser.close()\n",
    "\n",
    "try:\n",
    "    if loop and loop.is_running():\n",
    "        await debug_network()\n",
    "    else:\n",
    "        asyncio.run(debug_network())\n",
    "except Exception as e:\n",
    "    print(\"Network debug failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76a6bea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admin-ajax responses: 1\n",
      "URL: https://www.dsi.rs/wp-admin/admin-ajax.php\n",
      "Method: POST\n",
      "POST data: action=save_consent_log&extras=%7B%22strict%22%3A%221%22%2C%22thirdparty%22%3A%221%22%2C%22advanced%22%3A%221%22%2C%22version%22%3A%221%22%7D\n",
      "Re-fetched status: 200\n",
      "Re-fetched snippet: \n"
     ]
    }
   ],
   "source": [
    "# Deep network inspect: capture admin-ajax payload and response\n",
    "async def inspect_admin_ajax():\n",
    "    if async_playwright is None:\n",
    "        print(\"Playwright not available.\")\n",
    "        return\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        captured = []\n",
    "        page.on(\"response\", lambda resp: captured.append(resp))\n",
    "        await page.goto(SOURCE_URL, wait_until=\"networkidle\")\n",
    "        await _accept_cookies(page)\n",
    "        await page.wait_for_timeout(1500)\n",
    "        targets = [r for r in captured if 'admin-ajax.php' in r.url]\n",
    "        print(\"admin-ajax responses:\", len(targets))\n",
    "        for resp in targets[:3]:\n",
    "            try:\n",
    "                req = resp.request\n",
    "                body = req.post_data or ''\n",
    "                print(\"URL:\", resp.url)\n",
    "                print(\"Method:\", req.method)\n",
    "                print(\"POST data:\", (body[:500] if body else '(none)'))\n",
    "                # Try to get text via fetch if method+params identifiable\n",
    "                if req.method == 'POST' and body:\n",
    "                    import urllib.parse as _up\n",
    "                    parsed = dict(_up.parse_qsl(body))\n",
    "                    r2 = SESSION.post(resp.url, data=parsed, timeout=20)\n",
    "                    print(\"Re-fetched status:\", r2.status_code)\n",
    "                    print(\"Re-fetched snippet:\", (r2.text[:500] if r2.text else ''))\n",
    "            except Exception as e:\n",
    "                print(\"inspect error:\", e)\n",
    "        await browser.close()\n",
    "\n",
    "try:\n",
    "    if loop and loop.is_running():\n",
    "        await inspect_admin_ajax()\n",
    "    else:\n",
    "        asyncio.run(inspect_admin_ajax())\n",
    "except Exception as e:\n",
    "    print(\"Admin-ajax inspect failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e307a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our members tiles with URL-like attributes: 0\n",
      "Friends tiles with URL-like attributes: 0\n"
     ]
    }
   ],
   "source": [
    "# Debug: scan attributes for URL-like values within Members/Friends tiles\n",
    "async def debug_scan_attributes():\n",
    "    if async_playwright is None:\n",
    "        print(\"Playwright not available.\")\n",
    "        return\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(SOURCE_URL, wait_until=\"networkidle\")\n",
    "        await _accept_cookies(page)\n",
    "        await page.wait_for_timeout(1200)\n",
    "        js = r'''\n",
    "        (title) => {\n",
    "          function textClean(s){return (s||'').replace(/\\s+/g,' ').trim()}\n",
    "          function findHeading(title){\n",
    "            const hs = Array.from(document.querySelectorAll('h1,h2,h3,h4,h5,h6'))\n",
    "            return hs.find(h => textClean(h.textContent).toLowerCase() === title.toLowerCase()) ||\n",
    "                   hs.find(h => textClean(h.textContent).toLowerCase().includes(title.toLowerCase()))\n",
    "          }\n",
    "          const h = findHeading(title)\n",
    "          const container = h ? (h.closest('section,div') || h.parentElement) : document\n",
    "          const tiles = Array.from(container.querySelectorAll('*')).filter(el => el.querySelector && el.querySelector('img'))\n",
    "          const out = []\n",
    "          const urlRe = /https?:\\/\\//i\n",
    "          for(const el of tiles.slice(0,200)){\n",
    "            const attrs = el.getAttributeNames ? el.getAttributeNames() : []\n",
    "            for(const name of attrs){\n",
    "              const val = el.getAttribute(name)\n",
    "              if(typeof val === 'string' && urlRe.test(val)){\n",
    "                out.push({tag: el.tagName.toLowerCase(), attr: name, val})\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "          return out\n",
    "        }\n",
    "        '''\n",
    "        for section in [\"Our members\", \"Friends\"]:\n",
    "            data = await page.evaluate(js, section)\n",
    "            print(section, \"tiles with URL-like attributes:\", len(data))\n",
    "            for row in data[:5]:\n",
    "                print(\"  \", row)\n",
    "        await browser.close()\n",
    "\n",
    "try:\n",
    "    if loop and loop.is_running():\n",
    "        await debug_scan_attributes()\n",
    "    else:\n",
    "        asyncio.run(debug_scan_attributes())\n",
    "except Exception as e:\n",
    "    print(\"Attribute scan failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9e3947f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types status: 200\n",
      "Known types: ['post', 'page', 'attachment']\n",
      "Candidate custom types: []\n"
     ]
    }
   ],
   "source": [
    "# Discover WP post types to find a custom type for members/friends\n",
    "try:\n",
    "    r = SESSION.get(\"https://www.dsi.rs/wp-json/wp/v2/types\", timeout=20)\n",
    "    print(\"types status:\", r.status_code)\n",
    "    data = r.json()\n",
    "    print(\"Known types:\", list(data.keys())[:20])\n",
    "    # Look for anything with 'member' or similar\n",
    "    matches = {k:v for k,v in data.items() if any(x in k for x in ['member','partner','friend','company','logo'])}\n",
    "    print(\"Candidate custom types:\", list(matches.keys()))\n",
    "except Exception as e:\n",
    "    print(\"types fetch error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd9a5c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted rows with spatial filtering: total=35 | members=22 | friends=13\n",
      "Wrote /home/thiesen/Documents/AI-Innoscence_Ecosystem/Input/Novi Sad/digital_serbia_members_friends.csv\n"
     ]
    }
   ],
   "source": [
    "# Robust spatial/hover-based extractor for DSI Members & Friends\n",
    "# Strategy: filter anchors by vertical ranges between headings, infer names from image alts/labels,\n",
    "# and save CSV. Falls back to providing a headful hint if nothing is found in headless mode.\n",
    "\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "async def scrape_dsi_members_friends_spatial(headless: bool = True):\n",
    "    \"\"\"Collect anchors within the vertical ranges of 'Our members' and 'Friends'.\n",
    "    Returns dict with lists under keys 'mem' and 'fri'.\"\"\"\n",
    "    try:\n",
    "        from playwright.async_api import async_playwright\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Playwright is required for this cell. Please install it in the current kernel.\") from e\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=headless, args=[\"--no-sandbox\", \"--disable-dev-shm-usage\"])  # headful optional\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        await page.goto(SOURCE_URL, wait_until=\"domcontentloaded\", timeout=60000)\n",
    "\n",
    "        # Dismiss cookie banner if present\n",
    "        for selector in [\n",
    "            \"button:has-text('Accept')\",\n",
    "            \"#cn-accept-cookie\",\n",
    "            \"button[aria-label='Accept']\",\n",
    "            \"//button[contains(., 'Accept')]\",\n",
    "        ]:\n",
    "            try:\n",
    "                await page.locator(selector).first.click(timeout=2000)\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Let lazy assets settle\n",
    "        try:\n",
    "            await page.wait_for_load_state(\"networkidle\", timeout=15000)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        data = await page.evaluate(\n",
    "            \"\"\"\n",
    "            () => {\n",
    "                const y = (el) => el.getBoundingClientRect().top + window.scrollY;\n",
    "                const headings = Array.from(document.querySelectorAll('h1, h2, h3'));\n",
    "                const findH = (txt) => headings.find(h => (h.textContent || '').trim().toLowerCase().includes(txt));\n",
    "\n",
    "                const hMembers = findH('our members');\n",
    "                const hFriends = findH('friends');\n",
    "\n",
    "                const topMembers = hMembers ? y(hMembers) : 0;\n",
    "                const topFriends = hFriends ? y(hFriends) : Number.POSITIVE_INFINITY;\n",
    "                const bottomMembers = hFriends ? y(hFriends) : document.body.scrollHeight;\n",
    "                const bottomFriends = document.body.scrollHeight;\n",
    "\n",
    "                const all = Array.from(document.querySelectorAll('a[href]'));\n",
    "\n",
    "                const nameFor = (a) => {\n",
    "                    const altIn = a.querySelector('img[alt]');\n",
    "                    if (altIn && altIn.alt && altIn.alt.trim()) return altIn.alt.trim();\n",
    "                    let node = a;\n",
    "                    for (let i = 0; i < 3 && node; i++) {\n",
    "                        const img = node.querySelector && node.querySelector('img[alt]');\n",
    "                        if (img && img.alt && img.alt.trim()) return img.alt.trim();\n",
    "                        node = node.parentElement;\n",
    "                    }\n",
    "                    const label = a.getAttribute('aria-label') || a.title || (a.textContent || '').trim();\n",
    "                    return label || null;\n",
    "                };\n",
    "\n",
    "                const collect = (rangeTop, rangeBottom, label) => {\n",
    "                    const rows = [];\n",
    "                    for (const a of all) {\n",
    "                        const href = a.href;\n",
    "                        if (!href) continue;\n",
    "                        if (href.startsWith('mailto:') || href.startsWith('tel:')) continue;\n",
    "                        const rect = a.getBoundingClientRect();\n",
    "                        const ymid = (rect.top + rect.bottom) / 2 + window.scrollY;\n",
    "                        if (isNaN(ymid)) continue;\n",
    "                        if (ymid < rangeTop || ymid > rangeBottom) continue;\n",
    "                        rows.push({ group: label, name: nameFor(a), website: href, y: ymid, w: rect.width, h: rect.height });\n",
    "                    }\n",
    "                    return rows;\n",
    "                };\n",
    "\n",
    "                const mem = collect(topMembers, bottomMembers, 'members');\n",
    "                const fri = collect(topFriends, bottomFriends, 'friends');\n",
    "                return { mem, fri };\n",
    "            }\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        await browser.close()\n",
    "        return data\n",
    "\n",
    "# Run the spatial extraction in headless mode first\n",
    "try:\n",
    "    res = asyncio.get_event_loop().run_until_complete(scrape_dsi_members_friends_spatial(headless=True))\n",
    "except RuntimeError as err:\n",
    "    print(str(err))\n",
    "    res = {\"mem\": [], \"fri\": []}\n",
    "\n",
    "members, friends = res.get(\"mem\", []), res.get(\"fri\", [])\n",
    "\n",
    "# Utility helpers\n",
    "\n",
    "def extract_domain(u: str) -> str:\n",
    "    try:\n",
    "        host = urlparse(u).netloc.lower()\n",
    "        return host[4:] if host.startswith(\"www.\") else host\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def is_external(u: str) -> bool:\n",
    "    try:\n",
    "        netloc = urlparse(u).netloc.lower()\n",
    "        return netloc and 'dsi.rs' not in netloc\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "rows = []\n",
    "for rec in (members or []):\n",
    "    href = rec.get(\"website\")\n",
    "    if href and is_external(href):\n",
    "        rows.append({\n",
    "            \"group\": \"members\",\n",
    "            \"name\": (rec.get(\"name\") or \"\").strip(),\n",
    "            \"website\": href,\n",
    "            \"domain\": extract_domain(href),\n",
    "            \"source\": SOURCE_URL,\n",
    "        })\n",
    "for rec in (friends or []):\n",
    "    href = rec.get(\"website\")\n",
    "    if href and is_external(href):\n",
    "        rows.append({\n",
    "            \"group\": \"friends\",\n",
    "            \"name\": (rec.get(\"name\") or \"\").strip(),\n",
    "            \"website\": href,\n",
    "            \"domain\": extract_domain(href),\n",
    "            \"source\": SOURCE_URL,\n",
    "        })\n",
    "\n",
    "# Dedupe by (group, domain)\n",
    "seen = set()\n",
    "dedup = []\n",
    "for r in rows:\n",
    "    key = (r[\"group\"], r[\"domain\"])\n",
    "    if not r[\"domain\"] or key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    dedup.append(r)\n",
    "\n",
    "print(f\"Extracted rows with spatial filtering: total={len(dedup)} | members={sum(1 for r in dedup if r['group']=='members')} | friends={sum(1 for r in dedup if r['group']=='friends')}\")\n",
    "\n",
    "if dedup:\n",
    "    pd.DataFrame(dedup).to_csv(output_path, index=False)\n",
    "    print(f\"Wrote {output_path}\")\n",
    "else:\n",
    "    print(\"No rows found yet. If your environment supports it, try running a headful attempt by modifying headless=False inside scrape_dsi_members_friends_spatial().\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Innoscence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
