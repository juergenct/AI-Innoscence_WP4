{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e6a237",
   "metadata": {},
   "source": [
    "# TUHH Institutes Scraper\n",
    "\n",
    "Scrape all institute entries from the TUHH overview page and export a CSV with columns: `name`, `id`, `url`.\n",
    "\n",
    "- Source: https://www.tuhh.de/tuhh/dekanate/institute-im-ueberblick\n",
    "- The institute name is the text before the parentheses.\n",
    "- The institute ID is the letter/number code in parentheses, e.g., `(E-11)` or `(M-EXK6)`.\n",
    "- The website URL is the link embedded in each box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b57c84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports ready (Playwright: available )\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import asyncio\n",
    "import threading\n",
    "from queue import Queue\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# Optional: Playwright rendering fallback for JS-loaded content\n",
    "try:\n",
    "    from playwright.async_api import async_playwright  # type: ignore\n",
    "    HAS_PLAYWRIGHT = True\n",
    "except Exception:\n",
    "    HAS_PLAYWRIGHT = False\n",
    "\n",
    "# Optional: allow nested asyncio in notebooks\n",
    "try:\n",
    "    import nest_asyncio  # type: ignore\n",
    "    nest_asyncio.apply()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"✅ Imports ready (Playwright:\", \"available\" if HAS_PLAYWRIGHT else \"not available\", \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3e29618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TUHHInstitutesScraper updated: correct ID extraction (anchor-first, last-match) + URL validation\n"
     ]
    }
   ],
   "source": [
    "# Scraper implementation\n",
    "class TUHHInstitutesScraper:\n",
    "    def __init__(self, base_url: str = \"https://www.tuhh.de\", min_delay: float = 0.2, max_delay: float = 0.6, retry_attempts: int = 3, use_playwright: bool = True):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.session = requests.Session()\n",
    "        retry = Retry(total=retry_attempts, status_forcelist=[429, 500, 502, 503, 504], allowed_methods=[\"GET\", \"HEAD\", \"OPTIONS\"], backoff_factor=0.8)\n",
    "        adapter = HTTPAdapter(max_retries=retry)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "        self.min_delay = min_delay\n",
    "        self.max_delay = max_delay\n",
    "        self.use_playwright = use_playwright and HAS_PLAYWRIGHT\n",
    "        self.logger = logging.getLogger(\"tuhh-scraper\")\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "            self.logger.addHandler(handler)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "    def _clean_text(self, s: str) -> str:\n",
    "        return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "    def _extract_id_from_text(self, text: str) -> str | None:\n",
    "        # Return the LAST matching ID in the string to avoid picking IDs from sibling tiles\n",
    "        matches = re.findall(r\"\\(([A-Za-zÄÖÜ][A-Za-zÄÖÜ\\-]*-[A-Za-z0-9\\-]+)\\)\", text or \"\")\n",
    "        return matches[-1].strip() if matches else None\n",
    "\n",
    "    def _make_full_url(self, href: str) -> str:\n",
    "        return href if href.startswith('http') else urljoin(self.base_url, href)\n",
    "\n",
    "    def _valid_url(self, u: str) -> bool:\n",
    "        return bool(u and u.startswith(('http://','https://')) and len(u) > len('http://'))\n",
    "\n",
    "    async def _best_anchor_in_container(self, frame, a):\n",
    "        # Prefer a heading anchor within the same tile/card\n",
    "        try:\n",
    "            href = await a.get_attribute('href')\n",
    "            text = self._clean_text(await a.inner_text())\n",
    "            if text and self._extract_id_from_text(text):\n",
    "                return a  # this anchor already carries the label and id\n",
    "            # else look for heading anchors\n",
    "            heading_anchor = await a.evaluate_handle('''(el) => {\n",
    "                const c = el.closest('div,li,section,article');\n",
    "                if(!c) return null;\n",
    "                return c.querySelector('h2 a, h3 a, h4 a, strong a, a');\n",
    "            }''')\n",
    "            if heading_anchor:\n",
    "                return heading_anchor.as_element()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return a\n",
    "\n",
    "    async def _collect_from_frame(self, frame) -> list[dict]:\n",
    "        rows: list[dict] = []\n",
    "        try:\n",
    "            anchors = await frame.query_selector_all(\"main a[href], a[href]\")\n",
    "            seen = set()\n",
    "            for a in anchors:\n",
    "                try:\n",
    "                    a = await self._best_anchor_in_container(frame, a)\n",
    "                    href = await a.get_attribute('href')\n",
    "                    if not href or href.startswith('#') or href.startswith('mailto:'):\n",
    "                        continue\n",
    "                    full = self._make_full_url(href)\n",
    "                    if full in seen:\n",
    "                        continue\n",
    "                    text = self._clean_text(await a.inner_text())\n",
    "                    # Pull container text for fallback\n",
    "                    container_text = text\n",
    "                    try:\n",
    "                        container_text = self._clean_text(await a.evaluate('(el) => el.closest(\"div,li,section,article\")?.innerText || el.parentElement?.innerText || el.innerText'))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    # Prefer ID from anchor text; fallback to container\n",
    "                    inst_id = self._extract_id_from_text(text) or self._extract_id_from_text(container_text)\n",
    "                    if not inst_id:\n",
    "                        continue\n",
    "                    # Derive name from the most specific text (anchor or heading in container)\n",
    "                    label_text = text\n",
    "                    if not label_text or len(label_text) < 3 or inst_id not in label_text:\n",
    "                        try:\n",
    "                            label_text = self._clean_text(await a.evaluate('(el) => { const c = el.closest(\"div,li,section,article\"); if(!c) return \"\"; const h = c.querySelector(\"h2,h3,h4,strong\"); return h ? h.innerText : el.innerText }'))\n",
    "                        except Exception:\n",
    "                            label_text = container_text\n",
    "                    name_text = self._clean_text(label_text.replace(f\"({inst_id})\", \"\"))\n",
    "                    if len(name_text) < 2:\n",
    "                        continue\n",
    "                    if not self._valid_url(full):\n",
    "                        continue\n",
    "                    rows.append({\"name\": name_text, \"id\": inst_id, \"url\": full})\n",
    "                    seen.add(full)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        return rows\n",
    "\n",
    "    async def _scrape_async(self, url: str) -> list[dict]:\n",
    "        rows: list[dict] = []\n",
    "        try:\n",
    "            async with async_playwright() as p:  # type: ignore\n",
    "                browser = await p.chromium.launch()\n",
    "                ctx = await browser.new_context()\n",
    "                page = await ctx.new_page()\n",
    "                await page.goto(url, wait_until='domcontentloaded', timeout=45000)\n",
    "                # Accept cookies if present\n",
    "                for selector in [\n",
    "                    'button:has-text(\"AKZEPTIEREN\")',\n",
    "                    'button:has-text(\"Accept\")',\n",
    "                    \"#usercentrics-root button[aria-label='Akzeptieren']\",\n",
    "                    \"#usercentrics-root button[aria-label='Accept']\",\n",
    "                    \"#uc-btn-accept-all\",\n",
    "                ]:\n",
    "                    try:\n",
    "                        await page.locator(selector).click(timeout=1500)\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                # Try to open/activate the alphabetical section/tab\n",
    "                for tab_text in [\"alphabetisch\", \"alphabetic\", \"Alphabetisch\", \"Alphabetical\"]:\n",
    "                    try:\n",
    "                        await page.locator(f\"text={tab_text}\").first.click(timeout=2000)\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                try:\n",
    "                    await page.wait_for_selector(\"main\", timeout=15000)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                # Scroll to trigger any lazy content\n",
    "                for _ in range(3):\n",
    "                    try:\n",
    "                        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "                        await page.wait_for_timeout(300)\n",
    "                    except Exception:\n",
    "                        break\n",
    "                # Collect from page and frames\n",
    "                all_frames = [page] + page.frames\n",
    "                collected: list[dict] = []\n",
    "                for fr in all_frames:\n",
    "                    try:\n",
    "                        part = await self._collect_from_frame(fr)\n",
    "                        collected.extend(part)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                # Unique by (name,id,url)\n",
    "                seen = set()\n",
    "                for r in collected:\n",
    "                    key = (r['name'], r['id'], r['url'])\n",
    "                    if key in seen:\n",
    "                        continue\n",
    "                    seen.add(key)\n",
    "                    rows.append(r)\n",
    "                await page.close(); await ctx.close(); await browser.close()\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Async Playwright extraction failed: {e}\")\n",
    "        return rows\n",
    "\n",
    "    def _scrape_via_playwright(self, url: str) -> list[dict]:\n",
    "        if not self.use_playwright:\n",
    "            return []\n",
    "        try:\n",
    "            return asyncio.get_event_loop().run_until_complete(self._scrape_async(url))\n",
    "        except RuntimeError:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            try:\n",
    "                return loop.run_until_complete(self._scrape_async(url))\n",
    "            finally:\n",
    "                loop.close()\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Playwright extraction failed: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _request_soup(self, url: str) -> BeautifulSoup | None:\n",
    "        try:\n",
    "            time.sleep(random.uniform(self.min_delay, self.max_delay))\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36\",\n",
    "                \"Accept-Language\": \"de-DE,de;q=0.9,en;q=0.8\",\n",
    "            }\n",
    "            r = self.session.get(url, headers=headers, timeout=25, allow_redirects=True)\n",
    "            if r.status_code == 200 and r.text:\n",
    "                return BeautifulSoup(r.text, \"html.parser\")\n",
    "            self.logger.warning(f\"Non-200 for {url}: {r.status_code}\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Request error for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def get_page(self, url: str) -> BeautifulSoup | None:\n",
    "        if self.use_playwright:\n",
    "            html_rows = self._scrape_via_playwright(url)\n",
    "            if len(html_rows) >= 10:\n",
    "                self._playwright_rows_cache = html_rows\n",
    "                return BeautifulSoup(\"<html><body><main id='pw-ok'></main></body></html>\", 'html.parser')\n",
    "        return self._request_soup(url)\n",
    "\n",
    "    def parse_boxes(self, soup: BeautifulSoup) -> List[Dict[str, str]]:\n",
    "        cached = getattr(self, '_playwright_rows_cache', None)\n",
    "        if isinstance(cached, list) and cached:\n",
    "            return cached\n",
    "        # Static fallback\n",
    "        rows: List[Dict[str, str]] = []\n",
    "        if not soup:\n",
    "            return rows\n",
    "        root = soup.find('main') or soup\n",
    "        items = root.select('li, div, a')\n",
    "        seen = set()\n",
    "        for node in items:\n",
    "            text = self._clean_text(node.get_text(' ', strip=True))\n",
    "            inst_id = self._extract_id_from_text(text)\n",
    "            if not inst_id:\n",
    "                continue\n",
    "            a = node if node.name == 'a' and node.get('href') else node.find('a', href=True)\n",
    "            if not a:\n",
    "                continue\n",
    "            full = self._make_full_url(a['href'])\n",
    "            if not self._valid_url(full) or full in seen:\n",
    "                continue\n",
    "            # Name from node text\n",
    "            name_text = self._clean_text(text.replace(f\"({inst_id})\", \"\"))\n",
    "            if len(name_text) < 2:\n",
    "                name_text = self._clean_text(a.get_text(' ', strip=True))\n",
    "                name_text = self._clean_text(name_text.replace(f\"({inst_id})\", \"\"))\n",
    "            if len(name_text) < 2:\n",
    "                continue\n",
    "            rows.append({\"name\": name_text, \"id\": inst_id, \"url\": full})\n",
    "            seen.add(full)\n",
    "        return rows\n",
    "\n",
    "    def scrape(self, overview_url: str) -> List[Dict[str, str]]:\n",
    "        soup = self.get_page(overview_url)\n",
    "        rows = self.parse_boxes(soup) if soup else []\n",
    "        self.logger.info(f\"Collected {len(rows)} rows from overview (via Playwright if available)\")\n",
    "        # Final unique by URL\n",
    "        seen = set(); out = []\n",
    "        for r in rows:\n",
    "            if r['url'] in seen:\n",
    "                continue\n",
    "            seen.add(r['url']); out.append(r)\n",
    "        return out\n",
    "\n",
    "print(\"✅ TUHHInstitutesScraper updated: correct ID extraction (anchor-first, last-match) + URL validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cafbf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 17:05:19,298 - INFO - Collected 96 rows from overview (via Playwright if available)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96 institutes\n",
      "Saved 96 rows to data/tuhh_institutes_20250928_170519.csv\n"
     ]
    }
   ],
   "source": [
    "# Run and save CSV\n",
    "from datetime import datetime\n",
    "\n",
    "overview_url = \"https://www.tuhh.de/tuhh/dekanate/institute-im-ueberblick\"\n",
    "scraper = TUHHInstitutesScraper()\n",
    "rows = scraper.scrape(overview_url)\n",
    "print(f\"Found {len(rows)} institutes\")\n",
    "\n",
    "if rows:\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out = f\"data/tuhh_institutes_{ts}.csv\"\n",
    "    # ensure folder exists\n",
    "    import os\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"Saved {len(df)} rows to {out}\")\n",
    "else:\n",
    "    print(\"No rows scraped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b98cfd",
   "metadata": {},
   "source": [
    "## How to run\n",
    "\n",
    "1. Run cell 2 (Imports) and cell 3 (Scraper implementation).\n",
    "2. Run cell 4 to scrape and write `data/tuhh_institutes_YYYYMMDD_HHMMSS.csv`.\n",
    "\n",
    "The CSV will contain columns `name`, `id`, `url`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Innoscence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
