llm:
  model: 'ollama/llama3.1:8b'
  model_provider: 'ollama'
  model_tokens: 128000
  temperature: 0.0
  base_url: 'http://localhost:11434'

scraper:
  headless: true
  timeout: 30
  verbose: false
  max_retries: 3

browser:
  headless: true
  args:
    - '--disable-gpu'
    - '--no-sandbox'
    - '--disable-dev-shm-usage'
  
extraction:
  chunk_size: 4000
  overlap: 200
