{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e7eb09",
   "metadata": {},
   "source": [
    "# Hamburg Branchenbuch – Clean Pipeline\n",
    "\n",
    "This notebook contains only the essential steps:\n",
    "\n",
    "1) Imports and setup\n",
    "2) Scraper class definition (`EnhancedHamburgBranchenbuchScraper`)\n",
    "3) Large A–Z mapping runner (collect category → detail URLs with pagination, checkpoints)\n",
    "4) Extract ALL company details from the mapping (fast, threaded, progress)\n",
    "\n",
    "Run cells in order. The mapping runner writes mapping CSVs; the extraction cell uses the newest mapping and outputs a consolidated details CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb8a51e",
   "metadata": {},
   "source": [
    "# Hamburg Branchenbuch Scraper\n",
    "\n",
    "This notebook will scrape the Hamburg business directory (Branchenbuch) to extract all companies and their websites from all alphabetical sections and categories.\n",
    "\n",
    "## Overview\n",
    "- Target URL: https://www.hamburg.de/branchenbuch/hamburg/a-z/\n",
    "- Structure: Alphabetical sections → Business categories → Company listings\n",
    "- Goal: Extract company names, addresses, phone numbers, websites, and other contact details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea25ea3",
   "metadata": {},
   "source": [
    "# Clean Scraper Section\n",
    "\n",
    "This section contains the consolidated, production-ready scraper with robust pagination (single-page and multi-page), fast link collection, and detail extraction. Older exploratory cells above can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52097510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports ready\n"
     ]
    }
   ],
   "source": [
    "# 1) Imports and setup (keep)\n",
    "import os, re, json, time, random, glob, threading, collections, concurrent.futures as cf\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "print(\"✅ Imports ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949d95da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ EnhancedHamburgBranchenbuchScraper restored. Run this cell before mapping/extraction.\n"
     ]
    }
   ],
   "source": [
    "# 2) EnhancedHamburgBranchenbuchScraper (restored)\n",
    "import time, random, re, json\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "from urllib.parse import urljoin, urlparse, parse_qs, unquote\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "try:\n",
    "    import requests_cache  # type: ignore\n",
    "except Exception:\n",
    "    requests_cache = None\n",
    "\n",
    "class EnhancedHamburgBranchenbuchScraper:\n",
    "    def __init__(self,\n",
    "                 min_delay: float = 0.15,\n",
    "                 max_delay: float = 0.35,\n",
    "                 retry_attempts: int = 3,\n",
    "                 enable_caching: bool = True,\n",
    "                 cache_name: str = \"hh_cache\",\n",
    "                 cache_expire_seconds: int = 24 * 3600,\n",
    "                 timeout: int = 25):\n",
    "        self.min_delay = min_delay\n",
    "        self.max_delay = max_delay\n",
    "        self.retry_attempts = retry_attempts\n",
    "        self.timeout = timeout\n",
    "        self.user_agents = [\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122 Safari/537.36',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',\n",
    "        ]\n",
    "        self.headers = {\"User-Agent\": random.choice(self.user_agents)}\n",
    "\n",
    "        if enable_caching and requests_cache is not None:\n",
    "            self.session = requests_cache.CachedSession(\n",
    "                cache_name=cache_name,\n",
    "                backend='sqlite',\n",
    "                expire_after=cache_expire_seconds,\n",
    "            )\n",
    "        else:\n",
    "            self.session = requests.Session()\n",
    "\n",
    "        retry = Retry(\n",
    "            total=retry_attempts,\n",
    "            backoff_factor=0.5,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"GET\"],\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry, pool_connections=100, pool_maxsize=100)\n",
    "        self.session.mount('http://', adapter)\n",
    "        self.session.mount('https://', adapter)\n",
    "\n",
    "    def _sleep(self):\n",
    "        time.sleep(random.uniform(self.min_delay, self.max_delay))\n",
    "\n",
    "    def get_page(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        last_err = None\n",
    "        for _ in range(max(1, self.retry_attempts)):\n",
    "            try:\n",
    "                self.headers[\"User-Agent\"] = random.choice(self.user_agents)\n",
    "                r = self.session.get(url, headers=self.headers, timeout=self.timeout)\n",
    "                if 300 <= r.status_code < 400 and 'Location' in r.headers:\n",
    "                    loc = r.headers.get('Location')\n",
    "                    if loc:\n",
    "                        url = urljoin(url, loc)\n",
    "                r.raise_for_status()\n",
    "                return BeautifulSoup(r.text, 'html.parser')\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                time.sleep(0.3)\n",
    "        return None\n",
    "\n",
    "    def get_letters(self) -> List[str]:\n",
    "        base = \"https://www.hamburg.de/branchenbuch/hamburg/a-z/\"\n",
    "        return [urljoin(base, f\"{ch}/\") for ch in list(\"abcdefghijklmnopqrstuvwxyz\")]\n",
    "\n",
    "    def get_category_links(self, letters_soup: BeautifulSoup) -> List[Dict[str, str]]:\n",
    "        out = []\n",
    "        if not letters_soup:\n",
    "            return out\n",
    "        for a in letters_soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            if not isinstance(href, str):\n",
    "                continue\n",
    "            if '/branchenbuch/hamburg/' in href and re.search(r'/\\d+/(n\\d+/)?$', href):\n",
    "                name = a.get_text(' ', strip=True)\n",
    "                url = href if href.startswith('http') else urljoin('https://www.hamburg.de', href)\n",
    "                out.append({\"name\": name, \"url\": url})\n",
    "        # de-duplicate preserving order by url\n",
    "        seen = set()\n",
    "        uniq = []\n",
    "        for it in out:\n",
    "            if it['url'] not in seen:\n",
    "                seen.add(it['url'])\n",
    "                uniq.append(it)\n",
    "        return uniq\n",
    "\n",
    "    def _page_url(self, category_url: str, offset: int) -> str:\n",
    "        # Normalize and construct .../n{offset}/ URLs\n",
    "        if re.search(r'/n\\d+/?$', category_url):\n",
    "            return re.sub(r'/n\\d+/?$', f\"/n{offset}/\", category_url)\n",
    "        if not category_url.endswith('/'):\n",
    "            category_url += '/'\n",
    "        return urljoin(category_url, f\"n{offset}/\")\n",
    "\n",
    "    def _page_signature(self, soup: Optional[BeautifulSoup]) -> str:\n",
    "        if not soup:\n",
    "            return \"\"\n",
    "        # signature by first few company hrefs/texts\n",
    "        sig_parts = []\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            if '/branchenbuch/hamburg/eintrag/' in href:\n",
    "                sig_parts.append(href)\n",
    "                if len(sig_parts) >= 8:\n",
    "                    break\n",
    "        if not sig_parts:\n",
    "            # fallback: first headings/text blocks\n",
    "            heads = [h.get_text(' ', strip=True) for h in soup.find_all(['h2','h3'])][:8]\n",
    "            sig_parts = heads\n",
    "        return '|'.join(sig_parts)\n",
    "\n",
    "    def get_category_total_count(self, soup: Optional[BeautifulSoup]) -> Optional[int]:\n",
    "        if not soup:\n",
    "            return None\n",
    "        txt = soup.get_text(\" \", strip=True)\n",
    "        m = re.search(r'(\\d{1,6})\\s*(Treffer|Ergebnisse|Eintr[aä]ge)', txt, flags=re.I)\n",
    "        if m:\n",
    "            try:\n",
    "                return int(m.group(1))\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    def get_category_page_urls(self, category_url: str, first_page_soup: Optional[BeautifulSoup] = None,\n",
    "                               page_size: int = 20, probe_extra_pages: int = 3) -> List[str]:\n",
    "        if not first_page_soup:\n",
    "            first_page_soup = self.get_page(category_url)\n",
    "        if not first_page_soup:\n",
    "            return [category_url]\n",
    "        total = self.get_category_total_count(first_page_soup)\n",
    "        pages = []\n",
    "        seen_sigs = set()\n",
    "        # Start with n0 and advertised pages\n",
    "        max_pages = 1\n",
    "        if total is not None and total > 0:\n",
    "            max_pages = max(1, (total + page_size - 1) // page_size)\n",
    "        # Build initial set\n",
    "        for i in range(max_pages):\n",
    "            offset = i * page_size\n",
    "            url_i = self._page_url(category_url, offset)\n",
    "            soup_i = first_page_soup if i == 0 else self.get_page(url_i)\n",
    "            sig = self._page_signature(soup_i)\n",
    "            if not sig or sig in seen_sigs:\n",
    "                break\n",
    "            seen_sigs.add(sig)\n",
    "            pages.append(url_i)\n",
    "            self._sleep()\n",
    "        # Probe a few more in case advertised undercounts\n",
    "        i = len(pages)\n",
    "        while i < max_pages + probe_extra_pages:\n",
    "            offset = i * page_size\n",
    "            url_i = self._page_url(category_url, offset)\n",
    "            soup_i = self.get_page(url_i)\n",
    "            sig = self._page_signature(soup_i)\n",
    "            if not sig or sig in seen_sigs:\n",
    "                break\n",
    "            seen_sigs.add(sig)\n",
    "            pages.append(url_i)\n",
    "            i += 1\n",
    "            self._sleep()\n",
    "        return pages\n",
    "\n",
    "    def get_all_company_detail_links(self, category_url: str, cap_to_advertised: bool = False) -> List[str]:\n",
    "        first = self.get_page(category_url)\n",
    "        if not first:\n",
    "            return []\n",
    "        advertised = self.get_category_total_count(first)\n",
    "        page_urls = self.get_category_page_urls(category_url, first)\n",
    "        links: List[str] = []\n",
    "        seen = set()\n",
    "        for pu in page_urls:\n",
    "            soup = first if pu == page_urls[0] else self.get_page(pu)\n",
    "            if not soup:\n",
    "                continue\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                href = a['href']\n",
    "                if '/branchenbuch/hamburg/eintrag/' in href:\n",
    "                    absu = href if href.startswith('http') else urljoin('https://www.hamburg.de', href)\n",
    "                    if absu not in seen:\n",
    "                        seen.add(absu)\n",
    "                        links.append(absu)\n",
    "            if cap_to_advertised and advertised and len(links) >= advertised:\n",
    "                links = links[:advertised]\n",
    "                break\n",
    "            self._sleep()\n",
    "        return links\n",
    "\n",
    "    # Detail extraction (JSON-LD preferred, refined website + address fallbacks)\n",
    "    def extract_company_details(self, company_url: str, category_name: Optional[str] = None, letter: Optional[str] = None) -> Dict[str,str]:\n",
    "        soup = self.get_page(company_url)\n",
    "        try:\n",
    "            from datetime import datetime as _dt\n",
    "            ts = _dt.utcnow().isoformat()\n",
    "        except Exception:\n",
    "            ts = \"\"\n",
    "        data = {\n",
    "            \"source_url\": company_url,\n",
    "            \"name\": \"\",\n",
    "            \"website\": \"\",\n",
    "            \"phone\": \"\",\n",
    "            \"email\": \"\",\n",
    "            \"address\": \"\",\n",
    "            \"latitude\": \"\",\n",
    "            \"longitude\": \"\",\n",
    "            \"image\": \"\",\n",
    "            \"category\": category_name or \"\",\n",
    "            \"letter_section\": letter or \"\",\n",
    "            \"scraped_at\": ts,\n",
    "        }\n",
    "        if not soup:\n",
    "            return data\n",
    "\n",
    "        # Try JSON-LD first\n",
    "        jsonld = None\n",
    "        for tag in soup.find_all('script', type='application/ld+json'):\n",
    "            txt = tag.string or tag.text or ''\n",
    "            try:\n",
    "                obj = json.loads(txt)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if isinstance(obj, list):\n",
    "                for o in obj:\n",
    "                    if isinstance(o, dict) and o.get('@type'):\n",
    "                        jsonld = o\n",
    "                        break\n",
    "            elif isinstance(obj, dict) and obj.get('@type'):\n",
    "                jsonld = obj\n",
    "            if jsonld:\n",
    "                break\n",
    "        if isinstance(jsonld, dict):\n",
    "            data[\"name\"] = (jsonld.get('name') or data[\"name\"]).strip()\n",
    "            url_val = jsonld.get('url') or jsonld.get('@id')\n",
    "            if isinstance(url_val, str):\n",
    "                data[\"website\"] = url_val\n",
    "            elif isinstance(url_val, list):\n",
    "                data[\"website\"] = next((u for u in url_val if isinstance(u, str) and u.startswith('http')), data[\"website\"]) or \"\"\n",
    "            same = jsonld.get('sameAs')\n",
    "            if not data[\"website\"] and isinstance(same, list):\n",
    "                data[\"website\"] = next((u for u in same if isinstance(u, str) and u.startswith('http')), \"\")\n",
    "            adr = jsonld.get('address')\n",
    "            if isinstance(adr, dict):\n",
    "                parts = [adr.get('streetAddress',''), adr.get('postalCode',''), adr.get('addressLocality','')]\n",
    "                data[\"address\"] = ' '.join([p for p in parts if p]).strip()\n",
    "            geo = jsonld.get('geo')\n",
    "            if isinstance(geo, dict):\n",
    "                data[\"latitude\"] = str(geo.get('latitude') or '')\n",
    "                data[\"longitude\"] = str(geo.get('longitude') or '')\n",
    "\n",
    "        # Name fallback from headers if still empty\n",
    "        if not data[\"name\"]:\n",
    "            h = soup.find(['h1','h2'])\n",
    "            if h:\n",
    "                data[\"name\"] = h.get_text(' ', strip=True)\n",
    "\n",
    "        # Phone / Email via anchors\n",
    "        a_tel = soup.find('a', href=re.compile(r'^tel:', re.I))\n",
    "        if a_tel and a_tel.get('href'):\n",
    "            data[\"phone\"] = re.sub(r'^tel:', '', a_tel['href']).strip()\n",
    "        a_mail = soup.find('a', href=re.compile(r'^mailto:', re.I))\n",
    "        if a_mail and a_mail.get('href'):\n",
    "            data[\"email\"] = re.sub(r'^mailto:', '', a_mail['href']).strip()\n",
    "\n",
    "        # Address fallbacks\n",
    "        if not data[\"address\"]:\n",
    "            addr = soup.find('address')\n",
    "            if addr:\n",
    "                data[\"address\"] = ' '.join(addr.get_text(' ', strip=True).split())\n",
    "        if not data[\"address\"]:\n",
    "            # Look for microdata-style spans\n",
    "            street = soup.find(attrs={\"itemprop\": \"streetAddress\"})\n",
    "            plz = soup.find(attrs={\"itemprop\": \"postalCode\"})\n",
    "            city = soup.find(attrs={\"itemprop\": \"addressLocality\"})\n",
    "            parts = []\n",
    "            for el in (street, plz, city):\n",
    "                if el:\n",
    "                    parts.append(el.get_text(' ', strip=True))\n",
    "            if parts:\n",
    "                data[\"address\"] = ' '.join(parts)\n",
    "        if not data[\"address\"]:\n",
    "            # Generic class/id based fallback (address/Adresse/Anschrift)\n",
    "            cand_nodes = soup.select('[class*=\"address\" i], [id*=\"address\" i], .adresse, [class*=\"anschrift\" i]')\n",
    "            for node in cand_nodes:\n",
    "                txt = ' '.join(node.get_text(' ', strip=True).split())\n",
    "                if re.search(r'\\b\\d{5}\\b', txt) or any(ch.isdigit() for ch in txt):\n",
    "                    data[\"address\"] = txt\n",
    "                    break\n",
    "\n",
    "        # Image\n",
    "        og = soup.find('meta', property='og:image')\n",
    "        if og and og.get('content'):\n",
    "            data[\"image\"] = og['content'].strip()\n",
    "\n",
    "        # Refined website picking: avoid hamburg.de, berlin.de and redirect wrappers; prefer image-wrapped/link text cues\n",
    "        def _resolve_redirect(href: str) -> str:\n",
    "            try:\n",
    "                if not href or not href.startswith('http'):\n",
    "                    return ''\n",
    "                u = urlparse(href)\n",
    "                qs = parse_qs(u.query)\n",
    "                for k in ['url','dest','destination','to','link','u']:\n",
    "                    if k in qs and qs[k]:\n",
    "                        target = unquote(qs[k][0])\n",
    "                        if target.startswith('http'):\n",
    "                            return target\n",
    "                return href\n",
    "            except Exception:\n",
    "                return href\n",
    "        def _hostname(u: str) -> str:\n",
    "            try:\n",
    "                return urlparse(u).hostname or ''\n",
    "            except Exception:\n",
    "                return ''\n",
    "        def _is_portal(host: str) -> bool:\n",
    "            h = (host or '').lower()\n",
    "            return ('hamburg.de' in h) or ('berlin.de' in h)\n",
    "        BLOCKLIST = {'branchenbuch.hamburg.de','google.','maps.google','hvv.de','geofox.de','booking.com','yelp.','tripadvisor.','facebook.com','instagram.com','x.com','twitter.com','berlin.de'}\n",
    "\n",
    "        # If json-ld gave a portal URL, drop it so we can try anchors\n",
    "        if data[\"website\"] and _is_portal(_hostname(data[\"website\"])):\n",
    "            data[\"website\"] = ''\n",
    "\n",
    "        if not data[\"website\"]:\n",
    "            # 1) label hints + data-* attributes\n",
    "            candidates: List[Tuple[str,str,bool]] = []\n",
    "            for a in soup.find_all('a'):\n",
    "                href = (a.get('href') or '').strip()\n",
    "                durl = a.get('data-url') or a.get('data-href') or a.get('data-website')\n",
    "                href = _resolve_redirect(durl.strip()) if durl else _resolve_redirect(href)\n",
    "                if not href.startswith('http'):\n",
    "                    continue\n",
    "                if any(b in href for b in BLOCKLIST):\n",
    "                    continue\n",
    "                label = (a.get_text(' ', strip=True) or a.get('aria-label') or a.get('title') or '').lower()\n",
    "                has_img = a.find('img') is not None\n",
    "                candidates.append((href, label, has_img))\n",
    "            website_keywords = ('website','webseite','zur website','homepage','zur homepage','internetseite','zur internetseite')\n",
    "            for href, label, _ in candidates:\n",
    "                if any(p in label for p in website_keywords) and not _is_portal(_hostname(href)):\n",
    "                    data[\"website\"] = href\n",
    "                    break\n",
    "            # 2) image-wrapped\n",
    "            if not data[\"website\"]:\n",
    "                for href, _, has_img in candidates:\n",
    "                    if has_img and not _is_portal(_hostname(href)):\n",
    "                        data[\"website\"] = href\n",
    "                        break\n",
    "            # 3) any external\n",
    "            if not data[\"website\"]:\n",
    "                for href, _, _ in candidates:\n",
    "                    if not _is_portal(_hostname(href)):\n",
    "                        data[\"website\"] = href\n",
    "                        break\n",
    "\n",
    "        return data\n",
    "\n",
    "print(\"✅ EnhancedHamburgBranchenbuchScraper restored. Run this cell before mapping/extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a3be4",
   "metadata": {},
   "source": [
    "# How to run\n",
    "\n",
    "- Run the Imports cell and then the Scraper class cell to load the implementation.\n",
    "- Run the A–Z Mapping Runner cell to scrape all letters A–Z. It will:\n",
    "  - Respect category pagination using n0, n20, n40… with wrap detection.\n",
    "  - Cap each category’s links to the advertised count to avoid overshoot.\n",
    "  - Save partial mapping CSVs in `data/hh_branchenbuch_checkpoints/mapping/` and consolidate to `hamburg_branchenbuch_companies_category_map_<timestamp>.csv` in the workspace root.\n",
    "- Run the Details Extraction cell to backfill all company details from the latest mapping CSV (threaded, resumable):\n",
    "  - It auto-picks the newest `hamburg_branchenbuch_companies_category_map_*.csv` if a specific path isn’t configured.\n",
    "  - It saves partials under `data/hh_branchenbuch_checkpoints/from_map/` and consolidates to `hamburg_branchenbuch_companies_details_from_map_<timestamp>.csv`.\n",
    "\n",
    "Notes:\n",
    "- Requests are cached for speed and politeness; you can rerun to resume faster.\n",
    "- You can resume the mapping run; processed categories are tracked in `data/hh_branchenbuch_checkpoints/mapping/processed_categories.txt`.\n",
    "- Press Ctrl+C once to stop gracefully after the current item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c21b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Large A–Z mapping runner (collect category → detail URLs with pagination, checkpoints)\n",
    "# - Crawls all letter sections (A–Z)\n",
    "# - For each category under a letter, enumerates all paginated listing pages\n",
    "# - Collects company detail URLs and writes mapping CSV rows: (url, category, letter_section, category_url)\n",
    "# - Saves partial CSVs regularly and consolidates to a final mapping CSV at the end\n",
    "\n",
    "import os, glob, json, time\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Config ---\n",
    "LETTERS_OVERRIDE = None  # e.g., set to ['a','b'] to limit for testing; None → all a–z\n",
    "OUT_PART_DIR = os.path.join(os.getcwd(), \"data\", \"hh_branchenbuch_checkpoints\", \"mapping\")\n",
    "os.makedirs(OUT_PART_DIR, exist_ok=True)\n",
    "SAVE_EVERY_N_MAP = 5000\n",
    "CAP_TO_ADVERTISED = True  # be conservative; we probe extra in page URL builder already\n",
    "PROCESSED_CATS_PATH = os.path.join(OUT_PART_DIR, \"processed_categories.txt\")  # persistent resume\n",
    "\n",
    "# --- Helper: load/save processed categories for resume ---\n",
    "def _load_processed_categories(path: str) -> set:\n",
    "    s = set()\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    u = line.strip()\n",
    "                    if u:\n",
    "                        s.add(u)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return s\n",
    "\n",
    "def _append_processed_category(path: str, cat_url: str):\n",
    "    try:\n",
    "        with open(path, 'a', encoding='utf-8') as f:\n",
    "            f.write(cat_url.strip() + \"\\n\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- Instantiate scraper ---\n",
    "scraper = EnhancedHamburgBranchenbuchScraper(min_delay=0.05, max_delay=0.20, retry_attempts=3, enable_caching=True)\n",
    "letters_urls = scraper.get_letters()\n",
    "if LETTERS_OVERRIDE:\n",
    "    letters_urls = [u for u in letters_urls if any(u.rstrip('/').endswith(f\"/{ch}\") for ch in LETTERS_OVERRIDE)]\n",
    "\n",
    "# --- Mapping collection ---\n",
    "rows_map = []\n",
    "ts_map = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "part_idx_map = 1\n",
    "processed_cats = _load_processed_categories(PROCESSED_CATS_PATH)\n",
    "\n",
    "def _save_partial_map(force: bool=False):\n",
    "    global rows_map, part_idx_map\n",
    "    if not rows_map and not force:\n",
    "        return\n",
    "    dfp = pd.DataFrame(rows_map)\n",
    "    cols = [\"url\", \"category\", \"letter_section\", \"category_url\"]\n",
    "    for c in cols:\n",
    "        if c not in dfp.columns:\n",
    "            dfp[c] = \"\"\n",
    "    dfp = dfp[cols]\n",
    "    outp = os.path.join(OUT_PART_DIR, f\"category_map_{ts_map}_chunk{part_idx_map:03d}.csv\")\n",
    "    dfp.to_csv(outp, index=False)\n",
    "    print(f\"Saved mapping partial chunk {part_idx_map:03d}: {len(dfp)} rows → {outp}\")\n",
    "    part_idx_map += 1\n",
    "    rows_map = []\n",
    "\n",
    "print(f\"Starting A–Z mapping run across {len(letters_urls)} letter sections…\")\n",
    "start_map = time.time()\n",
    "\n",
    "for lidx, letter_url in enumerate(letters_urls, start=1):\n",
    "    letter = letter_url.rstrip('/').split('/')[-1]\n",
    "    soup_letter = scraper.get_page(letter_url)\n",
    "    if not soup_letter:\n",
    "        print(f\"[warn] Could not load letter page: {letter_url}\")\n",
    "        continue\n",
    "    categories = scraper.get_category_links(soup_letter)\n",
    "    print(f\"[{lidx}/{len(letters_urls)}] Letter '{letter.upper()}': {len(categories)} categories\")\n",
    "\n",
    "    for cidx, cat in enumerate(categories, start=1):\n",
    "        cat_name = cat.get('name', '').strip()\n",
    "        cat_url = cat.get('url', '').strip()\n",
    "        if not cat_url:\n",
    "            continue\n",
    "        if cat_url in processed_cats:\n",
    "            # already processed in a previous/resumed run\n",
    "            continue\n",
    "\n",
    "        # Enumerate all detail links for this category\n",
    "        try:\n",
    "            detail_links = scraper.get_all_company_detail_links(cat_url, cap_to_advertised=CAP_TO_ADVERTISED)\n",
    "        except Exception:\n",
    "            detail_links = []\n",
    "\n",
    "        # De-dup only within this category to avoid page repetition; preserve cross-category associations\n",
    "        seen_in_cat = set()\n",
    "        added = 0\n",
    "        for du in detail_links:\n",
    "            if du in seen_in_cat:\n",
    "                continue\n",
    "            seen_in_cat.add(du)\n",
    "            rows_map.append({\n",
    "                \"url\": du,\n",
    "                \"category\": cat_name,\n",
    "                \"letter_section\": letter,\n",
    "                \"category_url\": cat_url,\n",
    "            })\n",
    "            added += 1\n",
    "        _append_processed_category(PROCESSED_CATS_PATH, cat_url)\n",
    "        processed_cats.add(cat_url)\n",
    "\n",
    "        if added == 0:\n",
    "            # Even if empty, keep a heartbeat print to show progress\n",
    "            print(f\"  - [{cidx}/{len(categories)}] {cat_name} → 0 links\")\n",
    "        else:\n",
    "            if len(rows_map) >= SAVE_EVERY_N_MAP:\n",
    "                _save_partial_map()\n",
    "\n",
    "# Save any remaining mapping rows and consolidate\n",
    "_save_partial_map(force=True)\n",
    "partials = sorted(glob.glob(os.path.join(OUT_PART_DIR, f\"category_map_{ts_map}_chunk*.csv\")))\n",
    "all_map_df = []\n",
    "for p in partials:\n",
    "    try:\n",
    "        if os.path.getsize(p) > 0:\n",
    "            all_map_df.append(pd.read_csv(p))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if all_map_df:\n",
    "    map_df = pd.concat(all_map_df, ignore_index=True)\n",
    "    # Ensure columns order\n",
    "    map_df = map_df[[\"url\",\"category\",\"letter_section\",\"category_url\"]]\n",
    "    out_final_map = f\"hamburg_branchenbuch_companies_category_map_{ts_map}.csv\"\n",
    "    map_df.to_csv(out_final_map, index=False)\n",
    "    took = time.time() - start_map\n",
    "    print(f\"✅ Mapping done: {len(map_df):,} rows → {out_final_map} in {took/60:.1f} min.\")\n",
    "    display(map_df.head(5))\n",
    "else:\n",
    "    print(\"No mapping partials to consolidate (no rows?).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c97f6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique detail URLs to backfill: 67,899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:   3%|▎         | 2002/67899 [04:11<2:27:27,  7.45company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 001: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:   6%|▌         | 4000/67899 [08:22<1:55:57,  9.18company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 002: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk002.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:   9%|▉         | 6003/67899 [12:34<1:47:55,  9.56company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 003: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  12%|█▏        | 8000/67899 [16:45<1:45:01,  9.51company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 004: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk004.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  15%|█▍        | 10001/67899 [20:57<1:42:30,  9.41company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 005: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  18%|█▊        | 12007/67899 [25:12<1:04:49, 14.37company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 006: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk006.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  21%|██        | 14000/67899 [29:25<2:11:56,  6.81company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 007: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk007.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  24%|██▎       | 16000/67899 [33:36<1:59:48,  7.22company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 008: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk008.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  27%|██▋       | 17999/67899 [37:49<1:41:44,  8.17company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 009: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk009.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  29%|██▉       | 20002/67899 [42:01<1:20:46,  9.88company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 010: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  32%|███▏      | 21999/67899 [46:13<1:33:27,  8.18company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 011: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  35%|███▌      | 24002/67899 [50:25<1:36:23,  7.59company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 012: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  38%|███▊      | 26003/67899 [54:37<1:13:41,  9.47company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 013: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  41%|████      | 28003/67899 [58:50<1:09:05,  9.62company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 014: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk014.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  44%|████▍     | 30005/67899 [1:03:01<57:19, 11.02company/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 015: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk015.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  47%|████▋     | 32004/67899 [1:07:13<47:33, 12.58company/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 016: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  50%|█████     | 34005/67899 [1:11:26<52:06, 10.84company/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 017: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  53%|█████▎    | 36001/67899 [1:15:37<1:13:28,  7.24company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 018: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  56%|█████▌    | 38001/67899 [1:19:49<1:13:55,  6.74company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 019: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk019.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  59%|█████▉    | 40001/67899 [1:24:01<1:08:01,  6.84company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 020: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk020.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  62%|██████▏   | 42003/67899 [1:28:13<38:08, 11.31company/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 021: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk021.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  65%|██████▍   | 44001/67899 [1:32:24<43:03,  9.25company/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 022: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  68%|██████▊   | 46002/67899 [1:36:37<48:17,  7.56company/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 023: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk023.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  71%|███████   | 47999/67899 [1:40:51<45:17,  7.32company/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 024: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  74%|███████▎  | 50001/67899 [1:45:03<42:34,  7.01company/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 025: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk025.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  77%|███████▋  | 52002/67899 [1:49:15<35:22,  7.49company/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 026: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk026.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  80%|███████▉  | 53997/67899 [1:53:27<35:27,  6.54company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 027: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk027.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  82%|████████▏ | 56000/67899 [1:57:39<30:00,  6.61company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 028: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk028.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  85%|████████▌ | 57998/67899 [2:01:50<19:58,  8.26company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 029: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk029.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  88%|████████▊ | 60000/67899 [2:06:01<11:26, 11.50company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 030: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk030.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  91%|█████████▏| 62001/67899 [2:10:14<10:36,  9.27company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 031: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk031.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  94%|█████████▍| 64003/67899 [2:14:27<08:21,  7.77company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 032: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk032.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details:  97%|█████████▋| 66000/67899 [2:18:39<05:39,  5.59company/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 033: 2000 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk033.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting company details: 100%|██████████| 67899/67899 [2:22:38<00:00,  7.93company/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial chunk 034: 1899 rows → /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map/details_from_map_20250930_134410_chunk034.csv\n",
      "✅ Done: 67899 rows → hamburg_branchenbuch_companies_details_from_map_20250930_134410.csv in 142.7 min.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>phone</th>\n",
       "      <th>email</th>\n",
       "      <th>website</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>image</th>\n",
       "      <th>category</th>\n",
       "      <th>letter_section</th>\n",
       "      <th>categories_all</th>\n",
       "      <th>letter_sections_all</th>\n",
       "      <th>source_url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbruchunternehmen Peter Frank</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49407219189.0</td>\n",
       "      <td>postfach@abbruch-frank.de</td>\n",
       "      <td>http://www.abbruch-frank.de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abbruchunternehmen</td>\n",
       "      <td>a</td>\n",
       "      <td>[\"Abbruchunternehmen\", \"Bauen &amp; Wohnen\", \"Bauu...</td>\n",
       "      <td>[\"a\", \"b\"]</td>\n",
       "      <td>https://www.hamburg.de/branchenbuch/hamburg/ei...</td>\n",
       "      <td>2025-09-30T11:44:11.623791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PBR Peter Beuck Recycling GmbH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>494020905925.0</td>\n",
       "      <td>info@peter-beuck.de</td>\n",
       "      <td>https://peter-beuck.de/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abbruchunternehmen</td>\n",
       "      <td>a</td>\n",
       "      <td>[\"Abbruchunternehmen\", \"Auto &amp; Verkehr\", \"Baue...</td>\n",
       "      <td>[\"a\", \"b\", \"c\", \"e\", \"h\", \"l\", \"m\", \"o\", \"p\", ...</td>\n",
       "      <td>https://www.hamburg.de/branchenbuch/hamburg/ei...</td>\n",
       "      <td>2025-09-30T11:44:11.829827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DCD Schröder &amp; Kindler GbR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>494080007474.0</td>\n",
       "      <td>moin@dcd24.de</td>\n",
       "      <td>http://www.dcd24.de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abbruchunternehmen</td>\n",
       "      <td>a</td>\n",
       "      <td>[\"Abbruchunternehmen\", \"Bauen &amp; Wohnen\", \"Bauu...</td>\n",
       "      <td>[\"a\", \"b\", \"e\", \"h\", \"k\", \"l\", \"m\", \"o\", \"r\", ...</td>\n",
       "      <td>https://www.hamburg.de/branchenbuch/hamburg/ei...</td>\n",
       "      <td>2025-09-30T11:44:11.522036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abbruchunternehmen Hermann Mock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49405235288.0</td>\n",
       "      <td>info@abbruch-mock.de</td>\n",
       "      <td>http://www.abbruch-mock.de/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abbruchunternehmen</td>\n",
       "      <td>a</td>\n",
       "      <td>[\"Abbruchunternehmen\", \"Baudienstleistungen\", ...</td>\n",
       "      <td>[\"a\", \"b\", \"e\", \"o\", \"s\"]</td>\n",
       "      <td>https://www.hamburg.de/branchenbuch/hamburg/ei...</td>\n",
       "      <td>2025-09-30T11:44:11.790108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>André Howeiler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4917661743982.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.koeln.de/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abbruchunternehmen</td>\n",
       "      <td>a</td>\n",
       "      <td>[\"Abbruchunternehmen\", \"Bauen &amp; Wohnen\", \"Bauu...</td>\n",
       "      <td>[\"a\", \"b\", \"k\"]</td>\n",
       "      <td>https://www.hamburg.de/branchenbuch/hamburg/ei...</td>\n",
       "      <td>2025-09-30T11:44:11.874438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name  address            phone  \\\n",
       "0   Abbruchunternehmen Peter Frank      NaN    49407219189.0   \n",
       "1   PBR Peter Beuck Recycling GmbH      NaN   494020905925.0   \n",
       "2       DCD Schröder & Kindler GbR      NaN   494080007474.0   \n",
       "3  Abbruchunternehmen Hermann Mock      NaN    49405235288.0   \n",
       "4                   André Howeiler      NaN  4917661743982.0   \n",
       "\n",
       "                       email                      website  latitude  \\\n",
       "0  postfach@abbruch-frank.de  http://www.abbruch-frank.de       NaN   \n",
       "1        info@peter-beuck.de      https://peter-beuck.de/       NaN   \n",
       "2              moin@dcd24.de          http://www.dcd24.de       NaN   \n",
       "3       info@abbruch-mock.de  http://www.abbruch-mock.de/       NaN   \n",
       "4                        NaN        https://www.koeln.de/       NaN   \n",
       "\n",
       "   longitude  image            category letter_section  \\\n",
       "0        NaN    NaN  Abbruchunternehmen              a   \n",
       "1        NaN    NaN  Abbruchunternehmen              a   \n",
       "2        NaN    NaN  Abbruchunternehmen              a   \n",
       "3        NaN    NaN  Abbruchunternehmen              a   \n",
       "4        NaN    NaN  Abbruchunternehmen              a   \n",
       "\n",
       "                                      categories_all  \\\n",
       "0  [\"Abbruchunternehmen\", \"Bauen & Wohnen\", \"Bauu...   \n",
       "1  [\"Abbruchunternehmen\", \"Auto & Verkehr\", \"Baue...   \n",
       "2  [\"Abbruchunternehmen\", \"Bauen & Wohnen\", \"Bauu...   \n",
       "3  [\"Abbruchunternehmen\", \"Baudienstleistungen\", ...   \n",
       "4  [\"Abbruchunternehmen\", \"Bauen & Wohnen\", \"Bauu...   \n",
       "\n",
       "                                 letter_sections_all  \\\n",
       "0                                         [\"a\", \"b\"]   \n",
       "1  [\"a\", \"b\", \"c\", \"e\", \"h\", \"l\", \"m\", \"o\", \"p\", ...   \n",
       "2  [\"a\", \"b\", \"e\", \"h\", \"k\", \"l\", \"m\", \"o\", \"r\", ...   \n",
       "3                          [\"a\", \"b\", \"e\", \"o\", \"s\"]   \n",
       "4                                    [\"a\", \"b\", \"k\"]   \n",
       "\n",
       "                                          source_url  \\\n",
       "0  https://www.hamburg.de/branchenbuch/hamburg/ei...   \n",
       "1  https://www.hamburg.de/branchenbuch/hamburg/ei...   \n",
       "2  https://www.hamburg.de/branchenbuch/hamburg/ei...   \n",
       "3  https://www.hamburg.de/branchenbuch/hamburg/ei...   \n",
       "4  https://www.hamburg.de/branchenbuch/hamburg/ei...   \n",
       "\n",
       "                   scraped_at  \n",
       "0  2025-09-30T11:44:11.623791  \n",
       "1  2025-09-30T11:44:11.829827  \n",
       "2  2025-09-30T11:44:11.522036  \n",
       "3  2025-09-30T11:44:11.790108  \n",
       "4  2025-09-30T11:44:11.874438  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4) Extract ALL company metadata from mapping (fast, threaded, with progress)\n",
    "# - Reads the latest/target mapping CSV\n",
    "# - Aggregates ALL categories/letter sections per URL into JSON arrays\n",
    "# - Uses EnhancedHamburgBranchenbuchScraper.extract_company_details if available\n",
    "# - Fallbacks to a minimal extractor if the class isn't defined\n",
    "# - Post-processes website to avoid hamburg.de/berlin.de and handle image-wrapped/redirected anchors\n",
    "# - Saves partials and a consolidated CSV\n",
    "\n",
    "import os, time, random, glob, threading, collections, concurrent.futures as cf, json, re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "\n",
    "# --- Config ---\n",
    "# If this path doesn't exist, we'll auto-discover the newest mapping CSV in the workspace root.\n",
    "MAPPING_PATH = \"/home/thiesen/Documents/AI-Innoscence_Ecosystem/hamburg_branchenbuch_companies_category_map_20250929_050648.csv\"\n",
    "OUT_PART_DIR = os.path.join(os.getcwd(), \"data\", \"hh_branchenbuch_checkpoints\", \"from_map\")\n",
    "os.makedirs(OUT_PART_DIR, exist_ok=True)\n",
    "MAX_WORKERS_DETAILS = 32\n",
    "RATE_LIMIT_PER_SEC = 8\n",
    "SAVE_EVERY_N = 2000\n",
    "SMALL_JITTER = (0.01, 0.05)\n",
    "\n",
    "# --- Availability of full scraper ---\n",
    "try:\n",
    "    _ = EnhancedHamburgBranchenbuchScraper  # type: ignore\n",
    "    _HAS_FULL = True\n",
    "except Exception:\n",
    "    _HAS_FULL = False\n",
    "\n",
    "# --- Load mapping and build full context (all categories/letters per URL) ---\n",
    "if not os.path.exists(MAPPING_PATH):\n",
    "    # Auto-pick newest mapping CSV from the workspace root\n",
    "    cand = sorted(glob.glob(os.path.join(os.getcwd(), \"hamburg_branchenbuch_companies_category_map_*.csv\")), reverse=True)\n",
    "    if cand:\n",
    "        MAPPING_PATH = cand[0]\n",
    "        print(f\"Auto-selected latest mapping: {MAPPING_PATH}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No mapping CSV found. Please run the A–Z Mapping Runner first.\")\n",
    "\n",
    "map_df = pd.read_csv(MAPPING_PATH)\n",
    "if 'url' not in map_df.columns:\n",
    "    for cand in ['detail_url', 'source_url']:\n",
    "        if cand in map_df.columns:\n",
    "            map_df = map_df.rename(columns={cand: 'url'})\n",
    "            break\n",
    "assert 'url' in map_df.columns, \"Mapping CSV must contain a 'url' column.\"\n",
    "\n",
    "cat_col = 'category' if 'category' in map_df.columns else None\n",
    "let_col = 'letter_section' if 'letter_section' in map_df.columns else ('letter' if 'letter' in map_df.columns else None)\n",
    "\n",
    "# Aggregate ALL categories/letters per URL and keep also a primary for compatibility\n",
    "ctx_all = {}\n",
    "for _, r in map_df.iterrows():\n",
    "    u = r['url']\n",
    "    if not isinstance(u, str) or not u:\n",
    "        continue\n",
    "    cat = r[cat_col] if cat_col and isinstance(r.get(cat_col, None), str) else \"\"\n",
    "    let = r[let_col] if let_col and isinstance(r.get(let_col, None), str) else \"\"\n",
    "    if u not in ctx_all:\n",
    "        ctx_all[u] = {\n",
    "            'categories': set(),\n",
    "            'letters': set(),\n",
    "            'primary_category': cat or \"\",\n",
    "            'primary_letter': let or \"\"\n",
    "        }\n",
    "    if cat:\n",
    "        ctx_all[u]['categories'].add(cat)\n",
    "    if let:\n",
    "        ctx_all[u]['letters'].add(let)\n",
    "\n",
    "urls = list(ctx_all.keys())\n",
    "print(f\"Unique detail URLs to backfill: {len(urls):,}\")\n",
    "\n",
    "# --- Rate limiter ---\n",
    "class FixedWindowRateLimiter:\n",
    "    def __init__(self, max_per_sec: int):\n",
    "        self.max_per_sec = max_per_sec\n",
    "        self.lock = threading.Lock()\n",
    "        self.window = collections.deque()\n",
    "    def acquire(self):\n",
    "        while True:\n",
    "            with self.lock:\n",
    "                now = time.monotonic()\n",
    "                while self.window and (now - self.window[0]) > 1.0:\n",
    "                    self.window.popleft()\n",
    "                if len(self.window) < self.max_per_sec:\n",
    "                    self.window.append(now)\n",
    "                    return\n",
    "            time.sleep(0.01)\n",
    "\n",
    "limiter = FixedWindowRateLimiter(RATE_LIMIT_PER_SEC)\n",
    "\n",
    "# --- Website enhancer (handle image-wrapped and redirected anchors; avoid hamburg.de/berlin.de) ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "BLOCKLIST = {'branchenbuch.hamburg.de','google.','maps.google','hvv.de','geofox.de','booking.com','yelp.','tripadvisor.','facebook.com','instagram.com','x.com','twitter.com','berlin.de'}\n",
    "\n",
    "thread_local = threading.local()\n",
    "\n",
    "def _resolve_redirect(href: str) -> str:\n",
    "    try:\n",
    "        if not href or not href.startswith('http'):\n",
    "            return ''\n",
    "        u = urlparse(href)\n",
    "        qs = parse_qs(u.query)\n",
    "        for k in ['url','dest','destination','to','link','u']:\n",
    "            if k in qs and qs[k]:\n",
    "                target = qs[k][0]\n",
    "                target = unquote(target)\n",
    "                if target.startswith('http'):\n",
    "                    return target\n",
    "        return href\n",
    "    except Exception:\n",
    "        return href\n",
    "\n",
    "def _hostname(u: str) -> str:\n",
    "    try:\n",
    "        return urlparse(u).hostname or ''\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "def _is_portal_domain(host: str) -> bool:\n",
    "    host = (host or '').lower()\n",
    "    return ('hamburg.de' in host) or ('berlin.de' in host)\n",
    "\n",
    "def _name_tokens(name: str):\n",
    "    name = (name or '').lower()\n",
    "    name = re.sub(r'[^a-z0-9äöüß\\s-]', ' ', name)\n",
    "    toks = [t for t in re.split(r'[\\s-]+', name) if len(t) >= 3]\n",
    "    stop = {'gmbh','mbh','kg','ag','ug','ev','e.v','gbr','ohg','co','und','the','hamburg','firma','betrieb','service','services','recycling','gruppe'}\n",
    "    return [t for t in toks if t not in stop]\n",
    "\n",
    "def _pick_website_from_html(html: str, company_name: str = '') -> str:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # json-ld hint\n",
    "    jsonld_obj = None\n",
    "    for tag in soup.find_all('script', type='application/ld+json'):\n",
    "        try:\n",
    "            data = json.loads(tag.string or tag.text or '{}')\n",
    "        except Exception:\n",
    "            data = None\n",
    "        if isinstance(data, dict) and data.get('@type'):\n",
    "            jsonld_obj = data\n",
    "            break\n",
    "    # collect anchors\n",
    "    candidates = []\n",
    "    for a in soup.find_all('a'):\n",
    "        href = (a.get('href') or '').strip()\n",
    "        durl = a.get('data-url') or a.get('data-href') or a.get('data-website')\n",
    "        href = _resolve_redirect(durl.strip()) if durl else _resolve_redirect(href)\n",
    "        if not href.startswith('http'):\n",
    "            continue\n",
    "        if any(b in href for b in BLOCKLIST):\n",
    "            continue\n",
    "        label = (a.get_text(' ', strip=True) or a.get('aria-label') or a.get('title') or '').lower()\n",
    "        has_img = a.find('img') is not None\n",
    "        candidates.append((href, label, has_img))\n",
    "    # json-ld preference\n",
    "    if isinstance(jsonld_obj, dict):\n",
    "        for k in ('url','sameAs','@id'):\n",
    "            v = jsonld_obj.get(k)\n",
    "            if isinstance(v, str):\n",
    "                vv = _resolve_redirect(v)\n",
    "                if vv.startswith('http') and not any(b in vv for b in BLOCKLIST) and not _is_portal_domain(_hostname(vv)):\n",
    "                    return vv\n",
    "            elif isinstance(v, list):\n",
    "                for x in v:\n",
    "                    if isinstance(x, str):\n",
    "                        vv = _resolve_redirect(x)\n",
    "                        if vv.startswith('http') and not any(b in vv for b in BLOCKLIST) and not _is_portal_domain(_hostname(vv)):\n",
    "                            return vv\n",
    "    name_toks = set(_name_tokens(company_name))\n",
    "    website_keywords = ('website','webseite','zur website','homepage','zur homepage','internetseite','zur internetseite')\n",
    "    # label-based\n",
    "    for href, label, _ in candidates:\n",
    "        if any(p in label for p in website_keywords) and not _is_portal_domain(_hostname(href)):\n",
    "            return href\n",
    "    # image-wrapped\n",
    "    for href, _, has_img in candidates:\n",
    "        if has_img and not _is_portal_domain(_hostname(href)):\n",
    "            return href\n",
    "    # name-token match\n",
    "    for href, _, _ in candidates:\n",
    "        host = _hostname(href).lower()\n",
    "        if not _is_portal_domain(host) and any(tok in host for tok in name_toks):\n",
    "            return href\n",
    "    # any external\n",
    "    for href, _, _ in candidates:\n",
    "        if not _is_portal_domain(_hostname(href)):\n",
    "            return href\n",
    "    return ''\n",
    "\n",
    "def _get_fallback_session():\n",
    "    sess = getattr(thread_local, 'http_sess', None)\n",
    "    if sess is None:\n",
    "        sess = requests.Session()\n",
    "        retry = Retry(total=3, backoff_factor=0.4, status_forcelist=[429,500,502,503,504], allowed_methods=[\"GET\"]) \n",
    "        adapter = HTTPAdapter(max_retries=retry, pool_connections=100, pool_maxsize=100)\n",
    "        sess.mount('http://', adapter)\n",
    "        sess.mount('https://', adapter)\n",
    "        sess.headers.update({'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36'})\n",
    "        thread_local.http_sess = sess\n",
    "    return sess\n",
    "\n",
    "def enhance_website_if_needed(row: dict):\n",
    "    w = (row.get('website') or '').strip()\n",
    "    if w and not _is_portal_domain(_hostname(w)):\n",
    "        return row\n",
    "    url = row.get('source_url') or ''\n",
    "    if not url:\n",
    "        return row\n",
    "    try:\n",
    "        sess = _get_fallback_session()\n",
    "        r = sess.get(url, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        better = _pick_website_from_html(r.text, company_name=row.get('name',''))\n",
    "        if better:\n",
    "            row['website'] = better\n",
    "    except Exception:\n",
    "        pass\n",
    "    return row\n",
    "\n",
    "# --- Thread-local scraper instance ---\n",
    "\n",
    "def get_worker_scraper():\n",
    "    s = getattr(thread_local, 'scraper', None)\n",
    "    if s is None:\n",
    "        if _HAS_FULL:\n",
    "            s = EnhancedHamburgBranchenbuchScraper(min_delay=0.0, max_delay=0.02, retry_attempts=2, enable_caching=False)\n",
    "        else:\n",
    "            # Minimal inline extractor used if full class isn't defined\n",
    "            class MinimalExtractor:\n",
    "                def __init__(self):\n",
    "                    self.sess = _get_fallback_session()\n",
    "                def extract_company_details(self, company_url: str, category_name: str = '', letter: str = '') -> dict:\n",
    "                    try:\n",
    "                        r = self.sess.get(company_url, timeout=20)\n",
    "                        r.raise_for_status()\n",
    "                        html = r.text\n",
    "                        # basic name\n",
    "                        soup = BeautifulSoup(html, 'html.parser')\n",
    "                        name = ''\n",
    "                        h = soup.find(['h1','h2'])\n",
    "                        if h:\n",
    "                            name = h.get_text(strip=True)\n",
    "                        # phone/email/address\n",
    "                        tel = soup.find('a', href=re.compile(r'^tel:', re.I))\n",
    "                        phone = re.sub(r'^tel:', '', tel['href']).strip() if tel and tel.get('href') else ''\n",
    "                        mail = soup.find('a', href=re.compile(r'^mailto:', re.I))\n",
    "                        email = re.sub(r'^mailto:', '', mail['href']).strip() if mail and mail.get('href') else ''\n",
    "                        address = ''\n",
    "                        add_tag = soup.find('address')\n",
    "                        if add_tag:\n",
    "                            address = ' '.join(add_tag.get_text(' ', strip=True).split())\n",
    "                        website = _pick_website_from_html(html, company_name=name)\n",
    "                        return {\n",
    "                            'source_url': company_url, 'name': name, 'website': website, 'phone': phone, 'email': email,\n",
    "                            'address': address, 'latitude': '', 'longitude': '', 'image': '',\n",
    "                            'category': category_name, 'letter_section': letter,\n",
    "                            'scraped_at': datetime.utcnow().isoformat()\n",
    "                        }\n",
    "                    except Exception:\n",
    "                        return {'source_url': company_url, 'name': '', 'website': '', 'phone': '', 'email': '', 'address': '', 'latitude': '', 'longitude': '', 'image': '', 'category': category_name, 'letter_section': letter, 'scraped_at': datetime.utcnow().isoformat()}\n",
    "            s = MinimalExtractor()\n",
    "        thread_local.scraper = s\n",
    "    return s\n",
    "\n",
    "# --- Robust fetch using extract_company_details + website enhancement ---\n",
    "from datetime import datetime as _dt\n",
    "\n",
    "def _fallback_row(url: str, ctx: dict):\n",
    "    cats = sorted(list(ctx.get('categories', set())))\n",
    "    lets = sorted(list(ctx.get('letters', set())))\n",
    "    return {\n",
    "        \"source_url\": url,\n",
    "        \"name\": \"\",\n",
    "        \"website\": \"\",\n",
    "        \"phone\": \"\",\n",
    "        \"email\": \"\",\n",
    "        \"address\": \"\",\n",
    "        \"latitude\": \"\",\n",
    "        \"longitude\": \"\",\n",
    "        \"image\": \"\",\n",
    "        # keep compatibility single values, but also provide aggregated lists\n",
    "        \"category\": ctx.get('primary_category', cats[0] if cats else \"\"),\n",
    "        \"letter_section\": ctx.get('primary_letter', lets[0] if lets else \"\"),\n",
    "        \"categories_all\": json.dumps(cats, ensure_ascii=False),\n",
    "        \"letter_sections_all\": json.dumps(lets, ensure_ascii=False),\n",
    "        \"scraped_at\": _dt.utcnow().isoformat(),\n",
    "    }\n",
    "\n",
    "def fetch_detail(url: str) -> dict:\n",
    "    ctx = ctx_all.get(url, {})\n",
    "    cats = sorted(list(ctx.get('categories', set())))\n",
    "    lets = sorted(list(ctx.get('letters', set())))\n",
    "    primary_cat = ctx.get('primary_category', cats[0] if cats else \"\")\n",
    "    primary_let = ctx.get('primary_letter', lets[0] if lets else \"\")\n",
    "\n",
    "    limiter.acquire()\n",
    "    time.sleep(random.uniform(*SMALL_JITTER))\n",
    "    s = get_worker_scraper()\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            row = s.extract_company_details(url, category_name=primary_cat, letter=primary_let)\n",
    "            if isinstance(row, dict):\n",
    "                row.setdefault('source_url', url)\n",
    "                row['category'] = row.get('category') or primary_cat\n",
    "                row['letter_section'] = row.get('letter_section') or primary_let\n",
    "                row['categories_all'] = json.dumps(cats, ensure_ascii=False)\n",
    "                row['letter_sections_all'] = json.dumps(lets, ensure_ascii=False)\n",
    "                row.setdefault('scraped_at', _dt.utcnow().isoformat())\n",
    "                # Enhance website if missing/incorrect (hamburg/berlin portal)\n",
    "                row = enhance_website_if_needed(row)\n",
    "                # If name/address still blank, try quick fallbacks from HTML\n",
    "                if not row.get('name') or not row.get('address'):\n",
    "                    try:\n",
    "                        sess = _get_fallback_session()\n",
    "                        r = sess.get(url, timeout=20)\n",
    "                        r.raise_for_status()\n",
    "                        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "                        if not row.get('name'):\n",
    "                            h = soup.find(['h1','h2'])\n",
    "                            if h:\n",
    "                                row['name'] = h.get_text(' ', strip=True)\n",
    "                        if not row.get('address'):\n",
    "                            addr = soup.find('address')\n",
    "                            if addr:\n",
    "                                row['address'] = ' '.join(addr.get_text(' ', strip=True).split())\n",
    "                            if not row['address']:\n",
    "                                # microdata fallback\n",
    "                                street = soup.find(attrs={\"itemprop\": \"streetAddress\"})\n",
    "                                plz = soup.find(attrs={\"itemprop\": \"postalCode\"})\n",
    "                                city = soup.find(attrs={\"itemprop\": \"addressLocality\"})\n",
    "                                parts = []\n",
    "                                for el in (street, plz, city):\n",
    "                                    if el:\n",
    "                                        parts.append(el.get_text(' ', strip=True))\n",
    "                                if parts:\n",
    "                                    row['address'] = ' '.join(parts)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                return row\n",
    "        except Exception:\n",
    "            time.sleep(0.25 * (2 ** attempt) + random.uniform(0.02, 0.15))\n",
    "    return _fallback_row(url, ctx)\n",
    "\n",
    "# --- Execute with progress + partial saves ---\n",
    "rows = []\n",
    "started = time.time()\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "part_idx = 1\n",
    "\n",
    "EXPECTED_COLS = [\n",
    "    \"name\",\"address\",\"phone\",\"email\",\"website\",\"latitude\",\"longitude\",\"image\",\n",
    "    \"category\",\"letter_section\",\"categories_all\",\"letter_sections_all\",\n",
    "    \"source_url\",\"scraped_at\"\n",
    "]\n",
    "\n",
    "def save_partial(force: bool=False):\n",
    "    global rows, part_idx\n",
    "    if not rows and not force:\n",
    "        return\n",
    "    dfp = pd.DataFrame(rows)\n",
    "    # normalize columns\n",
    "    for c in EXPECTED_COLS:\n",
    "        if c not in dfp.columns:\n",
    "            dfp[c] = \"\"\n",
    "    dfp = dfp[EXPECTED_COLS]\n",
    "    if 'source_url' in dfp.columns:\n",
    "        dfp = dfp.drop_duplicates(subset=['source_url']).reset_index(drop=True)\n",
    "    outp = os.path.join(OUT_PART_DIR, f\"details_from_map_{ts}_chunk{part_idx:03d}.csv\")\n",
    "    dfp.to_csv(outp, index=False)\n",
    "    print(f\"Saved partial chunk {part_idx:03d}: {len(dfp)} rows → {outp}\")\n",
    "    part_idx += 1\n",
    "    rows = []\n",
    "\n",
    "with cf.ThreadPoolExecutor(max_workers=MAX_WORKERS_DETAILS) as ex:\n",
    "    futures = [ex.submit(fetch_detail, u) for u in urls]\n",
    "    for fut in tqdm(cf.as_completed(futures), total=len(futures), desc=\"Extracting company details\", unit=\"company\"):\n",
    "        try:\n",
    "            res = fut.result()\n",
    "            if isinstance(res, dict):\n",
    "                rows.append(res)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if len(rows) >= SAVE_EVERY_N:\n",
    "            save_partial()\n",
    "\n",
    "# Save remaining and consolidate\n",
    "save_partial(force=True)\n",
    "partials = sorted(glob.glob(os.path.join(OUT_PART_DIR, f\"details_from_map_{ts}_chunk*.csv\")))\n",
    "all_df = []\n",
    "for p in partials:\n",
    "    try:\n",
    "        if os.path.getsize(p) > 0:\n",
    "            all_df.append(pd.read_csv(p))\n",
    "    except Exception:\n",
    "        pass\n",
    "if all_df:\n",
    "    df = pd.concat(all_df, ignore_index=True)\n",
    "    for c in EXPECTED_COLS:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"\"\n",
    "    df = df[EXPECTED_COLS]\n",
    "    if 'source_url' in df.columns:\n",
    "        df = df.drop_duplicates(subset=['source_url']).reset_index(drop=True)\n",
    "    out_final = f\"hamburg_branchenbuch_companies_details_from_map_{ts}.csv\"\n",
    "    df.to_csv(out_final, index=False)\n",
    "    took = time.time() - started\n",
    "    print(f\"✅ Done: {len(df)} rows → {out_final} in {took/60:.1f} min.\")\n",
    "    display(df.head(5))\n",
    "else:\n",
    "    print(\"No partials to consolidate (no rows?).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "600f5bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 34 CSV file(s) to merge from: /home/thiesen/Documents/AI-Innoscence_Ecosystem/data/hh_branchenbuch_checkpoints/from_map\n",
      "Merged rows before de-duplication: 67,899\n",
      "Cleared website for 28,392 row(s) with placeholder https://www.koeln.de/ .\n",
      "Saved ALL entries: 67,899 rows → hamburg_branchenbuch_companies_details_from_map_20250930_162445_all.csv\n",
      "Saved entries WITH websites: 39,507 rows → hamburg_branchenbuch_companies_details_from_map_20250930_162445_with_websites.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>phone</th>\n",
       "      <th>email</th>\n",
       "      <th>website</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>image</th>\n",
       "      <th>category</th>\n",
       "      <th>letter_section</th>\n",
       "      <th>categories_all</th>\n",
       "      <th>letter_sections_all</th>\n",
       "      <th>source_url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbruchunternehmen Peter Frank</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49407219189.0</td>\n",
       "      <td>postfach@abbruch-frank.de</td>\n",
       "      <td>http://www.abbruch-frank.de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abbruchunternehmen</td>\n",
       "      <td>a</td>\n",
       "      <td>[\"Abbruchunternehmen\", \"Bauen &amp; Wohnen\", \"Bauu...</td>\n",
       "      <td>[\"a\", \"b\"]</td>\n",
       "      <td>https://www.hamburg.de/branchenbuch/hamburg/ei...</td>\n",
       "      <td>2025-09-30T11:44:11.623791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PBR Peter Beuck Recycling GmbH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>494020905925.0</td>\n",
       "      <td>info@peter-beuck.de</td>\n",
       "      <td>https://peter-beuck.de/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abbruchunternehmen</td>\n",
       "      <td>a</td>\n",
       "      <td>[\"Abbruchunternehmen\", \"Auto &amp; Verkehr\", \"Baue...</td>\n",
       "      <td>[\"a\", \"b\", \"c\", \"e\", \"h\", \"l\", \"m\", \"o\", \"p\", ...</td>\n",
       "      <td>https://www.hamburg.de/branchenbuch/hamburg/ei...</td>\n",
       "      <td>2025-09-30T11:44:11.829827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DCD Schröder &amp; Kindler GbR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>494080007474.0</td>\n",
       "      <td>moin@dcd24.de</td>\n",
       "      <td>http://www.dcd24.de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abbruchunternehmen</td>\n",
       "      <td>a</td>\n",
       "      <td>[\"Abbruchunternehmen\", \"Bauen &amp; Wohnen\", \"Bauu...</td>\n",
       "      <td>[\"a\", \"b\", \"e\", \"h\", \"k\", \"l\", \"m\", \"o\", \"r\", ...</td>\n",
       "      <td>https://www.hamburg.de/branchenbuch/hamburg/ei...</td>\n",
       "      <td>2025-09-30T11:44:11.522036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name  address           phone  \\\n",
       "0  Abbruchunternehmen Peter Frank      NaN   49407219189.0   \n",
       "1  PBR Peter Beuck Recycling GmbH      NaN  494020905925.0   \n",
       "2      DCD Schröder & Kindler GbR      NaN  494080007474.0   \n",
       "\n",
       "                       email                      website  latitude  \\\n",
       "0  postfach@abbruch-frank.de  http://www.abbruch-frank.de       NaN   \n",
       "1        info@peter-beuck.de      https://peter-beuck.de/       NaN   \n",
       "2              moin@dcd24.de          http://www.dcd24.de       NaN   \n",
       "\n",
       "   longitude  image            category letter_section  \\\n",
       "0        NaN    NaN  Abbruchunternehmen              a   \n",
       "1        NaN    NaN  Abbruchunternehmen              a   \n",
       "2        NaN    NaN  Abbruchunternehmen              a   \n",
       "\n",
       "                                      categories_all  \\\n",
       "0  [\"Abbruchunternehmen\", \"Bauen & Wohnen\", \"Bauu...   \n",
       "1  [\"Abbruchunternehmen\", \"Auto & Verkehr\", \"Baue...   \n",
       "2  [\"Abbruchunternehmen\", \"Bauen & Wohnen\", \"Bauu...   \n",
       "\n",
       "                                 letter_sections_all  \\\n",
       "0                                         [\"a\", \"b\"]   \n",
       "1  [\"a\", \"b\", \"c\", \"e\", \"h\", \"l\", \"m\", \"o\", \"p\", ...   \n",
       "2  [\"a\", \"b\", \"e\", \"h\", \"k\", \"l\", \"m\", \"o\", \"r\", ...   \n",
       "\n",
       "                                          source_url  \\\n",
       "0  https://www.hamburg.de/branchenbuch/hamburg/ei...   \n",
       "1  https://www.hamburg.de/branchenbuch/hamburg/ei...   \n",
       "2  https://www.hamburg.de/branchenbuch/hamburg/ei...   \n",
       "\n",
       "                   scraped_at  \n",
       "0  2025-09-30T11:44:11.623791  \n",
       "1  2025-09-30T11:44:11.829827  \n",
       "2  2025-09-30T11:44:11.522036  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>phone</th>\n",
       "      <th>email</th>\n",
       "      <th>website</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>image</th>\n",
       "      <th>category</th>\n",
       "      <th>letter_section</th>\n",
       "      <th>categories_all</th>\n",
       "      <th>letter_sections_all</th>\n",
       "      <th>source_url</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbruchunternehmen Peter Frank</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49407219189.0</td>\n",
       "      <td>postfach@abbruch-frank.de</td>\n",
       "      <td>http://www.abbruch-frank.de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abbruchunternehmen</td>\n",
       "      <td>a</td>\n",
       "      <td>[\"Abbruchunternehmen\", \"Bauen &amp; Wohnen\", \"Bauu...</td>\n",
       "      <td>[\"a\", \"b\"]</td>\n",
       "      <td>https://www.hamburg.de/branchenbuch/hamburg/ei...</td>\n",
       "      <td>2025-09-30T11:44:11.623791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PBR Peter Beuck Recycling GmbH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>494020905925.0</td>\n",
       "      <td>info@peter-beuck.de</td>\n",
       "      <td>https://peter-beuck.de/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abbruchunternehmen</td>\n",
       "      <td>a</td>\n",
       "      <td>[\"Abbruchunternehmen\", \"Auto &amp; Verkehr\", \"Baue...</td>\n",
       "      <td>[\"a\", \"b\", \"c\", \"e\", \"h\", \"l\", \"m\", \"o\", \"p\", ...</td>\n",
       "      <td>https://www.hamburg.de/branchenbuch/hamburg/ei...</td>\n",
       "      <td>2025-09-30T11:44:11.829827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DCD Schröder &amp; Kindler GbR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>494080007474.0</td>\n",
       "      <td>moin@dcd24.de</td>\n",
       "      <td>http://www.dcd24.de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abbruchunternehmen</td>\n",
       "      <td>a</td>\n",
       "      <td>[\"Abbruchunternehmen\", \"Bauen &amp; Wohnen\", \"Bauu...</td>\n",
       "      <td>[\"a\", \"b\", \"e\", \"h\", \"k\", \"l\", \"m\", \"o\", \"r\", ...</td>\n",
       "      <td>https://www.hamburg.de/branchenbuch/hamburg/ei...</td>\n",
       "      <td>2025-09-30T11:44:11.522036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name  address           phone  \\\n",
       "0  Abbruchunternehmen Peter Frank      NaN   49407219189.0   \n",
       "1  PBR Peter Beuck Recycling GmbH      NaN  494020905925.0   \n",
       "2      DCD Schröder & Kindler GbR      NaN  494080007474.0   \n",
       "\n",
       "                       email                      website  latitude  \\\n",
       "0  postfach@abbruch-frank.de  http://www.abbruch-frank.de       NaN   \n",
       "1        info@peter-beuck.de      https://peter-beuck.de/       NaN   \n",
       "2              moin@dcd24.de          http://www.dcd24.de       NaN   \n",
       "\n",
       "   longitude  image            category letter_section  \\\n",
       "0        NaN    NaN  Abbruchunternehmen              a   \n",
       "1        NaN    NaN  Abbruchunternehmen              a   \n",
       "2        NaN    NaN  Abbruchunternehmen              a   \n",
       "\n",
       "                                      categories_all  \\\n",
       "0  [\"Abbruchunternehmen\", \"Bauen & Wohnen\", \"Bauu...   \n",
       "1  [\"Abbruchunternehmen\", \"Auto & Verkehr\", \"Baue...   \n",
       "2  [\"Abbruchunternehmen\", \"Bauen & Wohnen\", \"Bauu...   \n",
       "\n",
       "                                 letter_sections_all  \\\n",
       "0                                         [\"a\", \"b\"]   \n",
       "1  [\"a\", \"b\", \"c\", \"e\", \"h\", \"l\", \"m\", \"o\", \"p\", ...   \n",
       "2  [\"a\", \"b\", \"e\", \"h\", \"k\", \"l\", \"m\", \"o\", \"r\", ...   \n",
       "\n",
       "                                          source_url  \\\n",
       "0  https://www.hamburg.de/branchenbuch/hamburg/ei...   \n",
       "1  https://www.hamburg.de/branchenbuch/hamburg/ei...   \n",
       "2  https://www.hamburg.de/branchenbuch/hamburg/ei...   \n",
       "\n",
       "                   scraped_at  \n",
       "0  2025-09-30T11:44:11.623791  \n",
       "1  2025-09-30T11:44:11.829827  \n",
       "2  2025-09-30T11:44:11.522036  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5) Combine all from_map chunks, clean wrong websites, and export consolidated CSVs\n",
    "import os, glob\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing partial chunks written by the backfill\n",
    "FROM_MAP_DIR = os.path.join(os.getcwd(), \"data\", \"hh_branchenbuch_checkpoints\", \"from_map\")\n",
    "assert os.path.isdir(FROM_MAP_DIR), f\"Directory not found: {FROM_MAP_DIR}\"\n",
    "\n",
    "# Prefer only chunk files; if none exist, fall back to any CSVs in the directory\n",
    "chunk_pattern = os.path.join(FROM_MAP_DIR, \"details_from_map_*_chunk*.csv\")\n",
    "chunk_files = sorted(glob.glob(chunk_pattern))\n",
    "if not chunk_files:\n",
    "    chunk_files = sorted(glob.glob(os.path.join(FROM_MAP_DIR, \"*.csv\")))\n",
    "    print(\"No explicit chunk files found; falling back to all CSVs in from_map directory.\")\n",
    "\n",
    "print(f\"Discovered {len(chunk_files)} CSV file(s) to merge from: {FROM_MAP_DIR}\")\n",
    "if not chunk_files:\n",
    "    raise FileNotFoundError(\"No CSV files found to merge. Ensure the backfill created chunk files in from_map.\")\n",
    "\n",
    "# Read and concatenate\n",
    "frames = []\n",
    "for p in chunk_files:\n",
    "    try:\n",
    "        if os.path.getsize(p) > 0:\n",
    "            frames.append(pd.read_csv(p))\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Skipping {p}: {e}\")\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"Found CSVs but none could be loaded. Inspect logs above.\")\n",
    "\n",
    "merged = pd.concat(frames, ignore_index=True)\n",
    "print(f\"Merged rows before de-duplication: {len(merged):,}\")\n",
    "\n",
    "# Normalize expected columns to avoid KeyErrors downstream\n",
    "EXPECTED_COLS = [\n",
    "    \"name\",\"address\",\"phone\",\"email\",\"website\",\"latitude\",\"longitude\",\"image\",\n",
    "    \"category\",\"letter_section\",\"categories_all\",\"letter_sections_all\",\n",
    "    \"source_url\",\"scraped_at\"\n",
    "]\n",
    "for c in EXPECTED_COLS:\n",
    "    if c not in merged.columns:\n",
    "        merged[c] = \"\"\n",
    "\n",
    "# Drop duplicate companies by source_url (keep the last occurrence in case later chunks contain improved data)\n",
    "if \"source_url\" in merged.columns:\n",
    "    merged = merged.drop_duplicates(subset=[\"source_url\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "# Clean incorrect website placeholder: delete website value where it's exactly https://www.koeln.de/\n",
    "# (Keep rows; just clear the incorrect website field)\n",
    "if \"website\" in merged.columns:\n",
    "    w = merged[\"website\"].astype(str).str.strip()\n",
    "    mask_wrong = w.str.lower().eq(\"https://www.koeln.de/\")\n",
    "    merged.loc[mask_wrong, \"website\"] = \"\"\n",
    "    print(f\"Cleared website for {mask_wrong.sum():,} row(s) with placeholder https://www.koeln.de/ .\")\n",
    "\n",
    "# Reorder columns (optional but consistent)\n",
    "merged = merged[EXPECTED_COLS]\n",
    "\n",
    "# Save outputs in workspace root\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_all = f\"hamburg_branchenbuch_companies_details_from_map_{ts}_all.csv\"\n",
    "out_with_sites = f\"hamburg_branchenbuch_companies_details_from_map_{ts}_with_websites.csv\"\n",
    "\n",
    "merged.to_csv(out_all, index=False)\n",
    "with_web = merged[merged[\"website\"].astype(str).str.strip().ne(\"\")].copy()\n",
    "with_web.to_csv(out_with_sites, index=False)\n",
    "\n",
    "print(f\"Saved ALL entries: {len(merged):,} rows → {out_all}\")\n",
    "print(f\"Saved entries WITH websites: {len(with_web):,} rows → {out_with_sites}\")\n",
    "\n",
    "display(merged.head(3))\n",
    "display(with_web.head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Innoscence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
